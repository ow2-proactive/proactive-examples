<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.11" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Visdom_Realtime_Digit_Classification" onTaskError="continueJobExecution" priority="normal" projectName="3. Visdom Workflows" xsi:schemaLocation="urn:proactive:jobdescriptor:3.11 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.11/schedulerjob.xsd">
  <variables>
    <variable model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
    <variable model="PA:Boolean" name="DOCKER_GPU_ENABLED" value="False"/>
    <variable model="PA:LIST(activeeon/dlm3,activeeon/cuda)" name="DOCKER_IMAGE" value="activeeon/dlm3"/>
    <variable name="NODE_ACCESS_TOKEN" value=""/>
  </variables>
  <description>
    <![CDATA[ Show an example of realtime plotting using the Visdom server for training a convolutional neural network (CNN) for MNIST digit classification. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="data-visualization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
<info name="Documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_visdom"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task name="Train_CNN">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <variables>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_GPU_ENABLED" value="False"/>
        <variable inherited="true" name="DOCKER_IMAGE" value="activeeon/dlm3"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$NODE_ACCESS_TOKEN"/>
      </genericInformation>
      <depends>
        <task ref="Start_Visdom_Service"/>
      </depends>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="python" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torchvision import datasets, transforms
from torch.autograd import Variable
from visdom import Visdom

visdom_endpoint = variables.get("VISDOM_ENDPOINT") if variables.get("VISDOM_ENDPOINT") else results[0].__str__()
print("VISDOM_ENDPOINT: ", visdom_endpoint)

if visdom_endpoint is not None:
    visdom_endpoint = visdom_endpoint.replace("http://", "")

(VISDOM_HOST, VISDOM_PORT) = visdom_endpoint.split(":")

# Training settings
parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                    help='input batch size for training (default: 64)')
parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                    help='input batch size for testing (default: 1000)')
parser.add_argument('--epochs', type=int, default=3, metavar='N',
                    help='number of epochs to train (default: 3)')
parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
                    help='learning rate (default: 0.01)')
parser.add_argument('--momentum', type=float, default=0.5, metavar='M',
                    help='SGD momentum (default: 0.5)')
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='disables CUDA training')
parser.add_argument('--seed', type=int, default=1, metavar='S',
                    help='random seed (default: 1)')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='how many batches to wait before logging training status')
parser.add_argument('--visdom_host', type=str, default=VISDOM_HOST,
                    help='IP of the visdom server')
parser.add_argument('--visdom_port', type=int, default=VISDOM_PORT,
                    help='IP port of the visdom server')
args = parser.parse_args()

print("Connecting to %s:%s" % (args.visdom_host, args.visdom_port))
viz = Visdom(server="http://"+args.visdom_host, port=args.visdom_port)
assert viz.check_connection()

win_epoch_loss = viz.line(Y=np.array([np.nan]), X=np.array([np.nan]),
                          opts=dict(
                                  xlabel='Iteration',
                                  ylabel='Loss',
                                  title='Training loss (per iteration)',
                                  ),
                          )
win_global_loss = viz.line(Y=np.array([np.nan]), X=np.array([np.nan]),
                           opts=dict(
                                  xlabel='Epoch',
                                  ylabel='Loss',
                                  title='Training loss (per epoch)',
                                  ),
                           )
win_test_loss = viz.line(Y=np.array([np.nan]), X=np.array([np.nan]),
                           opts=dict(
                                  xlabel='Epoch',
                                  ylabel='Loss',
                                  title='Test loss (per epoch)',
                                  ),
                           )
win_test_acc = viz.line(Y=np.array([np.nan]), X=np.array([np.nan]),
                           opts=dict(
                                  xlabel='Epoch',
                                  ylabel='Accuracy',
                                  title='Test accuracy (per epoch)',
                                  ),
                           )
win_train_log = viz.text("Training log:\n")
win_test_log = viz.text("Testing log:\n")


use_cuda = not args.no_cuda and torch.cuda.is_available()
print("Use CUDA? " + str(use_cuda))

device = torch.device("cuda" if use_cuda else "cpu")

torch.manual_seed(args.seed)

kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}

train_loader = torch.utils.data.DataLoader(
  datasets.MNIST('data', train=True, download=True,
                 transform=transforms.Compose([
                     transforms.ToTensor(),
                     transforms.Normalize((0.1307,), (0.3081,))
                 ])),
  batch_size=args.batch_size, shuffle=True, **kwargs)

test_loader = torch.utils.data.DataLoader(
  datasets.MNIST('data', train=False, transform=transforms.Compose([
                     transforms.ToTensor(),
                     transforms.Normalize((0.1307,), (0.3081,))
                 ])),
  batch_size=args.test_batch_size, shuffle=True, **kwargs)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4*4*50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4*4*50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


model = Net().to(device)

optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

epoch_loss_train = []
average_loss_train = 0
iteration_train = 0
def train(epoch):
    global iteration_train
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        iteration_train = iteration_train + 1
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        # store loss values
        epoch_loss_train.append(loss.data.item())
        average_loss_train = sum(epoch_loss_train) / len(epoch_loss_train)

        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                  epoch, batch_idx * len(data), len(train_loader.dataset),
                  100. * batch_idx / len(train_loader), loss.data.item()))
            viz.text('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                  epoch, batch_idx * len(data), len(train_loader.dataset),
                  100. * batch_idx / len(train_loader), loss.data.item()), win=win_train_log, append=True)
            # plot loss per iteration
            viz.line(Y=np.array([average_loss_train]), X=np.array([iteration_train]), win=win_epoch_loss, update='append')

    # plot loss per epoch
    viz.line(Y=np.array([average_loss_train]), X=np.array([epoch]), win=win_global_loss, update='append')


epoch_loss_test = []
average_loss_test = 0
def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    test_acc = 100. * correct / len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
          test_loss, correct, len(test_loader.dataset), test_acc))
    viz.text('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
          test_loss, correct, len(test_loader.dataset), test_acc), win=win_test_log, append=True)

    # plot loss per epoch
    viz.line(Y=np.array([test_loss]), X=np.array([epoch]), win=win_test_loss, update='append')
    viz.line(Y=np.array([test_acc]), X=np.array([epoch]), win=win_test_acc, update='append')


for epoch in range(1, args.epochs+1):
    train(epoch)
    test(epoch)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            279.296875
        </positionTop>
        <positionLeft>
            328.7109375
        </positionLeft>
      </metadata>
    </task>
    <task name="Start_Visdom_Service" onTaskError="cancelJob">
      <description>
        <![CDATA[ Start the Visdom server as a service. ]]>
      </description>
      <variables>
        <variable inherited="false" name="SERVICE_ID" value="Visdom"/>
        <variable inherited="true" name="INSTANCE_NAME" value="visdom-server"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/cloud-automation-scripts/resources/Service_Start/raw"/>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
variables.put("ENDPOINT_VISDOM",variables.get("ENDPOINT_" + variables.get("INSTANCE_NAME")))
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
            122.421875
        </positionTop>
        <positionLeft>
            298.88671875
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2646px;
            height:3493px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-117.421875px;left:-293.88671875px"><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_376" style="top: 279.312px; left: 328.711px; z-index: 24;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png" width="20px">&nbsp;<span class="name">Train_CNN</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_379" style="top: 122.422px; left: 298.887px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png" width="20px">&nbsp;<span class="name">Start_Visdom_Service</span></a></div><svg style="position:absolute;left:361.5px;top:162.5px" width="27.7109375" height="117.3125" pointer-events="none" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 6.7109375 116.3125 C 16.7109375 66.3125 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M8.834040453124999,88.645396375 L13.713854770397335,68.02532333369038 L7.37653678004685,74.72147153166063 L-0.21007007294204882,69.48282700676853 L8.834040453124999,88.645396375" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M8.834040453124999,88.645396375 L13.713854770397335,68.02532333369038 L7.37653678004685,74.72147153166063 L-0.21007007294204882,69.48282700676853 L8.834040453124999,88.645396375" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 368.711px; top: 309.312px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 368.711px; top: 269.312px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 362px; top: 153px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
    </visualization>
  </metadata>
</job>
