<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.10" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Visdom_Realtime_Digit_Classification" onTaskError="continueJobExecution" priority="normal" projectName="2. Visdom Workflows" xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd">
  <variables>
    <variable model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
    <variable name="visdom_instance_name" value="visdom-server-1"/>
  </variables>
  <description>
    <![CDATA[ Shows an example of realtime plotting using the Visdom server for training a convolutional neural network (CNN) for MNIST digit classification. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="data-visualization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
<info name="Documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_visdom"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task name="Train_CNN">
      <description>
        <![CDATA[ The simplest task, ran by a python engine. ]]>
      </description>
      <variables>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Start_Visdom_Service"/>
      </depends>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torchvision import datasets, transforms
from torch.autograd import Variable
from visdom import Visdom

visdom_endpoint = variables.get("visdom_endpoint")

if visdom_endpoint is not None:
  visdom_endpoint = visdom_endpoint.replace("http://", "")

(VISDOM_HOST, VISDOM_PORT) = visdom_endpoint.split(":")

# Training settings
parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                    help='input batch size for training (default: 64)')
parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                    help='input batch size for testing (default: 1000)')
parser.add_argument('--epochs', type=int, default=3, metavar='N',
                    help='number of epochs to train (default: 3)')
parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
                    help='learning rate (default: 0.01)')
parser.add_argument('--momentum', type=float, default=0.5, metavar='M',
                    help='SGD momentum (default: 0.5)')
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='disables CUDA training')
parser.add_argument('--seed', type=int, default=1, metavar='S',
                    help='random seed (default: 1)')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='how many batches to wait before logging training status')
parser.add_argument('--visdom_host', type=str, default=VISDOM_HOST, 
                    help='IP of the visdom server')
parser.add_argument('--visdom_port', type=int, default=VISDOM_PORT, 
                    help='IP port of the visdom server')
args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()
print("Cuda is available: " + str(args.cuda))

print("Connecting to %s:%s" % (args.visdom_host, args.visdom_port))
viz = Visdom(server="http://"+args.visdom_host, port=args.visdom_port)
assert viz.check_connection()

win_epoch_loss = viz.line(Y = np.array([1]), X = np.array([1]),
                          opts = dict(
                                  xlabel = 'Iteration',
                                  ylabel = 'Loss',
                                  title = 'Training loss (per iteration)',
                                  ),
                          )
win_global_loss = viz.line(Y = np.array([1]), X = np.array([1]),
                           opts = dict(
                                  xlabel = 'Epoch',
                                  ylabel = 'Loss',
                                  title = 'Training loss (per epoch)',
                                  ),
                           )
win_test_loss = viz.line(Y = np.array([1]), X = np.array([1]),
                           opts = dict(
                                  xlabel = 'Epoch',
                                  ylabel = 'Loss',
                                  title = 'Test loss (per epoch)',
                                  ),
                           )
win_test_acc = viz.line(Y = np.array([1]), X = np.array([1]),
                           opts = dict(
                                  xlabel = 'Epoch',
                                  ylabel = 'Accuracy',
                                  title = 'Test accuracy (per epoch)',
                                  ),
                           )
win_train_log = viz.text("Training log:\n")
win_test_log = viz.text("Testing log:\n")

torch.manual_seed(args.seed)
if args.cuda:
  torch.cuda.manual_seed(args.seed)

kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}

train_loader = torch.utils.data.DataLoader(
  datasets.MNIST('../data', train=True, download=True,
                 transform=transforms.Compose([
                     transforms.ToTensor(),
                     transforms.Normalize((0.1307,), (0.3081,))
                 ])),
  batch_size=args.batch_size, shuffle=True, **kwargs)

test_loader = torch.utils.data.DataLoader(
  datasets.MNIST('../data', train=False, transform=transforms.Compose([
                     transforms.ToTensor(),
                     transforms.Normalize((0.1307,), (0.3081,))
                 ])),
  batch_size=args.test_batch_size, shuffle=True, **kwargs)


class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
    self.conv2_drop = nn.Dropout2d()
    self.fc1 = nn.Linear(320, 50)
    self.fc2 = nn.Linear(50, 10)

  def forward(self, x):
    x = F.relu(F.max_pool2d(self.conv1(x), 2))
    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
    x = x.view(-1, 320)
    x = F.relu(self.fc1(x))
    x = F.dropout(x, training=self.training)
    x = self.fc2(x)
    return F.log_softmax(x)

model = Net()
if args.cuda:
  model.cuda()

optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

epoch_loss_train = []
average_loss_train = 0
iteration_train = 0
def train(epoch):
  global iteration_train
  model.train()
  for batch_idx, (data, target) in enumerate(train_loader):
    if args.cuda:
      data, target = data.cuda(), target.cuda()
    iteration_train = iteration_train + 1
    data, target = Variable(data), Variable(target)
    optimizer.zero_grad()
    output = model(data)
    loss = F.nll_loss(output, target)
    loss.backward()
    optimizer.step()
    # store loss values
    epoch_loss_train.append(loss.data[0])
    average_loss_train = sum(epoch_loss_train) / len(epoch_loss_train)

    if batch_idx % args.log_interval == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
            epoch, batch_idx * len(data), len(train_loader.dataset),
            100. * batch_idx / len(train_loader), loss.data[0]))
      viz.text('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
            epoch, batch_idx * len(data), len(train_loader.dataset),
            100. * batch_idx / len(train_loader), loss.data[0]), win=win_train_log, append=True)
      # plot loss per iteration
      if iteration_train == 1:
        viz.line(Y = np.array([average_loss_train]), X = np.array([iteration_train]), win = win_epoch_loss, update='replace')
      else:
        viz.line(Y = np.array([average_loss_train]), X = np.array([iteration_train]), win = win_epoch_loss, update='append')

  # plot loss per epoch
  if epoch == 1:
    viz.line(Y = np.array([average_loss_train]), X = np.array([epoch]), win = win_global_loss, update='replace')
  else:
    viz.line(Y = np.array([average_loss_train]), X = np.array([epoch]), win = win_global_loss, update='append')


epoch_loss_test = []
average_loss_test = 0
def test(epoch):
  model.eval()
  test_loss = 0
  correct = 0
  for data, target in test_loader:
    if args.cuda:
        data, target = data.cuda(), target.cuda()
    data, target = Variable(data, volatile=True), Variable(target)
    output = model(data)
    test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss
    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
    correct += pred.eq(target.data.view_as(pred)).cpu().sum()

  test_loss /= len(test_loader.dataset)
  test_acc = 100. * correct / len(test_loader.dataset)

  print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset), test_acc))
  viz.text('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset), test_acc), win=win_test_log, append=True)

  # plot loss per epoch
  if epoch == 1:
    viz.line(Y = np.array([test_loss]), X = np.array([epoch]), win = win_test_loss, update='replace')
    viz.line(Y = np.array([test_acc]), X = np.array([epoch]), win = win_test_acc, update='replace')
  else:
    viz.line(Y = np.array([test_loss]), X = np.array([epoch]), win = win_test_loss, update='append')
    viz.line(Y = np.array([test_acc]), X = np.array([epoch]), win = win_test_acc, update='append')


for epoch in range(1, args.epochs + 1):
  train(epoch)
  test(epoch)
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Start_Visdom_Service" onTaskError="cancelJob">
      <description>
        <![CDATA[ Start the Visdom server as a service. ]]>
      </description>
      <variables>
        <variable inherited="false" name="visdom_service_id" value="Visdom"/>
        <variable inherited="true" name="visdom_instance_name" value="visdom-server-1"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html"/>
      </genericInformation>
      <inputFiles>
        <files accessMode="transferFromGlobalSpace" includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
      </inputFiles>
      <forkEnvironment>
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Start_Visdom_Service ---")

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Check existing service instances
def service_id = variables.get("visdom_service_id")
def instance_name = variables.get("visdom_instance_name")
println("*_service_id:    " + service_id)
println("*_instance_name: " + instance_name)

boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
	if ( (service_instance_data.getServiceId() == service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      if (service_instance_data.getVariables().get("INSTANCE_NAME") == instance_name) {
        instance_exists = true
        instance_id = service_instance_data.getInstanceId()
  		endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
        println("*_instance_id: " + instance_id)
        println("*_endpoint:    " + endpoint)
        variables.put("visdom_instance_id", instance_id)
        variables.put("visdom_endpoint", endpoint)
        break
      }
  	}
}

println("instance_exists: " + instance_exists)

if (!instance_exists){
  // Prepare service description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(service_id) 
  serviceDescription.putVariablesItem("INSTANCE_NAME", instance_name)
  
  // Run service
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire service Instance ID
  def service_instance_id = service_instance_data.getInstanceId()
  println("service_instance_id: " + service_instance_id)
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id
  println("channel: " + channel)
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire service endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  instance_name = service_instance_data.getVariables().get("INSTANCE_NAME")
  instance_id = service_instance_data.getInstanceId()
  endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
  
  println("INSTANCE_NAME: " + instance_name)
  println("*_instance_id: " + instance_id)
  println("*_endpoint: " + endpoint)
  
  variables.put("visdom_instance_id", instance_id)
  variables.put("visdom_endpoint", endpoint)
  
  result = '<meta http-equiv="refresh" content="1; url=' + endpoint + '" />'
  result+= '<h2><span style="color:black">Please wait while redirecting...</span></h2>'
  resultMetadata.put("content.type", "text/html")
}

println("--- END Start_Visdom_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Finish_Visdom_Service" onTaskError="cancelJob">
      <description>
        <![CDATA[ Finish the Visdom service. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html"/>
      </genericInformation>
      <depends>
        <task ref="Wait_For_Web_Validation"/>
      </depends>
      <inputFiles>
        <files accessMode="transferFromGlobalSpace" includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
      </inputFiles>
      <forkEnvironment>
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Finish_Visdom_Service ---")

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

instance_id = variables.get("visdom_instance_id")
println("*_instance_id: " + instance_id)
assert instance_id != null

// Finish service
ServiceDescription service = new ServiceDescription()
service.setBucketName("cloud-automation") 
service.setWorkflowName("Finish_Visdom")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, instance_id, service)

println("--- END Finish_Visdom_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Wait_For_Web_Validation" onTaskError="pauseJob">
      <description>
        <![CDATA[ Task to pause the job and send a validation message to the notification service ]]>
      </description>
      <genericInformation>
        <info name="TASK.ICON" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Train_CNN"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="python">
            <![CDATA[
# Please fill variables
notification_message = 'Please, confirm to stop the Visdom service'

# Don't change code below unless you know what you are doing
from org.ow2.proactive.addons.webhook import Webhook

jobid = variables.get("PA_JOB_ID")
userName = variables.get("PA_USER")
schedulerURL =  variables.get("PA_SCHEDULER_REST_URL")

print schedulerURL
# get sessionid
schedulerapi.connect()

# pause job
schedulerapi.pauseJob(jobid)

# send web validation
print "Sending web validation..."
url = schedulerURL.replace("/rest", "") +'/notification-service/notifications'
headers = '{\"Content-Type\" : \"application/json\" }'
notification_content = '{\"description\": \"'+notification_message+'\", \"jobId\": \"'+jobid+'\" , \"validation\": \"true\", \"userName\":  \"'+userName+'\"}'
Webhook.execute('POST', url, headers, notification_content);
print "Web Validation sent"
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>
