<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="RNN" projectName="3. Text Classification"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="GPU_NODES_ONLY" value="False" model="PA:Boolean"/>
    <variable name="DOCKER_ENABLED" value="True" model="PA:Boolean"/>
  </variables>
  <description>
    <![CDATA[ A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle.
http://pytorch.org/docs/0.3.1/nn.html#torch.nn.RNN ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="deep-learning"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_text_classification.png"/>
    <info name="Documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_rnn"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="RNN">
      <description>
        <![CDATA[ Simple RNN Model ]]>
      </description>
      <variables>
        <variable name="EMBEDDING_DIM" value="50" inherited="false" />
        <variable name="HIDDEN_DIM" value="40" inherited="false" />
        <variable name="BATCH_SIZE" value="2" inherited="false" />
        <variable name="DROPOUT" value="0.5" inherited="false" />
        <variable name="GPU_NODES_ONLY" value="False" inherited="true" model="PA:Boolean"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_text_classification.png"/>
        <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_rnn"/>
      </genericInformation>
      <selection>
        <script
         type="static" >
          <code language="python">
            <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Create a RNN model")
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F

BATCH_SIZE=2
HIDDEN_DIM=50
EMBEDDING_DIM=50
DROPOUT=0.5

if 'variables' in locals():  
  if variables.get("BATCH_SIZE") is not None:
    BATCH_SIZE = variables.get("BATCH_SIZE")
  else:
    print("BATCH_SIZE not defined by the user. Using the default value:"+BATCH_SIZE)
  if variables.get("HIDDEN_DIM") is not None:
    HIDDEN_DIM = variables.get("HIDDEN_DIM")
  else:
    print("HIDDEN_DIM not defined by the user. Using the default value:"+HIDDEN_DIM)
  if variables.get("EMBEDDING_DIM") is not None:
    EMBEDDING_DIM = variables.get("EMBEDDING_DIM")
  else:
    print("EMBEDDING_DIM not defined by the user. Using the default value:"+EMBEDDING_DIM)
  if variables.get("DROPOUT") is not None:
    DROPOUT = variables.get("DROPOUT")
  else:
    print("DROPOUT not defined by the user. Using the default value:"+DROPOUT)


MODEL_TYPE = 'RNN'
MODEL_CLASS = """
class RNN(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, use_gpu, batch_size, dropout=0.5):
        super(RNN, self).__init__()
        self.hidden_dim = hidden_dim
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        self.dropout = dropout
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.RNN = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim)
        self.hidden2label = nn.Linear(hidden_dim, label_size)
        self.hidden = self.init_hidden()

    def init_hidden(self):
        if self.use_gpu:
            return (Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda()))
        else:
            return (Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))

    def forward(self, sentence):
        x = self.embeddings(sentence).view(len(sentence), self.batch_size, -1)
        rnn_out, self.hidden = self.RNN(x, self.hidden)
        y = self.hidden2label(rnn_out[-1])
        log_probs = F.log_softmax(y)
        return log_probs"""
    
MODEL_DEF = """
MODEL = RNN(embedding_dim="""+str(EMBEDDING_DIM)+""", hidden_dim="""+str(HIDDEN_DIM)+""", vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,use_gpu=USE_GPU, batch_size=BATCH_SIZE)
"""
print(MODEL_DEF)

# Forward model
try:
    variables.put("MODEL_CLASS", MODEL_CLASS)
    variables.put("MODEL_DEF", MODEL_DEF)
    variables.put("BATCH_SIZE", BATCH_SIZE)
    variables.put("HIDDEN_DIM", HIDDEN_DIM)
    variables.put("EMBEDDING_DIM", EMBEDDING_DIM)
    variables.put("DROPOUT", DROPOUT)
except NameError as err:
    print("{0}".format(err))
    print("Warning: this script is running outside from ProActive.")
    pass

print("END RNN model built")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>