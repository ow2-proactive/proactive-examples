<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.11" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Import_Image_Dataset" onTaskError="continueJobExecution" priority="normal" projectName="1. Input and Output" xsi:schemaLocation="urn:proactive:jobdescriptor:3.11 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.11/schedulerjob.xsd">
  <variables>
    <variable model="PA:Boolean" name="GPU_NODES_ONLY" value="False"/>
    <variable model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
  </variables>
  <description>
    <![CDATA[ Load and return an image dataset. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="deep-learning-dev"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png"/>
<info name="Documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_import_image_dataset"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task name="Import_Image_Dataset">
      <description>
        <![CDATA[ Load and return an image dataset. ]]>
      </description>
      <variables>
        <variable inherited="true" model="PA:Boolean" name="GPU_NODES_ONLY" value="False"/>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
        <variable inherited="true" name="DATASET_URL" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/coco.zip"/>
        <variable inherited="true" name="SPLIT_TRAIN" value="0.60"/>
        <variable inherited="true" name="SPLIT_VAL" value="0.15"/>
        <variable inherited="true" name="SPLIT_TEST" value="0.25"/>
        <variable inherited="true" model="PA:LIST(Classification, Detection, Segmentation)" name="DATASET_TYPE" value="Detection"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png"/>
        <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_import_image_dataset"/>
      </genericInformation>
      <selection>
        <script type="static">
          <code language="python">
            <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Import_Image_Dataset")

import re
import os
import json
import wget
import uuid 
import shutil
import zipfile
from os import remove, listdir, makedirs
from os.path import basename, splitext, exists, join
from sklearn.model_selection import train_test_split
 
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/ants_vs_bees.zip'  #CLASSIFICATION DATASET
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/pascal_voc.zip'     #DTECTION DATASET
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/pascal_coco.zip'     #DTECTION DATASET
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/oxford.zip'        #SEGMENTATION DATASET

SPLIT_SETS  = ['train','val','test']

if 'variables' in locals():
  if variables.get("DATASET_URL") is not None:
    DATASET_URL = variables.get("DATASET_URL")
  if variables.get("SPLIT_TRAIN") is not None:
    SPLIT_TRAIN = float(str(variables.get("SPLIT_TRAIN")))
  if variables.get("SPLIT_VAL") is not None:
    SPLIT_VAL = float(str(variables.get("SPLIT_VAL")))
  if variables.get("SPLIT_TEST") is not None:
    SPLIT_TEST = float(str(variables.get("SPLIT_TEST")))
  
DATASET_TYPE = variables.get("DATASET_TYPE")
DATASET_TYPE = DATASET_TYPE.upper()
print(DATASET_TYPE)
# Get an unique ID
ID = str(uuid.uuid4())

# Define localspace
LOCALSPACE = join('data', ID)
if exists(LOCALSPACE):
  shutil.rmtree(LOCALSPACE)
makedirs(LOCALSPACE)

print("LOCALSPACE:  " + LOCALSPACE)
print("DATASET_URL: " + DATASET_URL)
assert DATASET_URL is not None

print("Split information: ")
print("SPLIT_TRAIN: " + str(SPLIT_TRAIN))
print("SPLIT_VAL:   " + str(SPLIT_VAL))
print("SPLIT_TEST:  " + str(SPLIT_TEST))

assert SPLIT_TRAIN >= 0.0
assert SPLIT_VAL >= 0.0
assert SPLIT_TEST >= 0.0
assert (SPLIT_TRAIN + SPLIT_VAL + SPLIT_TEST) == 1.0
if SPLIT_TRAIN == 0.0 and SPLIT_VAL > 0.0:
  raise AssertionError("SPLIT_VAL cannot be defined when SPLIT_TRAIN equals zero") 

DATASET_NAME = splitext(DATASET_URL[DATASET_URL.rfind("/")+1:])[0]
DATASET_PATH = join(LOCALSPACE, DATASET_NAME)

if exists(DATASET_PATH):
  shutil.rmtree(DATASET_PATH)
makedirs(DATASET_PATH)

print("Dataset information: ")
print("DATASET_NAME: " + DATASET_NAME)
print("DATASET_PATH: " + DATASET_PATH)

print("Downloading...")
filename = wget.download(DATASET_URL, DATASET_PATH)
print("FILENAME: " + filename)
print("OK")

print("Extracting...")
dataset_zip = zipfile.ZipFile(filename)
dataset_zip.extractall(DATASET_PATH)
dataset_zip.close()
remove(filename)
print("OK")

#CLASSIFICATION DATASET 
if DATASET_TYPE == 'CLASSIFICATION':  
    k = 0
    images_list = []
    images_label = []
    labels_name = []
    for root in listdir(DATASET_PATH):
        if (not root.startswith('.')):
            labels_name.append(root)
            label_dir = join(DATASET_PATH, root)
            print(label_dir)
            files = listdir(label_dir)
            files[:] = [join(label_dir,file) for file in files]
            files_size = len(files)
            files_label = [k] * files_size
            images_list = images_list + files
            images_label = images_label + files_label
            k = k + 1


    print("Splitting the dataset into train and test")
    images_train, images_test, labels_train, labels_test = train_test_split(images_list, images_label, test_size=SPLIT_TEST, random_state=1)

    images_val = []
    labels_val = []
    if SPLIT_TRAIN != 0.0 and SPLIT_VAL != 0.0:
        print("Splitting the train into train and val")
        images_train, images_val, labels_train, labels_val = train_test_split(images_train, labels_train, test_size=SPLIT_VAL, random_state=1)

    images_split = {SPLIT_SETS[0] : images_train, SPLIT_SETS[1] : images_val, SPLIT_SETS[2] : images_test}
    labels_split = {SPLIT_SETS[0] : labels_train, SPLIT_SETS[1] : labels_val, SPLIT_SETS[2] : labels_test}

    def move_images(_path, _images_list, _labels_list, _labels_name):
        if exists(_path):
            shutil.rmtree(_path)
        makedirs(_path)
        for _label_name in _labels_name:
            makedirs(join(_path, _label_name))
        for _image_path, _image_label in zip(_images_list, _labels_list):
            _image_path_dest = join(_path, _labels_name[_image_label], basename(_image_path))
            print("Moving " + _image_path + " to " + _image_path_dest)
            shutil.move(_image_path, _image_path_dest)

    DATASET_LABELS = json.dumps(labels_name)
    print("DATASET_LABELS: " + DATASET_LABELS)


    for SPLIT_NAME in SPLIT_SETS:
        SPLIT_PATH = join(DATASET_PATH, SPLIT_NAME)
        print("SPLIT_PATH: " + SPLIT_PATH)
        move_images(SPLIT_PATH, images_split[SPLIT_NAME], labels_split[SPLIT_NAME], labels_name)

    if 'variables' in locals():
        variables.put("DATASET_NAME", DATASET_NAME)
        variables.put("DATASET_PATH", DATASET_PATH)
        variables.put("DATASET_LABELS", DATASET_LABELS)
        variables.put("DATASET_TYPE", DATASET_TYPE)        
    
    print("END Import_Image_Dataset")
    
#DETECTION / SEGMENTATION DATASET    
elif DATASET_TYPE == 'DETECTION' or DATASET_TYPE == 'SEGMENTATION':
    print(DATASET_TYPE)
    k = 0
    images_list = []
    images_list_gt = []
    labels_name = []              
    num_folder = len(next(os.walk(DATASET_PATH))[1])


    if num_folder == 2:
        for root in listdir(DATASET_PATH):
            labels_name.append(root)
            label_dir = join(DATASET_PATH, root)
            print(label_dir)
            files = listdir(label_dir)
            files[:] = [join(label_dir,file) for file in files]
            files_size = len(files)
            if k == 0:
                images_list = images_list + files
            if k == 1:
                images_list_gt = images_list_gt + files  
            k = k + 1   

    try:
        new_list = [x for x in images_list if re.search('.DS_Store', x)]
        images_list.remove(''.join(new_list))
    except:
        pass            

    try:
        new_list_gt = [x for x in images_list_gt if re.search('.DS_Store', x)]
        images_list_gt.remove(''.join(new_list_gt))    
    except:
        pass    
    
    images_list = sorted(images_list)
    images_list_gt = sorted(images_list_gt)

    print("Splitting the dataset into train and test")
    images_train, images_test, images_train_gt, images_test_gt = train_test_split(images_list, images_list_gt, test_size=SPLIT_TEST, random_state=1)
    
    images_val = []

    if SPLIT_TRAIN != 0.0 and SPLIT_VAL != 0.0:
        print("Splitting the train into train and val")
        images_train, images_val, images_train_gt, images_val_gt = train_test_split(images_train, images_train_gt, test_size=SPLIT_VAL, random_state=1)

        images_split = {SPLIT_SETS[0] : images_train, SPLIT_SETS[1] : images_val, SPLIT_SETS[2] : images_test}
        images_split_gt = {SPLIT_SETS[0] : images_train_gt, SPLIT_SETS[1] : images_val_gt, SPLIT_SETS[2] : images_test_gt}

    for SPLIT_NAME in SPLIT_SETS:
        SPLIT_PATH = join(DATASET_PATH, SPLIT_NAME)
        print("SPLIT_PATH: " + SPLIT_PATH)

            
    def move_seg_images(_path, _images_list, _labels_name):
        if exists(_path):
            shutil.rmtree(_path)
        makedirs(_path)
        for _image_path in _images_list:
            _image_path_dest = join(_path, basename(_image_path))
            print("Moving " + _image_path + " to " + _image_path_dest)
            shutil.move(_image_path, _image_path_dest)

    k = 0
    for bx in labels_name:
        for SPLIT_NAME in SPLIT_SETS:
            SPLIT_PATH = join(DATASET_PATH, SPLIT_NAME)
            if k == 0:
                print("SPLIT_PATH: " + SPLIT_PATH)
                move_seg_images(SPLIT_PATH + '/' + labels_name[0], images_split[SPLIT_NAME], labels_name)           
            if k == 1:
                print("SPLIT_PATH: " + SPLIT_PATH)
                move_seg_images(SPLIT_PATH + '/' + labels_name[1], images_split_gt[SPLIT_NAME], labels_name)
        k = k + 1            
        
    
    if 'variables' in locals():
        variables.put("DATASET_NAME", DATASET_NAME)
        variables.put("DATASET_PATH", DATASET_PATH)
        variables.put("DATASET_TYPE", DATASET_TYPE) 
        
    print("END Import_Image_Dataset")

    if num_folder != 2:    
        print('Please, check your dataset!')
        
else: 
    print('Please, check your dataset type variable!')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <outputFiles>
        <files accessMode="transferToGlobalSpace" includes="$DATASET_PATH/**"/>
      </outputFiles>
      <metadata>
        <positionTop>
            573.5
        </positionTop>
        <positionLeft>
            1141.75
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
</job>
