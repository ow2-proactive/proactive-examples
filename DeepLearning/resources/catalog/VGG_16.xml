<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Copyright Activeeon 2007-2021. All rights reserved. --><job xmlns="urn:proactive:jobdescriptor:3.12" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="VGG_16" onTaskError="continueJobExecution" priority="normal" projectName="2.1 Image Classification" xsi:schemaLocation="urn:proactive:jobdescriptor:3.12 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.12/schedulerjob.xsd">
  <variables>
    <variable name="NATIVE_SCHEDULER" value=""/>
    <variable name="NATIVE_SCHEDULER_PARAMS" value=""/>
    <variable name="NODE_SOURCE_NAME" value=""/>
    <variable name="NODE_ACCESS_TOKEN" value=""/>
    <variable model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable model="PA:Boolean" name="CONTAINER_GPU_ENABLED" value="True"/>
    <variable model="PA:LIST(,docker://activeeon/dlm3,docker://activeeon/cuda,docker://activeeon/cuda2,docker://activeeon/rapidsai,docker://activeeon/tensorflow:latest,docker://activeeon/tensorflow:latest-gpu)" name="CONTAINER_IMAGE" value=""/>
  </variables>
  <description>
    <![CDATA[ The VGG-16 is a convolutional neural network. It contains the definition of each layer along with pre-trained set of weights.  ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="deep-learning"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_learning.png"/>
<info name="NS" value="$NATIVE_SCHEDULER"/>
<info name="Documentation" value="PML/PMLUserGuide.html#_vgg_16"/>
<info name="NODE_ACCESS_TOKEN" value="$NODE_ACCESS_TOKEN"/>
<info name="NS_BATCH" value="$NATIVE_SCHEDULER_PARAMS"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="VGG_16">
      <description>
        <![CDATA[ The VGG-16 is a convolutional neural network. It contains the definition of each layer along with pre-trained set of weights. You can see more details in: http://pytorch.org/docs/master/torchvision/models.html ]]>
      </description>
      <variables>
        <variable inherited="false" name="USE_PRETRAINED_MODEL" value="True"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_learning.png"/>
        <info name="task.documentation" value="PML/PMLUserGuide.html#_vgg_16"/>
      </genericInformation>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_source_name/raw"/>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <file language="cpython" url="${PA_CATALOG_REST_URL}/buckets/deep-learning/resources/VGG_16_Script/raw"/>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            237.171875
        </positionTop>
        <positionLeft>
            345.140625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2144px;
            height:2820px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-232.171875px;left:-340.140625px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_4" style="top: 237.172px; left: 345.141px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="The VGG-16 is a convolutional neural network. It contains the definition of each layer along with pre-trained set of weights. You can see more details in: http://pytorch.org/docs/master/torchvision/models.html"><img src="/automation-dashboard/styles/patterns/img/wf-icons/deep_learning.png" width="20px">&nbsp;<span class="name">VGG_16</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 385px; top: 267px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
