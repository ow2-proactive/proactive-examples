<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.9"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.9 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.9/schedulerjob.xsd"
    name="Train_Image_Model" projectName="4. Train Model"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="GPU_NODES_ONLY" value="False" />
    <variable name="GPU_CUDA_PATH" value="/usr/local/cuda" />
    <variable name="DOCKER_ENABLED" value="True" />
  </variables>
  <description>
    <![CDATA[ Train a model using a CNN algorithm. ]]>
  </description>
    <genericInformation>
    <info name="bucketName" value="deep-learning"/>
    <info name="pca.action.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/train_deep.png"/>
    <info name="Documentation" value="http://activeeon.com/resources/automated-machine-learning-activeeon.pdf"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Train_Image_Model">
      <description>
        <![CDATA[ Train a model using a CNN algorithm. ]]>
      </description>
      <variables>
        <variable name="GPU_NODES_ONLY" value="False" inherited="true" />
        <variable name="GPU_CUDA_PATH" value="/usr/local/cuda" inherited="true" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" />
        <variable name="NUM_EPOCHS" value="1" inherited="true" />
        <variable name="BATCH_SIZE" value="4" inherited="false" />
        <variable name="NUM_WORKERS" value="2" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/train_deep.png"/>
      </genericInformation>
      <selection>
        <script
         type="static" >
          <code language="javascript">
            <![CDATA[
selected = ((variables.get("GPU_NODES_ONLY").equalsIgnoreCase("false")) || (variables.get("GPU_NODES_ONLY").equalsIgnoreCase("true") && org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkFileExist(variables.get("GPU_CUDA_PATH"))));
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Train_Image_Model")

import time
import copy
import uuid
import json

import torch
import torch.nn as nn
import torch.optim as optim

from os.path import join
from torch.optim import lr_scheduler
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms

NUM_EPOCHS  = 1
BATCH_SIZE  = 4
NUM_WORKERS = 2
SHUFFLE     = True
GLOBALSPACE = './data/'

if 'variables' in locals():
  if variables.get("NUM_EPOCHS") is not None:
    NUM_EPOCHS = int(str(variables.get("NUM_EPOCHS")))
  
  GLOBALSPACE   = variables.get("GLOBALSPACE")
  DATASET_PATH  = variables.get("DATASET_PATH")
  CNN_MODEL     = variables.get("CNN_MODEL")
  CNN_TRANSFORM = variables.get("CNN_TRANSFORM")

assert DATASET_PATH is not None
assert CNN_MODEL is not None
assert CNN_TRANSFORM is not None

# Load CNN transform
# data_transforms
exec(CNN_TRANSFORM)

# Load dataset
image_datasets = {x: 
  datasets.ImageFolder(join(DATASET_PATH, x), data_transforms[x]) 
  for x in ['train', 'val']}

data_loaders = {x: 
  DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS) 
  for x in ['train', 'val']}

# Training data is required
assert len(image_datasets['train']) > 0

# If validation set is empty, use the trainning set
if len(image_datasets['val']) == 0:
  image_datasets['val'] = image_datasets['train']
  data_loaders['val'] = data_loaders['train']

dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes
num_classes = len(class_names)

# Load CNN model
exec(CNN_MODEL)

# http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available
# Returns a bool indicating if CUDA is currently available.
use_gpu = torch.cuda.is_available()
if use_gpu:
  cnn = cnn.cuda()

# http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss
# This criterion combines LogSoftMax and NLLLoss in one single class.
criterion = nn.CrossEntropyLoss()

# http://pytorch.org/docs/master/optim.html#torch.optim.SGD
# Implements stochastic gradient descent (optionally with momentum).
# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)

# http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.StepLR
# Sets the learning rate of each parameter group to the initial lr decayed by gamma every step_size epochs. 
# When last_epoch=-1, sets initial lr as lr.
# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
  since = time.time()

  best_model_wts = copy.deepcopy(model.state_dict())
  best_acc = 0.0

  for epoch in range(num_epochs):
    print('Epoch {}/{}'.format(epoch, num_epochs - 1))
    print('-' * 10)

    # Each epoch has a training and validation phase
    for phase in ['train', 'val']:
      if phase == 'train':
        scheduler.step()
        model.train(True)  # Set model to training mode
      else:
        model.train(False)  # Set model to evaluate mode

      running_loss = 0.0
      running_corrects = 0

      # Iterate over data.
      for data in data_loaders[phase]:
        # get the inputs
        inputs, labels = data

        # wrap them in Variable
        if use_gpu:
          inputs = Variable(inputs.cuda())
          labels = Variable(labels.cuda())
        else:
          inputs, labels = Variable(inputs), Variable(labels)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward
        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)

        # backward + optimize only if in training phase
        if phase == 'train':
          loss.backward()
          optimizer.step()

        # statistics
        running_loss += loss.data[0] * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)

      epoch_loss = running_loss / dataset_sizes[phase]
      epoch_acc = running_corrects / dataset_sizes[phase]

      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

      # deep copy the model
      if phase == 'val' and epoch_acc > best_acc:
        best_acc = epoch_acc
        best_model_wts = copy.deepcopy(model.state_dict())
      
    print()

  time_elapsed = time.time() - since
  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
  print('Best val Acc: {:4f}'.format(best_acc))

  # load best model weights
  model.load_state_dict(best_model_wts)
  return model

# Return the best model
best_cnn = train_model(cnn, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=NUM_EPOCHS)

# Get an unique ID
file_id = str(uuid.uuid4())

# Save trained model
print('Saving trained model...')
MODEL_FILENAME = file_id + ".pt"
MODEL_PATH = join(GLOBALSPACE, MODEL_FILENAME)
torch.save(best_cnn, MODEL_PATH)

# Save labels
print('Saving labels to a text file...')
LABELS_FILENAME = file_id + ".txt"
LABELS_PATH = join(GLOBALSPACE, LABELS_FILENAME)
with open(LABELS_PATH, 'w') as outfile:
  json.dump(class_names, outfile)

if 'variables' in locals():
  variables.put("MODEL_PATH", MODEL_PATH)
  variables.put("LABELS_PATH", LABELS_PATH)

print("END Train_Image_Model")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>