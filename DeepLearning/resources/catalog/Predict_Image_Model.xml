<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.9"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.9 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.9/schedulerjob.xsd"
    name="Predict_Image_Model" projectName="5. Predict"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="GPU_NODES_ONLY" value="False" />
    <variable name="GPU_CUDA_PATH" value="/usr/local/cuda" />
    <variable name="DOCKER_ENABLED" value="True" />
  </variables>
  <description>
    <![CDATA[ Predict a model using a deep learning algorithm. ]]>
  </description>
    <genericInformation>
    <info name="bucketName" value="deep-learning"/>
    <info name="pca.action.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict_deep_learning.png"/>
    <info name="Documentation" value="http://activeeon.com/resources/automated-machine-learning-activeeon.pdf"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Predict_Image_Model">
      <description>
        <![CDATA[ Predict a model using a deep learning algorithm. ]]>
      </description>
      <variables>
        <variable name="GPU_NODES_ONLY" value="False" inherited="true" />
        <variable name="GPU_CUDA_PATH" value="/usr/local/cuda" inherited="true" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" />
        <variable name="BATCH_SIZE" value="4" inherited="false" />
        <variable name="NUM_WORKERS" value="2" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/predict_deep_learning.png"/>
      </genericInformation>
      <selection>
        <script
         type="static" >
          <code language="javascript">
            <![CDATA[
selected = ((variables.get("GPU_NODES_ONLY").equalsIgnoreCase("false")) || (variables.get("GPU_NODES_ONLY").equalsIgnoreCase("true") && org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkFileExist(variables.get("GPU_CUDA_PATH"))));
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Predict_Image_Model")

import torch
import json

from os.path import join
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms

BATCH_SIZE  = 4
NUM_WORKERS = 2
SHUFFLE     = True

if 'variables' in locals():
  MODEL_PATH     = variables.get("MODEL_PATH")
  DATASET_PATH   = variables.get("DATASET_PATH")
  LABELS_PATH    = variables.get("LABELS_PATH")
  CNN_TRANSFORM  = variables.get("CNN_TRANSFORM")

assert MODEL_PATH is not None
assert DATASET_PATH is not None
assert LABELS_PATH is not None
assert CNN_TRANSFORM is not None

class_names = None
with open(LABELS_PATH, 'r') as f:
  class_names = json.load(f)
assert class_names is not None

# Load trained model
model = torch.load(MODEL_PATH)

# http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available
# Returns a bool indicating if CUDA is currently available.
use_gpu = torch.cuda.is_available()
if use_gpu:
  model = model.cuda()

# Load CNN transform
# data_transforms
exec(CNN_TRANSFORM)

# Load dataset
image_dataset = {x: 
  datasets.ImageFolder(join(DATASET_PATH, x), data_transforms[x]) 
  for x in ['test']}

data_loader = {x: 
  DataLoader(image_dataset[x], batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS) 
  for x in ['test']}

def predict_model(_model, _data_loader, _use_gpu, _class_names, _max_images=None):
  images_so_far = 0
  
  for i, data in enumerate(_data_loader['test']):
    inputs, labels = data
    
    if _use_gpu:
      inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())
    else:
      inputs, labels = Variable(inputs), Variable(labels)

    outputs = _model(inputs)
    _, preds = torch.max(outputs.data, 1)
    print(preds)

    for j in range(inputs.size()[0]):
      images_so_far += 1
      # 'predicted: {}'.format(_class_names[preds[j]])
      # inputs.cpu().data[j]
      
      if images_so_far == _max_images:
        return
    
    break

predict_model(model, data_loader, use_gpu, class_names)

print("END Predict_Image_Model")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>