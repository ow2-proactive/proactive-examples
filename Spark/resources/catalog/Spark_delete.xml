<?xml version="1.0" encoding="UTF-8"?>
<job xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="urn:proactive:jobdescriptor:3.8"
	xsi:schemaLocation="urn:proactive:jobdescriptor:3.8 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.8/schedulerjob.xsd"
	name="Spark_delete" projectName="Cloud Automation - Lifecycle"
	priority="normal" onTaskError="continueJobExecution"
	maxNumberOfExecution="2">
	<description>
    	<![CDATA[ Delete a Spark installation. ]]>
    </description>
	<genericInformation>
	    <info name="pca.service.id" value="HDFS-Spark" />
		<info name="pca.states" value="(SPARK_DEPLOYED,SWARM_DEPLOYED)(HDFS_SPARK_DEPLOYED,HDFS_DEPLOYED)"/>
	    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" />
	    <info name="group" value="public-objects" />
	</genericInformation>
	<taskFlow>
		<task name="delete_spark_and_update_servince_instance">
			<genericInformation>
				<info name="task.icon"
					  value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" />
			</genericInformation>
			<inputFiles>
				<files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
			</inputFiles>
			<forkEnvironment >
				<additionalClasspath>
					<pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
				</additionalClasspath>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="groovy">
						<![CDATA[
org.ow2.proactive.pca.service.client.ApiClient
org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
org.ow2.proactive.pca.service.client.model.ServiceInstanceData

// Retrieve variables
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def service_instance_id = variables.get("PCA_INSTANCE_ID") as Long

// Define other variables
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Get some service infos for the deletion
def service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
def instance_name = service_instance_data.getVariables().get("instance_name")

// List the spark slave docker containers to remove
def containers_id = new StringBuilder()
def cmd = ["docker", "ps", "-a", "-q", "--filter", "name=" + instance_name + "-spark-slave"]
cmd.execute().waitForProcessOutput(containers_id, System.err)

// Remove them
cmd = ["docker", "rm", "-fv"] + containers_id.toString().split("\n").toList()
cmd.execute().waitForProcessOutput(System.out, System.err)

// Remove the spark master docker container
cmd = ["docker", "rm", "-fv", instance_name + "-spark-master"]
cmd.execute().waitForProcessOutput(System.out, System.err)

// Update the related service instance status
def service_instance_status = service_instance_data.getInstanceStatus()

if (service_instance_status == "SPARK_DEPLOYED->SWARM_DEPLOYED")
	service_instance_data.setInstanceStatus("SWARM_DEPLOYED")
else if (service_instance_status == "HDFS_SPARK_DEPLOYED->HDFS_DEPLOYED")
	service_instance_data.setInstanceStatus("HDFS_DEPLOYED")

service_instance_data.getInstanceEndpoints().remove("spark_UI")
service_instance_rest_api.updateServiceInstanceUsingPUT(service_instance_id, service_instance_data)
]]>
					</code>
				</script>
			</scriptExecutable>
			<controlFlow block="none"></controlFlow>
		</task>
	</taskFlow>
</job>
