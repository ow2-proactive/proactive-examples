<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="Kafka_Storm_Visdom_Dataflow" projectName="Data Stream Processing"
    priority="normal"
    onTaskError="pauseJob"
     maxNumberOfExecution="2">
  <variables>
    <variable name="automatic_kill" value="true" model="PA:Boolean"/>
    <variable name="dataflow_name" value="BitcoinExchangeDataflow" />
    <variable name="execution_duration" value="180" />
    <variable name="kafka_instance_name" value="kafka-server-1" />
    <variable name="kafka_service_id" value="Kafka" />
    <variable name="message_topic" value="BitCoinExchangeTopic" />
    <variable name="storm_instance_name" value="storm-cluster-1" />
    <variable name="storm_service_id" value="Storm" />
    <variable name="visdom_instance_name" value="visdom-server-1" />
    <variable name="visdom_service_id" value="Visdom" />
    <variable name="zookeeper_instance_name" value="zookeeper-server-1" />
    <variable name="zookeeper_service_id" value="Zookeeper" />
  </variables>
  <genericInformation>
    <info name="bucketName" value="big-data"/>
    <info name="Documentation" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/workflow-documentation/big-data-streaming/ActiveEon-Data+Streaming-Doc.pdf"/>
    <info name="group" value="public-objects"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/bigdata.png"/>
  </genericInformation>
  <taskFlow>
    <task name="Start_Kafka">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/kafka.png"/>
      </genericInformation>
      <depends>
        <task ref="Start_Zookeeper"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.1.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.1.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import io.swagger.client.ApiClient
import io.swagger.client.api.ServiceInstanceRestApi
import io.swagger.client.model.ServiceInstanceData
import io.swagger.client.model.ServiceDescription


// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()
println(session_id)

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Check existing Kafka instances
def kafka_service_id = variables.get("kafka_service_id") 
def kafka_instance_name = variables.get("kafka_instance_name")
boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
  
	if ( (service_instance_data.getServiceId() == kafka_service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      
      if (service_instance_data.getVariables().get("instance_name") == kafka_instance_name) {
        instance_exists = true
        variables.put("kafka_instance_id", service_instance_data.getInstanceId())
        variables.put("kafka_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
        println(variables.get("kafka_endpoint"))
        break
      }
  	}
}

if (!instance_exists){
  // Prepare kafka description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(variables.get("kafka_service_id"))
  serviceDescription.putVariablesItem("instance_name",variables.get("kafka_instance_name"))
  
  // Run kafka
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire kafka Instance ID
  def service_instance_id  = service_instance_data.getInstanceId()
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id  
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire kafka endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  kafka_instance_name = service_instance_data.getVariables().get("instance_name")
  variables.put("kafka_instance_id", service_instance_data.getInstanceId())
  variables.put("kafka_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
  println(variables.get("kafka_endpoint"))
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Create_Topic">
      <description>
        <![CDATA[ Create Kafa topic (input of the dataflow) ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/kafka.png"/>
      </genericInformation>
      <depends>
        <task ref="Start_Kafka"/>
        <task ref="Start_Visdom"/>
        <task ref="Start_Storm"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
ZOOKEEPER=$variables_zookeeper_endpoint
KAFKA=$variables_kafka_instance_name
TOPIC=$variables_message_topic

res=$(docker exec $KAFKA /opt/kafka/bin/kafka-topics.sh --list --zookeeper $ZOOKEEPER | grep $TOPIC)

echo $res

if [[ $res != $TOPIC ]]; then
echo "create kafka topic: $TOPIC"
   docker exec $KAFKA /opt/kafka/bin/kafka-topics.sh --create --topic $TOPIC --replication-factor 1 --partitions 1 --zookeeper $ZOOKEEPER --config compression.type=lz4
fi
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Submit_Dataflow">
      <description>
        <![CDATA[ Submit Storm topology ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/storm.png"/>
      </genericInformation>
      <depends>
        <task ref="Create_Topic"/>
      </depends>
      <inputFiles>
        <files  includes="BitcoinExchangeDataflow-1.0.jar" accessMode="transferFromGlobalSpace"/>
        <files  includes="BitcoinExchangeDataflow.yaml" accessMode="transferFromGlobalSpace"/>
        <files  includes="BitcoinExchangeDataflow.properties" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
NIMBUS=$variables_storm_instance_name"-nimbus"
DATAFLOW=$variables_dataflow_name

ZOOKEEPER_ENDPOINT=$variables_zookeeper_endpoint
KAFKA_ENDPOINT=$variables_kafka_endpoint

echo "kafka.zookeeper.hosts: $ZOOKEEPER_ENDPOINT" >> BitcoinExchangeDataflow.properties  
echo "kafka.bootstrap.servers: $KAFKA_ENDPOINT" >> BitcoinExchangeDataflow.properties  

docker exec $NIMBUS mkdir -p /topologies

docker cp BitcoinExchangeDataflow-1.0.jar $NIMBUS:/topologies/
docker cp BitcoinExchangeDataflow.yaml $NIMBUS:/topologies/
docker cp BitcoinExchangeDataflow.properties $NIMBUS:/topologies/

##test if dataflow exists
docker exec -i  $NIMBUS storm list | grep -w "$DATAFLOW" && result=$?

if [[ "$result" == 0 ]]; then
	echo "Dataflow $DATAFLOW already exists, killing it"
    docker exec -i  $NIMBUS storm kill $DATAFLOW -w 1
    sleep 10    
fi

## deploy dataflow
docker exec -i $NIMBUS storm jar /topologies/BitcoinExchangeDataflow-1.0.jar org.apache.storm.flux.Flux --remote /topologies/BitcoinExchangeDataflow.yaml --filter /topologies/BitcoinExchangeDataflow.properties
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Vis-Rate">
      <description>
        <![CDATA[ Send bitcoin rate data to Visdom server ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Submit_Dataflow"/>
      </depends>
      <inputFiles>
        <files  includes="visdomRateClient.py" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
KAFKA=$variables_kafka_instance_name
VISDOM=$variables_visdom_instance_name
TOPIC=BitCoinRateTopic

pyscript=visdomRateClient.py
timeout=$variables_execution_duration

docker cp $pyscript $VISDOM:/root/$pyscript

docker exec $VISDOM /bin/bash -c 'rm rate.pckl'

timeout $timeout docker exec -i $KAFKA /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic $TOPIC | \
while read LINE; do
    docker exec $VISDOM /bin/bash -c "python $pyscript --value $LINE"
done

exit 0
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Vis_Variation">
      <description>
        <![CDATA[ Send bitcoin rate variation to Visdom server ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Submit_Dataflow"/>
      </depends>
      <inputFiles>
        <files  includes="visdomGapClient.py" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
KAFKA=$variables_kafka_instance_name
VISDOM=$variables_visdom_instance_name
TOPIC=BitCoinGapTopic

pyscript=visdomGapClient.py
timeout=$variables_execution_duration

docker cp $pyscript $VISDOM:/root/$pyscript

docker exec $VISDOM /bin/bash -c 'rm gap.pckl'

timeout $timeout docker exec -i $KAFKA /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic $TOPIC | \
while read LINE; do
    docker exec $VISDOM /bin/bash -c "python $pyscript --value $LINE"
done

exit 0
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="View_Result">
      <description>
        <![CDATA[ Give link to view results on Visdom and dataflow status on Storm ]]>
      </description>
      <depends>
        <task ref="Submit_Dataflow"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
String visdom_endpoint = variables.get("visdom_endpoint");

result = '<meta http-equiv="refresh" content="1; url=http://' + visdom_endpoint + '/" />'
result+= '<h2><span style="color:black">Please wait while redirecting...</span></h2>'
resultMetadata.put("content.type", "text/html")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Poll_Data">
      <description>
        <![CDATA[ Poll online data and send it to Kafka topic (input of the dataflow) ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/kafka.png"/>
      </genericInformation>
      <depends>
        <task ref="Create_Topic"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
ZOOKEEPER=$variables_zookeeper_endpoint
KAFKA=$variables_kafka_instance_name
TOPIC=$variables_message_topic

end=$((SECONDS+$variables_execution_duration))

counter=0
while [ $SECONDS -lt $end ]; do

	message1=$(curl -s "https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE&from_currency=BTC&to_currency=CNY&apikey=PVCAYZTDBSG37ARW")
	message2=$(curl -s "https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE&from_currency=BTC&to_currency=CNY&apikey=PVCAYZTDBSG37ARW")
	message3=$(curl -s "https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE&from_currency=BTC&to_currency=CNY&apikey=PVCAYZTDBSG37ARW")
	
 	docker exec -i $KAFKA /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic $TOPIC <<< $message1$'\n'$message2$'\n'$message3	
    (( counter=counter+3 ))
    
done
echo $counter
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Vis_Alerts">
      <description>
        <![CDATA[ Send bitcoin alerts to Visdom server ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Submit_Dataflow"/>
      </depends>
      <inputFiles>
        <files  includes="visdomAlertClient.py" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
KAFKA=$variables_kafka_instance_name
VISDOM=$variables_visdom_instance_name
TOPIC=BitCoinAlertTopic

pyscript=visdomAlertClient.py
timeout=$variables_execution_duration

docker cp $pyscript $VISDOM:/root/$pyscript

docker exec $VISDOM /bin/bash -c 'rm alerts.pckl'

timeout $timeout docker exec -i $KAFKA /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic $TOPIC | \
while read LINE; do
    docker exec $VISDOM /bin/bash -c "python $pyscript --value '$LINE'"
done

exit 0
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Kill_Services">
      <description>
        <![CDATA[ Stop PCA services, Zookeeper, Kafka, Storm and Visdom. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/proactive.png"/>
      </genericInformation>
      <depends>
        <task ref="Kill_Dataflow"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.1.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.1.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import io.swagger.client.ApiClient
import io.swagger.client.api.ServiceInstanceRestApi
import io.swagger.client.model.ServiceInstanceData
import io.swagger.client.model.ServiceDescription


// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()
println(session_id)

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Delete Zookeeper
ServiceDescription zookeeperDeleteService = new ServiceDescription()
zookeeperDeleteService.setBucketName("cloud-automation") 
zookeeperDeleteService.setWorkflowName("Zookeeper_Delete")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, variables.get("zookeeper_instance_id"), zookeeperDeleteService)

// Delete Visdom
ServiceDescription visdomDeleteService = new ServiceDescription()
visdomDeleteService.setBucketName("cloud-automation") 
visdomDeleteService.setWorkflowName("Visdom_Delete")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, variables.get("visdom_instance_id"), visdomDeleteService)

// Delete Kafka
ServiceDescription kafkaDeleteService = new ServiceDescription()
kafkaDeleteService.setBucketName("cloud-automation") 
kafkaDeleteService.setWorkflowName("Kafka_Delete")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, variables.get("kafka_instance_id"), kafkaDeleteService)

// Delete Storm
ServiceDescription stormDeleteService = new ServiceDescription()
stormDeleteService.setBucketName("cloud-automation") 
stormDeleteService.setWorkflowName("Storm_Delete")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, variables.get("storm_instance_id"), stormDeleteService)
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Finish">
      <depends>
        <task ref="Vis-Rate"/>
        <task ref="Vis_Variation"/>
        <task ref="Vis_Alerts"/>
        <task ref="View_Result"/>
        <task ref="Poll_Data"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[

]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow >
        <if target="Immediately"
        else="After_Web_Validation"
        continuation="Kill_Dataflow">
          <script>
            <code language="javascript">
              <![CDATA[
kill=variables.get("automatic_kill")
if(kill=="true"){
    branch = "if";
} else {
    branch = "else";
}
]]>
            </code>
          </script>
        </if>
      </controlFlow>
    </task>
    <task name="Immediately">
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
echo "Stopping dataflow and removing PCA platforms immediately"
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="After_Web_Validation">
      <scriptExecutable>
        <script>
          <code language="python">
            <![CDATA[
# Please fill variables
notification_message = 'Please validate to terminate the Kafka-Storm-Visdom Dataflow'

# Don't change code below unless you know what you are doing
from org.ow2.proactive.addons.webhook import Webhook

jobid = variables.get("PA_JOB_ID")
schedulerURL =  variables.get("PA_SCHEDULER_REST_URL")

# get sessionid
schedulerapi.connect()

# pause job
schedulerapi.pauseJob(jobid)


# send web validation
print "Sending web validation..."
url = schedulerURL.replace("/rest", "") +'/notification-service/notifications'
print url
headers = '{\"Content-Type\" : \"application/json\" }'
notification_content = '{\"description\": \"'+notification_message+'\", \"jobId\": \"'+jobid+'\" , \"validation\": \"true\"}'
Webhook.execute ( 'POST', url, headers, notification_content);
print "Web Validation sent"
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Kill_Dataflow">
      <description>
        <![CDATA[ Remove Storm topology ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/storm.png"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
ZOOKEEPER=$variables_zookeeper_endpoint
NIMBUS=$variables_storm_instance_name"-nimbus"
KAFKA=$variables_kafka_instance_name

docker exec -i $NIMBUS storm kill $variables_dataflow_name -w 5

docker exec $KAFKA /opt/kafka/bin/kafka-topics.sh --zookeeper $ZOOKEEPER --delete --topic BitCoinRateTopic
docker exec $KAFKA /opt/kafka/bin/kafka-topics.sh --zookeeper $ZOOKEEPER --delete --topic BitCoinGapTopic
docker exec $KAFKA /opt/kafka/bin/kafka-topics.sh --zookeeper $ZOOKEEPER --delete --topic BitCoinAlertTopic
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Start_Zookeeper">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/zookeeper.jpg"/>
      </genericInformation>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.1.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.1.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import io.swagger.client.ApiClient
import io.swagger.client.api.ServiceInstanceRestApi
import io.swagger.client.model.ServiceInstanceData
import io.swagger.client.model.ServiceDescription


// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()
println(session_id)

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Check existing Zookeeper instances
def zookeeper_service_id = variables.get("zookeeper_service_id") 
def zookeeper_instance_name = variables.get("zookeeper_instance_name")
boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
  
	if ( (service_instance_data.getServiceId() == zookeeper_service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      
      if (service_instance_data.getVariables().get("instance_name") == zookeeper_instance_name) {
        instance_exists = true        
        variables.put("zookeeper_instance_id", service_instance_data.getInstanceId())
        variables.put("zookeeper_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
        println(variables.get("zookeeper_endpoint"))
        break
      }
  	}
}

if (!instance_exists){
  // Prepare Zookeeper description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(variables.get("zookeeper_service_id"))
  serviceDescription.putVariablesItem("instance_name",variables.get("zookeeper_instance_name"))
  
  // Run Zookeeper
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire Zookeeper Instance ID
  def service_instance_id  = service_instance_data.getInstanceId()
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id  
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire Zookeeper endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  zookeeper_instance_name = service_instance_data.getVariables().get("instance_name")
  variables.put("zookeeper_instance_id", service_instance_data.getInstanceId())
  variables.put("zookeeper_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
  println(variables.get("zookeeper_endpoint"))
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Start_Visdom">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.1.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.1.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import io.swagger.client.ApiClient
import io.swagger.client.api.ServiceInstanceRestApi
import io.swagger.client.model.ServiceInstanceData
import io.swagger.client.model.ServiceDescription


// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()
println(session_id)

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Check existing visdom instances
def visdom_service_id = variables.get("visdom_service_id") 
def visdom_instance_name = variables.get("visdom_instance_name")
boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
  
	if ( (service_instance_data.getServiceId() == visdom_service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      
      if (service_instance_data.getVariables().get("instance_name") == visdom_instance_name) {
        instance_exists = true
        variables.put("visdom_instance_id", service_instance_data.getInstanceId())
        variables.put("visdom_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
        println(variables.get("visdom_endpoint"))
        break
      }
  	}
}

if (!instance_exists){
  // Prepare visdom description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(variables.get("visdom_service_id"))
  serviceDescription.putVariablesItem("instance_name",variables.get("visdom_instance_name"))
  
  // Run visdom
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire visdom Instance ID
  def service_instance_id  = service_instance_data.getInstanceId()
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id  
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire visdom endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  visdom_instance_name = service_instance_data.getVariables().get("instance_name")
  variables.put("visdom_instance_id", service_instance_data.getInstanceId())
  variables.put("visdom_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
  println(variables.get("visdom_endpoint"))
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Start_Storm">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/storm.png"/>
      </genericInformation>
      <depends>
        <task ref="Start_Zookeeper"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.1.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.1.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import io.swagger.client.ApiClient
import io.swagger.client.api.ServiceInstanceRestApi
import io.swagger.client.model.ServiceInstanceData
import io.swagger.client.model.ServiceDescription


// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()
println(session_id)

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Check existing storm instances
def storm_service_id = variables.get("storm_service_id") 
def storm_instance_name = variables.get("storm_instance_name")
boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
  
	if ( (service_instance_data.getServiceId() == storm_service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      
      if (service_instance_data.getVariables().get("instance_name") == storm_instance_name) {
        instance_exists = true
        variables.put("storm_instance_id", service_instance_data.getInstanceId())
        variables.put("storm_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
        println(variables.get("storm_endpoint"))
        break
      }
  	}
}

if (!instance_exists){
  // Prepare storm description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(variables.get("storm_service_id"))
  serviceDescription.putVariablesItem("instance_name",variables.get("storm_instance_name"))
  
  // Run storm
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire storm Instance ID
  def service_instance_id  = service_instance_data.getInstanceId()
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id  
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire storm endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  storm_instance_name = service_instance_data.getVariables().get("instance_name")
  variables.put("storm_instance_id", service_instance_data.getInstanceId())
  variables.put("storm_endpoint", service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue())
  println(variables.get("storm_endpoint"))
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>