<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="MLOps_Model_Server_Inference_Test_Continuous" tags="MLOps,Dashboard,Model Management,Model Deployment,Model Monitoring,Triton,Service,Service Automation" onTaskError="continueJobExecution" priority="normal" projectName="3. MLOps Model Server Workflows" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="false" description="GRPC inference url of the model server (e.g. localhost:8001)." group="Model Server" hidden="false" model="PA:REGEXP((([0-9]+(\\.[0-9]+){3})|([0-9a-zA-Z-]+([\\.0-9a-zA-Z-]+){2})):[0-9]+)?" name="GRPC_INFERENCE_URL" value=""/>
    <variable advanced="false" description="Name of the model to be tested." group="Model Server" hidden="false" model="PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)" name="MODEL_NAME" value="simple"/>
    <variable advanced="false" description="Handler for image path" group="Model Server" hidden="true" model="PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'densenet_onnx' || variables['MODEL_NAME'].toLowerCase() == 'inception_graphdef' ? showVar('IMAGE_PATH') : hideVar('IMAGE_PATH'))" name="IMAGE_PATH_HANDLER" value=""/>
    <variable advanced="false" description="Path of the image to be used for inference." group="Model Server" hidden="false" model="PA:URL" name="IMAGE_PATH" value="https://activeeon-public.s3.eu-west-2.amazonaws.com/images/bee.jpg"/>
    <variable advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(,nvcr.io/nvidia/tritonserver:22.10-py3-sdk)" name="CONTAINER_IMAGE" value="nvcr.io/nvidia/tritonserver:22.10-py3-sdk"/>
  </variables>
  <description>
    <![CDATA[ Simple workflow for MaaS Server inference test. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-mlops-dashboard"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
<info name="Documentation" value="PAIO/PAIOUserGuide.html"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="MLOps_Model_Server_Inference_Test" runAsMe="true">
      <description>
        <![CDATA[ Simple task for MaaS Server inference test. ]]>
      </description>
      <variables>
        <variable advanced="false" hidden="false" inherited="false" name="TASK_FILE_PATH" value="main.py"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html"/>
        <info name="PRE_SCRIPT_AS_FILE" value="$TASK_FILE_PATH"/>
      </genericInformation>
      <depends>
        <task ref="Start"/>
      </depends>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_source_name/raw"/>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[
import os
import sys
import argparse
import uuid
import queue
import gevent.ssl
import numpy as np

from PIL import Image
from PIL.Image import Resampling

import tritonclient.http as httpclient
import tritonclient.grpc as grpcclient
import grpc

import tritonclient.grpc.model_config_pb2 as mc

from tritonclient.grpc import service_pb2, service_pb2_grpc
from tritonclient.utils import InferenceServerException
from tritonclient.utils import triton_to_np_dtype

from tritonclient import utils


def test_model_simple(args, triton_client):
    # Infer
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "INT32"))
    inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "INT32"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    input0_data = np.arange(start=0, stop=16, dtype=np.int32)
    input0_data = np.expand_dims(input0_data, axis=0)
    input1_data = np.ones(shape=(1, 16), dtype=np.int32)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Test with outputs
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    # print("response:\n", results.get_response())
    statistics = triton_client.get_inference_statistics(model_name=args.model_name)
    # print("statistics:\n", statistics)
    if len(statistics.model_stats) != 1:
        print("FAILED: Inference Statistics")
        sys.exit(1)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    print("model-outputs:")
    for i in range(16):
        print(str(input0_data[0][i]) + " + " + str(input1_data[0][i]) + " = " + str(output0_data[0][i]))
        print(str(input0_data[0][i]) + " - " + str(input1_data[0][i]) + " = " + str(output1_data[0][i]))
        if (input0_data[0][i] + input1_data[0][i]) != output0_data[0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[0][i] - input1_data[0][i]) != output1_data[0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple')


def test_model_simple_identity(args, triton_client):
    # Example using BYTES input tensor with utf-8 encoded string that
    # has an embedded null character.
    # null_chars_array = np.array(["he\x00llo".encode('utf-8') for i in range(16)], dtype=np.object_)
    # null_char_data = null_chars_array.reshape([1, 16])
    null_chars_array = np.array(["he\x00llo".encode('utf-8') for i in range(1)], dtype=np.object_)
    null_char_data = null_chars_array.reshape([1, 1])
    # Example using BYTES input tensor with 16 elements, where each
    # element is a 4-byte binary blob with value 0x00010203. Can use dtype=np.bytes_ in this case.
    # bytes_data = [b'\x00\x01\x02\x03' for i in range(16)]
    # np_bytes_data = np.array(bytes_data, dtype=np.bytes_)
    # np_bytes_data = np_bytes_data.reshape([1, 16])
    np_array = null_char_data
    # np_array = np_bytes_data
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', np_array.shape, "BYTES"))
    inputs[0].set_data_from_numpy(np_array)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    if (np_array.dtype == np.object_):
        print(results.as_numpy('OUTPUT0'))
        if not np.array_equal(np_array, results.as_numpy('OUTPUT0')):
            print(results.as_numpy('OUTPUT0'))
            print("error: incorrect output")
            sys.exit(1)
    else:
        encoded_results = np.char.encode(results.as_numpy('OUTPUT0').astype(str))
        print(encoded_results)
        if not np.array_equal(np_array, encoded_results):
            print("error: incorrect output")
            sys.exit(1)
    print('PASS: simple_identity')


def test_model_simple_int8(args, triton_client):
    # We use a simple model that takes 2 input tensors of 16 integers
    # each and returns 2 output tensors of 16 integers each. One
    # output tensor is the element-wise sum of the inputs and one
    # output is the element-wise difference.
    # model_name = "simple_int8"
    model_version = ""
    batch_size = 1
    # Create gRPC stub for communicating with the server
    channel = grpc.insecure_channel(args.model_server)
    grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)
    # Generate the request
    request = service_pb2.ModelInferRequest()
    request.model_name = args.model_name
    request.model_version = model_version
    # Input data
    input0_data = [i for i in range(16)]
    input1_data = [1 for i in range(16)]
    # Populate the inputs in inference request
    input0 = service_pb2.ModelInferRequest().InferInputTensor()
    input0.name = "INPUT0"
    input0.datatype = "INT8"
    input0.shape.extend([1, 16])
    input0.contents.int_contents[:] = input0_data
    input1 = service_pb2.ModelInferRequest().InferInputTensor()
    input1.name = "INPUT1"
    input1.datatype = "INT8"
    input1.shape.extend([1, 16])
    input1.contents.int_contents[:] = input1_data
    request.inputs.extend([input0, input1])
    # Populate the outputs in the inference request
    output0 = service_pb2.ModelInferRequest().InferRequestedOutputTensor()
    output0.name = "OUTPUT0"
    output1 = service_pb2.ModelInferRequest().InferRequestedOutputTensor()
    output1.name = "OUTPUT1"
    request.outputs.extend([output0, output1])
    response = grpc_stub.ModelInfer(request)
    output_results = []
    index = 0
    for output in response.outputs:
        shape = []
        for value in output.shape:
            shape.append(value)
        output_results.append(
            np.frombuffer(response.raw_output_contents[index], dtype=np.int8))
        output_results[-1] = np.resize(output_results[-1], shape)
        index += 1
    if len(output_results) != 2:
        print("expected two output results")
        sys.exit(1)
    for i in range(16):
        print(str(input0_data[i]) + " + " + str(input1_data[i]) + " = " + str(output_results[0][0][i]))
        print(str(input0_data[i]) + " - " + str(input1_data[i]) + " = " + str(output_results[1][0][i]))
        if (input0_data[i] + input1_data[i]) != output_results[0][0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[i] - input1_data[i]) != output_results[1][0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple_int8')


def test_model_simple_sequence(args, triton_client):
    # UserData class
    class UserData:
        def __init__(self):
            self._completed_requests = queue.Queue()
    # sync_send function
    def sync_send(triton_client, result_list, values, batch_size, sequence_id, model_name, model_version):
        count = 1
        for value in values:
            # Create the tensor for INPUT
            value_data = np.full(shape=[batch_size, 1], fill_value=value, dtype=np.int32)
            inputs = []
            inputs.append(grpcclient.InferInput('INPUT', value_data.shape, "INT32"))
            # Initialize the data
            inputs[0].set_data_from_numpy(value_data)
            outputs = []
            outputs.append(grpcclient.InferRequestedOutput('OUTPUT'))
            # Issue the synchronous sequence inference.
            result = triton_client.infer(model_name=model_name,
                                        inputs=inputs,
                                        outputs=outputs,
                                        sequence_id=sequence_id,
                                        sequence_start=(count == 1),
                                        sequence_end=(count == len(values)))
            result_list.append(result.as_numpy('OUTPUT'))
            count = count + 1
    # We use custom "sequence" models which take 1 input
    # value. The output is the accumulated value of the inputs. See
    # src/custom/sequence.
    offset = 0
    dyna = False
    int_sequence_model_name = "simple_dyna_sequence" if dyna else "simple_sequence"
    string_sequence_model_name = "simple_string_dyna_sequence" if dyna else "simple_sequence"
    model_version = ""
    batch_size = 1
    # values = [11, 7, 5, 3, 2, 0, 1]
    values = [11] # 6 inferences
    # Will use two sequences and send them synchronously. Note the
    # sequence IDs should be non-zero because zero is reserved for
    # non-sequence requests.
    int_sequence_id0 = 1000 + offset * 2
    int_sequence_id1 = 1001 + offset * 2
    # For string sequence IDs, the dyna backend requires that the
    # sequence id be decodable into an integer, otherwise we'll use
    # a UUID4 sequence id and a model that doesn't require corrid
    # control.
    string_sequence_id0 = str(1002) if dyna else str(uuid.uuid4())
    int_result0_list = []
    int_result1_list = []
    string_result0_list = []
    user_data = UserData()
    try:
        sync_send(triton_client, int_result0_list, 
                  values, batch_size,
                  int_sequence_id0, int_sequence_model_name, model_version)
        # sync_send(triton_client, int_result0_list, 
        #           [0] + values, batch_size,
        #           int_sequence_id0, int_sequence_model_name, model_version)
        # sync_send(triton_client, int_result1_list,
        #           [100] + [-1 * val for val in values], batch_size,
        #           int_sequence_id1, int_sequence_model_name, model_version)
        # sync_send(triton_client, string_result0_list,
        #           [20] + [-1 * val for val in values], batch_size,
        #           string_sequence_id0, string_sequence_model_name,
        #           model_version)
    except InferenceServerException as error:
        print(error)
        sys.exit(1)
    for i in range(len(int_result0_list)):
        int_seq0_expected = 1 if (i == 0) else values[i - 1]
        int_seq1_expected = 101 if (i == 0) else values[i - 1] * -1
        # For string sequence ID we are testing two different backends
        if i == 0 and dyna:
            string_seq0_expected = 20
        elif i == 0 and not dyna:
            string_seq0_expected = 21
        elif i != 0 and dyna:
            string_seq0_expected = values[i - 1] * -1 + int(
                string_result0_list[i - 1][0][0])
        else:
            string_seq0_expected = values[i - 1] * -1
        # The dyna_sequence custom backend adds the correlation ID
        # to the last request in a sequence.
        if dyna and (i != 0) and (values[i - 1] == 1):
            int_seq0_expected += int_sequence_id0
            # int_seq1_expected += int_sequence_id1
            # string_seq0_expected += int(string_sequence_id0)
        # print("[" + str(i) + "] " + str(int_result0_list[i][0][0]) + " : " +
        #       str(int_result1_list[i][0][0]) + " : " +
        #       str(string_result0_list[i][0][0]))
        print("[" + str(i) + "] " + str(int_result0_list[i][0][0]))
        # if ((int_seq0_expected != int_result0_list[i][0][0]) or
        #     (int_seq1_expected != int_result1_list[i][0][0]) or
        #     (string_seq0_expected != string_result0_list[i][0][0])):
        # if ((int_seq0_expected != int_result0_list[i][0][0])):
        #     print("[ expected ] " + str(int_seq0_expected) + " : " +
        #           str(int_seq1_expected) + " : " + str(string_seq0_expected))
        #     sys.exit(1)
    print("PASS: simple_sequence")


def test_model_simple_string(args, triton_client):
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "BYTES"))
    inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "BYTES"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    in0 = np.arange(start=0, stop=16, dtype=np.int32)
    in0 = np.expand_dims(in0, axis=0)
    in1 = np.ones(shape=(1, 16), dtype=np.int32)
    expected_sum = np.add(in0, in1)
    expected_diff = np.subtract(in0, in1)
    # The 'simple_string' model expects 2 BYTES tensors where each
    # element in those tensors is the utf-8 string representation of
    # an integer. The BYTES tensors must be represented by a numpy
    # array with dtype=np.object_.
    in0n = np.array([str(x).encode('utf-8') for x in in0.reshape(in0.size)], dtype=np.object_)
    input0_data = in0n.reshape(in0.shape)
    in1n = np.array([str(x).encode('utf-8') for x in in1.reshape(in1.size)], dtype=np.object_)
    input1_data = in1n.reshape(in1.shape)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Make inference
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    for i in range(16):
        print(str(input0_data[0][i]) + " + " + str(input1_data[0][i]) + " = " + str(output0_data[0][i]))
        print(str(input0_data[0][i]) + " - " + str(input1_data[0][i]) + " = " + str(output1_data[0][i]))
        # Convert result from string to int to check result
        r0 = int(output0_data[0][i])
        r1 = int(output1_data[0][i])
        if expected_sum[0][i] != r0:
            print("error: incorrect sum")
            sys.exit(1)
        if expected_diff[0][i] != r1:
            print("error: incorrect difference")
            sys.exit(1)
    print("PASS: simple_string")


def test_model_image_classification(args, triton_client):
    def parse_model(model_metadata, model_config):
        """
        Check the configuration of a model to make sure it meets the
        requirements for an image classification network (as expected by
        this client)
        """
        if len(model_metadata.inputs) != 1:
            raise Exception("expecting 1 input, got {}".format(
                len(model_metadata.inputs)))
        if len(model_metadata.outputs) != 1:
            raise Exception("expecting 1 output, got {}".format(
                len(model_metadata.outputs)))
        if len(model_config.input) != 1:
            raise Exception(
                "expecting 1 input in model configuration, got {}".format(
                    len(model_config.input)))
        input_metadata = model_metadata.inputs[0]
        input_config = model_config.input[0]
        output_metadata = model_metadata.outputs[0]
        if output_metadata.datatype != "FP32":
            raise Exception("expecting output datatype to be FP32, model '" +
                            model_metadata.name + "' output type is " +
                            output_metadata.datatype)
        # Output is expected to be a vector. But allow any number of
        # dimensions as long as all but 1 is size 1 (e.g. { 10 }, { 1, 10
        # }, { 10, 1, 1 } are all ok). Ignore the batch dimension if there
        # is one.
        output_batch_dim = (model_config.max_batch_size > 0)
        non_one_cnt = 0
        for dim in output_metadata.shape:
            if output_batch_dim:
                output_batch_dim = False
            elif dim > 1:
                non_one_cnt += 1
                if non_one_cnt > 1:
                    raise Exception("expecting model output to be a vector")
        # Model input must have 3 dims, either CHW or HWC (not counting
        # the batch dimension), either CHW or HWC
        input_batch_dim = (model_config.max_batch_size > 0)
        expected_input_dims = 3 + (1 if input_batch_dim else 0)
        if len(input_metadata.shape) != expected_input_dims:
            raise Exception(
                "expecting input to have {} dimensions, model '{}' input has {}".
                format(expected_input_dims, model_metadata.name,
                    len(input_metadata.shape)))
        if type(input_config.format) == str:
            FORMAT_ENUM_TO_INT = dict(mc.ModelInput.Format.items())
            input_config.format = FORMAT_ENUM_TO_INT[input_config.format]
        if ((input_config.format != mc.ModelInput.FORMAT_NCHW) and
            (input_config.format != mc.ModelInput.FORMAT_NHWC)):
            raise Exception("unexpected input format " +
                            mc.ModelInput.Format.Name(input_config.format) +
                            ", expecting " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NCHW) +
                            " or " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NHWC))
        if input_config.format == mc.ModelInput.FORMAT_NHWC:
            h = input_metadata.shape[1 if input_batch_dim else 0]
            w = input_metadata.shape[2 if input_batch_dim else 1]
            c = input_metadata.shape[3 if input_batch_dim else 2]
        else:
            c = input_metadata.shape[1 if input_batch_dim else 0]
            h = input_metadata.shape[2 if input_batch_dim else 1]
            w = input_metadata.shape[3 if input_batch_dim else 2]
        return (model_config.max_batch_size, input_metadata.name,
                output_metadata.name, c, h, w, input_config.format,
                input_metadata.datatype)
    def preprocess(img, format, dtype, c, h, w, scaling):
        """
        Pre-process an image to meet the size, type and format
        requirements specified by the parameters.
        """
        # np.set_printoptions(threshold='nan')
        if c == 1:
            sample_img = img.convert('L')
        else:
            sample_img = img.convert('RGB')
        resized_img = sample_img.resize((w, h), Resampling.BILINEAR) # Image.BILINEAR will be removed in Pillow 10 (2023-07-01)
        resized = np.array(resized_img)
        if resized.ndim == 2:
            resized = resized[:, :, np.newaxis]
        npdtype = triton_to_np_dtype(dtype)
        typed = resized.astype(npdtype)
        if scaling == 'INCEPTION':
            scaled = (typed / 127.5) - 1
        elif scaling == 'VGG':
            if c == 1:
                scaled = typed - np.asarray((128,), dtype=npdtype)
            else:
                scaled = typed - np.asarray((123, 117, 104), dtype=npdtype)
        else:
            scaled = typed
        # Swap to CHW if necessary
        if format == mc.ModelInput.FORMAT_NCHW:
            ordered = np.transpose(scaled, (2, 0, 1))
        else:
            ordered = scaled
        # Channels are in RGB order. Currently model configuration data
        # doesn't provide any information as to other channel orderings
        # (like BGR) so we just assume RGB.
        return ordered
    class UserData:
        def __init__(self):
            self._completed_requests = queue.Queue()
    def requestGenerator(client, batched_image_data, input_name, output_name, dtype, args):
        # Set the input data
        inputs = [grpcclient.InferInput(input_name, batched_image_data.shape, dtype)]
        inputs[0].set_data_from_numpy(batched_image_data)
        outputs = [
            grpcclient.InferRequestedOutput(output_name, class_count=args.classes)
        ]
        yield inputs, outputs, args.model_name, args.model_version
    def postprocess(results, output_name, batch_size, supports_batching):
        """
        Post-process results to show classifications.
        """
        output_array = results.as_numpy(output_name)
        if supports_batching and len(output_array) != batch_size:
            raise Exception("expected {} results, got {}".format(batch_size, len(output_array)))
        # Include special handling for non-batching models
        for results in output_array:
            if not supports_batching:
                results = [results]
            for result in results:
                if output_array.dtype.type == np.object_:
                    cls = "".join(chr(x) for x in result).split(':')
                else:
                    cls = result.split(':')
                print("    {} ({}) = {}".format(cls[0], cls[1], cls[2]))
    # Make sure the model matches our requirements, and get some
    # properties of the model that we need for preprocessing
    try:
        model_metadata = triton_client.get_model_metadata(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the metadata: " + str(e))
        sys.exit(1)
    # print('-'*30, "\nmodel_metadata:\n", model_metadata, '-'*30)
    try:
        model_config = triton_client.get_model_config(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the config: " + str(e))
        sys.exit(1)
    # print('-'*30, "\nmodel_config:\n", model_config, '-'*30)
    model_config = model_config.config
    max_batch_size, input_name, output_name, c, h, w, format, dtype = parse_model(model_metadata, model_config)
    supports_batching = max_batch_size > 0
    if not supports_batching and args.batch_size != 1:
        print("ERROR: This model doesn't support batching.")
        sys.exit(1)
    if args.image_path is None:
        print("ERROR: This model requires an input image.")
        sys.exit(1)
    filenames = []
    if os.path.isdir(args.image_path):
        filenames = [
            os.path.join(args.image_path, f)
            for f in os.listdir(args.image_path)
            if os.path.isfile(os.path.join(args.image_path, f))
        ]
    else:
        filenames = [
            args.image_path,
        ]
    assert(len(filenames) > 0)
    filenames.sort()
    # print("filenames:\n", filenames)
    # Preprocess the images into input data according to model
    # requirements
    image_data = []
    for filename in filenames:
        img = Image.open(filename)
        image_data.append(preprocess(img, format, dtype, c, h, w, args.scaling))
    # Send requests of args.batch_size images. If the number of
    # images isn't an exact multiple of args.batch_size then just
    # start over with the first images until the batch is filled.
    requests = []
    responses = []
    result_filenames = []
    request_ids = []
    image_idx = 0
    last_request = False
    user_data = UserData()
    # Holds the handles to the ongoing HTTP async requests.
    async_requests = []
    sent_count = 0
    # if args.streaming:
    #     triton_client.start_stream(partial(completion_callback, user_data))
    while not last_request:
        input_filenames = []
        repeated_image_data = []
        for idx in range(args.batch_size):
            input_filenames.append(filenames[image_idx])
            repeated_image_data.append(image_data[image_idx])
            image_idx = (image_idx + 1) % len(image_data)
            if image_idx == 0:
                last_request = True
        if supports_batching:
            batched_image_data = np.stack(repeated_image_data, axis=0)
        else:
            batched_image_data = repeated_image_data[0]
        # Send request
        try:
            for inputs, outputs, model_name, model_version in requestGenerator(
                    triton_client, batched_image_data, input_name, output_name, dtype, args):
                sent_count += 1
                responses.append(
                        triton_client.infer(args.model_name,
                                            inputs,
                                            request_id=str(sent_count),
                                            model_version=args.model_version,
                                            outputs=outputs))
                # if args.streaming:
                #     triton_client.async_stream_infer(
                #         args.model_name,
                #         inputs,
                #         request_id=str(sent_count),
                #         model_version=args.model_version,
                #         outputs=outputs)
                # elif args.async_set:
                #     if args.protocol.lower() == "grpc":
                #         triton_client.async_infer(
                #             args.model_name,
                #             inputs,
                #             partial(completion_callback, user_data),
                #             request_id=str(sent_count),
                #             model_version=args.model_version,
                #             outputs=outputs)
                #     else:
                #         async_requests.append(
                #             triton_client.async_infer(
                #                 args.model_name,
                #                 inputs,
                #                 request_id=str(sent_count),
                #                 model_version=args.model_version,
                #                 outputs=outputs))
                # else:
                #     responses.append(
                #         triton_client.infer(args.model_name,
                #                             inputs,
                #                             request_id=str(sent_count),
                #                             model_version=args.model_version,
                #                             outputs=outputs))
        except InferenceServerException as e:
            print("inference failed: " + str(e))
            # if args.streaming:
            #     triton_client.stop_stream()
            sys.exit(1)
    # if args.streaming:
    #     triton_client.stop_stream()
    # if args.protocol.lower() == "grpc":
    #     if args.streaming or args.async_set:
    #         processed_count = 0
    #         while processed_count < sent_count:
    #             (results, error) = user_data._completed_requests.get()
    #             processed_count += 1
    #             if error is not None:
    #                 print("inference failed: " + str(error))
    #                 sys.exit(1)
    #             responses.append(results)
    # else:
    #     if args.async_set:
    #         # Collect results from the ongoing async requests
    #         # for HTTP Async requests.
    #         for async_request in async_requests:
    #             responses.append(async_request.get_result())
    for response in responses:
        this_id = response.get_response().id
        # if args.protocol.lower() == "grpc":
        #     this_id = response.get_response().id
        # else:
        #     this_id = response.get_response()["id"]
        print("Request {}, batch size {}".format(this_id, args.batch_size))
        postprocess(response, output_name, args.batch_size, supports_batching)
    print("PASS: " + args.model_name)


def main(args):
    print("model-server: ", args.model_server)
    print("model-name:   ", args.model_name)
    try:
        # triton_client = httpclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
        triton_client = grpcclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
    except Exception as e:
        print("channel creation failed: " + str(e))
        sys.exit(1)
    if args.model_name == "simple":
        test_model_simple(args, triton_client)
    elif args.model_name == "simple_identity":
        test_model_simple_identity(args, triton_client)
    elif args.model_name == "simple_int8":
        test_model_simple_int8(args, triton_client)
    elif args.model_name == "simple_sequence":
        test_model_simple_sequence(args, triton_client)
    elif args.model_name == "simple_string":
        test_model_simple_string(args, triton_client)
    elif args.model_name == "inception_graphdef" or args.model_name == "densenet_onnx":
        test_model_image_classification(args, triton_client)
    else:
        print("Invalid model name: " + args.model_name)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-s',
                        '--model-server',
                        type=str,
                        required=True,
                        help='Name of the model server.')
    parser.add_argument('-n',
                        '--model-name',
                        type=str,
                        required=True,
                        help='Name of the deployed model to be tested.')
    parser.add_argument('-x',
                        '--model-version',
                        type=str,
                        required=False,
                        default="",
                        help='Version of model. Default is to use latest version.')
    parser.add_argument('-v',
                        '--verbose',
                        action="store_true",
                        required=False,
                        default=False,
                        help='Enable verbose output.')
    parser.add_argument('-b',
                        '--batch-size',
                        type=int,
                        required=False,
                        default=1,
                        help='Batch size. Default is 1.')
    parser.add_argument('-l',
                        '--scaling',
                        type=str,
                        choices=['NONE', 'INCEPTION', 'VGG'],
                        required=False,
                        default='INCEPTION',
                        help='Type of scaling to apply to image pixels. Default is INCEPTION (only for image-based models).')
    parser.add_argument('-i',
                        '--image_path',
                        type=str,
                        nargs='?',
                        default=None,
                        help='Input image / Input folder (mandatory for image-based models).')
    parser.add_argument('-c',
                        '--classes',
                        type=int,
                        required=False,
                        default=3,
                        help='Number of class results to report (only for image-based models). Default is 3.')
    args = parser.parse_args()
    main(args)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
echo "---"
echo "Debugging info"
python -V
which python
pip -V
echo "---"
if [ $variables_MODEL_NAME = "inception_graphdef" ] || [ $variables_MODEL_NAME = "densenet_onnx" ]; then
	if [ -z "$variables_IMAGE_PATH" ]; then
		echo "IMAGE_PATH is empty!"
		exit -1
    else
		echo "Downloading $variables_IMAGE_PATH as image.png"
		echo "wget -O image.png $variables_IMAGE_PATH"
		wget -O image.png $variables_IMAGE_PATH
		echo "python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME -i image.png"
		python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME -i image.png
    fi
else
	echo "python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME"
	python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME
fi
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            205.16334533691406
        </positionTop>
        <positionLeft>
            413.3735656738281
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Wait_for_Signal" runAsMe="true">
      <description>
        <![CDATA[ A template task that sends a ready notification for all the signals specified in the variable SIGNALS, then loops until one signal among those specified is received by the job. ]]>
      </description>
      <variables>
        <variable advanced="false" description="The list of comma-separated signals expected by this task." hidden="false" inherited="false" name="SIGNALS" value="Terminate_Job,New_Inference_Request"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/signal-wait.png"/>
        <info name="TASK.DOCUMENTATION" value="user/ProActiveUserGuide.html#_task_signal_api"/>
      </genericInformation>
      <depends>
        <task ref="MLOps_Model_Server_Inference_Test"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import com.google.common.base.Splitter;
import org.ow2.proactive.scheduler.common.job.JobVariable;

public static boolean containsIgnoreCase(String str, String searchStr) {
    if(str == null || searchStr == null) return false;
    final int length = searchStr.length();
    if (length == 0)
        return true;
    for (int i = str.length() - length; i >= 0; i--) {
        if (str.regionMatches(true, i, searchStr, 0, length))
            return true;
    }
    return false;
}

if (variables.get("PA_TASK_ITERATION_SIGNAL") == null) {
	variables.put("PA_TASK_ITERATION_SIGNAL", 0)
}

if (variables.get("PA_TASK_ITERATION_SIGNAL") == 0) {
    List <JobVariable> signalVariables = new java.util.ArrayList<JobVariable>()
    signalVariables.add(new JobVariable("IMAGE_PATH", "https://activeeon-public.s3.eu-west-2.amazonaws.com/images/bee.jpg", "PA:URL", "Path of the image to be used for inference.", "", false, false))
    signalVariables.add(new JobVariable("IMAGE_PATH_HANDLER", "", "PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'densenet_onnx' || variables['MODEL_NAME'].toLowerCase() == 'inception_graphdef' ? showVar('IMAGE_PATH') : hideVar('IMAGE_PATH'))", "Handler for IMAGE_PATH variable.", "", false, true))
    signalVariables.add(new JobVariable("MODEL_NAME", "simple", "PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)", "Name of the model to be tested.", "", false, false))

    // Read the variable SIGNALS
    signals = variables.get("SIGNALS")

    // Split the value of the variable SIGNALS and transform it into a list
    Set signalsSet = new HashSet<>(Splitter.on(',').trimResults().omitEmptyStrings().splitToList(signals))

    // Send a ready notification for each signal in the set with updated variables
    println("Ready for signals "+ signalsSet)
    signalsSet.each{ signal ->
        if(signal.equals("Terminate_Job")) {
            signalapi.readyForSignal(signal);
        } else {
            signalapi.readyForSignal(signal, signalVariables)
        }
    }

    // Add the signals set as a variable to be used by next tasks
    variables.put("SIGNALS_SET", signalsSet)
}
variables.put("PA_TASK_ITERATION_SIGNAL", variables.get("PA_TASK_ITERATION_SIGNAL")+1)

//Read the variable SIGNALS_SET
Set signalsSet = variables.get("SIGNALS_SET")

// Check whether one signal among those specified as input is received
println("Checking whether one signal in the set "+ signalsSet +" is received")
receivedSignals = signalapi.checkForSignals(signalsSet)

// If a signal is received, remove ready signals and break the loop
// If a signal is received, remove ready signals and break the loop, else sleep 10 seconds then restart
if (receivedSignals != null && !receivedSignals.isEmpty()) {
    // remove ready signals
    signalapi.removeManySignals(new HashSet<>(signalsSet.collect { signal -> "ready_"+signal }))
    
    // Print the received signal
    println("Received signals: " + receivedSignals.toString())
    terminate = false
    for (String s : Arrays.asList(receivedSignals.keySet())) {
    	if (containsIgnoreCase(s, "Terminate_Job")) {
            println("Terminate_Job")
            result = "Terminate_Job"
            terminate = true
        }
        if (containsIgnoreCase(s, "New_Inference_Request") && !terminate) {
            println("New_Inference_Request")
            updatedVariables = receivedSignals.get("New_Inference_Request")
    		variables.put("MODEL_NAME", updatedVariables.get("MODEL_NAME"))
            variables.put("IMAGE_PATH", updatedVariables.get("IMAGE_PATH"))
            result = "New_Inference_Request"
        }
    }
} else {
    result = null
}

variables.put("RECEIVED_SIGNAL", result)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow>
        <loop target="Wait_for_Signal">
          <script>
            <code language="groovy">
              <![CDATA[
if (result!=null){
    loop = false
} else {
    loop = "* * * * *"
}
]]>
            </code>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
            321.17185974121094
        </positionTop>
        <positionLeft>
            446.1079406738281
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Start" runAsMe="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_loop"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="javascript">
            <![CDATA[
print('Loop block start ' + variables.get('PA_TASK_ITERATION'))
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="start"/>
      <metadata>
        <positionTop>
            32.18040466308594
        </positionTop>
        <positionLeft>
            603.3593444824219
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Loop" runAsMe="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
      </genericInformation>
      <depends>
        <task ref="Wait_for_Signal"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="javascript">
            <![CDATA[
variables.remove("PA_TASK_ITERATION_SIGNAL")
print('Loop block end ' + variables.get('PA_TASK_ITERATION'))
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="end">
        <loop target="Start">
          <script>
            <code language="javascript">
              <![CDATA[
if("New_Inference_Request".equalsIgnoreCase(variables.get('RECEIVED_SIGNAL'))) {
    loop = true;
} else {
    loop = false;
}
]]>
            </code>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
            485.1775360107422
        </positionTop>
        <positionLeft>
            556.1079406738281
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:1944px;
            height:2501px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-27.180404663085938px;left:-408.3735656738281px"><div class="task ui-draggable _jsPlumb_endpoint_anchor_ active-task" id="jsPlumb_1_100" style="top: 205.164px; left: 413.381px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Simple task for MaaS Server inference test."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">MLOps_Model_Server_Inference_Test</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_103" style="top: 321.173px; left: 446.108px; z-index: 24;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A template task that sends a ready notification for all the signals specified in the variable SIGNALS, then loops until one signal among those specified is received by the job."><img src="/automation-dashboard/styles/patterns/img/wf-icons/signal-wait.png" width="20px">&nbsp;<span class="name">Wait_for_Signal</span></a></div><div class="task block-start ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_106" style="top: 32.1807px; left: 603.366px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Start</span></a></div><div class="task block-end ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_109" style="top: 485.178px; left: 556.115px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Loop</span></a></div><svg style="position:absolute;left:486px;top:71.5px" width="177.5" height="134" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 133 C -10 83 166.5 50 156.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M22.546634625000003,97.50250824999999 L43.06820315638805,92.22375602892123 L34.19189402616467,89.73154090483723 L35.29723581122529,80.57849662775658 L22.546634625000003,97.50250824999999" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M22.546634625000003,97.50250824999999 L43.06820315638805,92.22375602892123 L34.19189402616467,89.73154090483723 L35.29723581122529,80.57849662775658 L22.546634625000003,97.50250824999999" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:486px;top:244.5px" width="23.107940673828125" height="77.17329406738281" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 2.107940673828125 76.17329406738281 C 12.107940673828125 26.173294067382812 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M4.705079080078124,57.23839670800781 L10.644429366529774,36.898187314424206 L3.969394357662242,43.257739785344846 L-3.3362275561331898,37.63387203684009 L4.705079080078124,57.23839670800781" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M4.705079080078124,57.23839670800781 L10.644429366529774,36.898187314424206 L3.969394357662242,43.257739785344846 L-3.3362275561331898,37.63387203684009 L4.705079080078124,57.23839670800781" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:531.1346176002057px;top:270.6732940673828px" width="20.473323073622403" height="141" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 40 C -10 90 -10 -50 0 0 " transform="translate(19.973323073622403,50.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#316b31" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.4569999999999963,49.16001999999999 L-8.714346841294152,28.91537600442066 L-10.77778447022079,37.90104376767174 L-19.973323073622403,37.23616047464146 L-2.4569999999999963,49.16001999999999" class="" stroke="#316b31" fill="#316b31" transform="translate(19.973323073622403,50.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.4569999999999963,49.16001999999999 L-8.714346841294152,28.91537600442066 L-10.77778447022079,37.90104376767174 L-19.973323073622403,37.23616047464146 L-2.4569999999999963,49.16001999999999" class="" stroke="#316b31" fill="#316b31" transform="translate(19.973323073622403,50.5)"></path></svg><div class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_123" style="position: absolute; transform: translate(-50%, -50%); left: 543.108px; top: 340.673px;">loop</div><svg style="position:absolute;left:488.1079406738281px;top:360.6732940673828px" width="128.39205932617188" height="124.82670593261719" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 107.39205932617188 123.82670593261719 C 117.39205932617188 73.82670593261719 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M94.64932713085938,91.62581191308594 L84.47281735714655,73.03983201356148 L84.24654591305762,82.25659940964943 L75.10360485371005,83.44261323136324 L94.64932713085938,91.62581191308594" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M94.64932713085938,91.62581191308594 L84.47281735714655,73.03983201356148 L84.24654591305762,82.25659940964943 L75.10360485371005,83.44261323136324 L94.64932713085938,91.62581191308594" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:645.5px;top:71.5px" width="68" height="414" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 47 0 C 57 50 -10 363 0 413 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#316b31" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M40.983343749999996,89.97896875000002 L45.212937859598036,110.74217018109088 L39.08911793806357,103.85023071758587 L31.341675892012184,108.84794436915446 L40.983343749999996,89.97896875000002" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M40.983343749999996,89.97896875000002 L45.212937859598036,110.74217018109088 L39.08911793806357,103.85023071758587 L31.341675892012184,108.84794436915446 L40.983343749999996,89.97896875000002" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_132" style="position: absolute; transform: translate(-50%, -50%); left: 679px; top: 278px;">loop</div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 486.5px; top: 235px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 486.5px; top: 195px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 488.608px; top: 351.173px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 488.608px; top: 311.173px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint loop-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 541.108px; top: 311.173px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint loop-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 541.108px; top: 351.173px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 643px; top: 62px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint loop-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 693px; top: 62px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 596px; top: 515px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 596px; top: 475px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint loop-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 646px; top: 475px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
