<?xml version="1.0" encoding="UTF-8"?>
<job xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="urn:proactive:jobdescriptor:3.8"
	xsi:schemaLocation="urn:proactive:jobdescriptor:3.8 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.8/schedulerjob.xsd"
	name="Write read HDFS" projectName="Basic Big Data" priority="normal"
	onTaskError="continueJobExecution" maxNumberOfExecution="2">
	<variables>
		<variable name="hdfs_directory" value="/persons" />
		<variable name="service_instance_id" value="1" model="PA:LONG"/>
	</variables>
	<description>
    <![CDATA[ A workflow to submit a Spark job from a docker container, to write and read Persons from/to HDFS ]]>
  </description>
	<genericInformation>
		<info name="workflow.icon"
			value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" />
		<info name="Documentation"
			  value="http://activeeon.com/resources/activeeon-deploy-swarm-hdfs-spark.pdf" />
	</genericInformation>
	<taskFlow>
		<task name="submit_write_read_persons_HDFS">
			<genericInformation>
				<info name="task.icon"
					value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" />
			</genericInformation>
			<inputFiles>
				<files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
			</inputFiles>
			<forkEnvironment >
				<additionalClasspath>
					<pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
				</additionalClasspath>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="groovy">
            <![CDATA[
org.ow2.proactive.pca.service.client.ApiClient
org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
org.ow2.proactive.pca.service.client.model.ServiceInstanceData

// Retrieve variables
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def service_instance_id = variables.get("service_instance_id") as Long
def hdfs_directory = variables.get("hdfs_directory")

// Define other variables
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Get the service instance variables
def service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
def service_instance_variables = service_instance_data.getVariables()
def network_name = service_instance_variables.get("network_name")
def instance_name = service_instance_variables.get("instance_name")

// Spark job submission
def spark_submit_command = "spark-submit --deploy-mode client --class jobs.MapR_fs_job --num-executors 1 --driver-memory 600m --executor-cores 2 --queue default --master spark://" + instance_name + "-spark-master:7077 /MapR_fs_job-0.0.1-SNAPSHOT.jar hdfs://" + instance_name + "-namenode:9000" + hdfs_directory
cmd = ["docker", "run", "--rm", "--net", network_name, "activeeon/hdfs-spark:1.0", "/bin/sh", "-c", spark_submit_command]
cmd.execute().waitForProcessOutput(System.out, System.err)


//container_id="$(docker run --net=$variables_network -di activeeon/hdfs-spark:1.0 /bin/sh -c 'bash')"
//docker exec $container_id /bin/sh -c 'spark-submit --deploy-mode client --class jobs.MapR_fs_job --deploy-mode client --num-executors 1 --driver-memory 1g --executor-cores 2 --queue default --master '$variables_spark_master_url' /MapR_fs_job-0.0.1-SNAPSHOT.jar '$variables_hdfs_directory
//docker rm -f $container_id
]]>
					</code>
				</script>
			</scriptExecutable>
		</task>
	</taskFlow>
</job>