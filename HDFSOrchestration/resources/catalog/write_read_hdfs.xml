<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.11" xsi:schemaLocation="urn:proactive:jobdescriptor:3.11 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.11/schedulerjob.xsd"  name="Write_Read_HDFS" projectName="Basic Big Data" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2" >
  <variables>
    <variable name="hdfs_directory" value="/persons" />
    <variable name="service_instance_id" value="1" model="PA:LONG"/>
  </variables>
  <description>
    <![CDATA[ A workflow to submit a Spark job from a docker container, to write and read Persons from/to HDFS. This workflow requires a HDFS installation. ]]>
  </description>
  <genericInformation>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
    <info name="Documentation" value="https://ow2-proactive.github.io/proactive-examples/DockerSwarm/resources/doc/V1/activeeon-deploy-swarm-hdfs-spark.pdf"/>
  </genericInformation>
  <taskFlow>
    <task name="submit_write_read_persons_HDFS" >
      <description>
        <![CDATA[ A workflow to submit a Spark job from a docker container, to write and read Persons from/to HDFS. This workflow requires a HDFS installation. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData

// Retrieve variables
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def service_instance_id = variables.get("service_instance_id") as Long
def hdfs_directory = variables.get("hdfs_directory")

// Define other variables
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Get the service instance variables
def service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
def service_instance_variables = service_instance_data.getVariables()
def network_name = service_instance_variables.get("network_name")
def instance_name = service_instance_variables.get("instance_name")

// Spark job submission
def spark_submit_command = "spark-submit --deploy-mode client --class jobs.MapR_fs_job --num-executors 1 --driver-memory 600m --executor-cores 2 --queue default --master spark://" + instance_name + "-spark-master:7077 /MapR_fs_job-0.0.1-SNAPSHOT.jar hdfs://" + instance_name + "-namenode:9000" + hdfs_directory
cmd = ["docker", "run", "--rm", "--net", network_name, "activeeon/hdfs-spark:1.0", "/bin/sh", "-c", spark_submit_command]
cmd.execute().waitForProcessOutput(System.out, System.err)


//container_id="$(docker run --net=$variables_network -di activeeon/hdfs-spark:1.0 /bin/sh -c 'bash')"
//docker exec $container_id /bin/sh -c 'spark-submit --deploy-mode client --class jobs.MapR_fs_job --deploy-mode client --num-executors 1 --driver-memory 1g --executor-cores 2 --queue default --master '$variables_spark_master_url' /MapR_fs_job-0.0.1-SNAPSHOT.jar '$variables_hdfs_directory
//docker rm -f $container_id
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:1139px;
            height:566px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-333.9875030517578px;left:-430px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_326" style="top: 339px; left: 435px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" width="20px">&nbsp;<span class="name">submit_write_read_persons_HDFS</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 520px; top: 369px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
    </visualization>
  </metadata>
</job>