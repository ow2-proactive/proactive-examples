<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.13" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="MaaS_ML_Service_Start" onTaskError="continueJobExecution" priority="normal" projectName="1. MaaS_ML" xsi:schemaLocation="urn:proactive:jobdescriptor:3.13 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.13/schedulerjob.xsd">
  <variables>
    <variable advanced="true" description="The name of the Native Scheduler node source to use when the workflow tasks must be deployed inside a cluster such as SLURM, LSF, etc." group="Resource Management" hidden="false" name="NATIVE_SCHEDULER" value=""/>
    <variable advanced="true" description="The parameters given to the native scheduler (SLURM, LSF, etc) while requesting a ProActive node used to deploy the workflow tasks." group="Resource Management" hidden="false" name="NATIVE_SCHEDULER_PARAMS" value=""/>
    <variable advanced="true" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." group="Resource Management" hidden="false" name="NODE_ACCESS_TOKEN" value=""/>
    <variable advanced="true" description="The container platform to be used for executing the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(docker,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable advanced="true" description="The ervice instance name." group="MaaS_ML Service Configuration" hidden="false" name="MODEL_SERVICE_INSTANCE_NAME" value="maas-ml-${PA_JOB_ID}"/>
    <variable advanced="false" description="True if a proxy is needed to protect the access to this model-service endpoint. It takes by default the value of MODEL_SERVICE_PROXYFIED workflow variable." group="MaaS_ML Service Configuration" hidden="false" model="PA:Boolean" name="MODEL_SERVICE_PROXYFIED" value="True"/>
    <variable advanced="true" description="This entry script starts the service and defines the different functions to deploy the model, scores the prediction requests based on the deployed model, and returns the results. This script is specific to your model. It takes by default the value of MODEL_SERVICE_ENTRYPOINT workflow variable." group="MaaS_ML Service Configuration" hidden="false" name="MODEL_SERVICE_ENTRYPOINT" value="ml_service"/>
    <variable advanced="true" description="A YAML file that describes the OpenAPI Specification ver. 2 (known as Swagger Spec) of the service. This file should be stored in the catalog under the model_as_service_resources bucket." group="MaaS_ML Service Configuration" hidden="false" name="MODEL_SERVICE_YAML_FILE" value="ml_service-api"/>
    <variable advanced="false" description="A valid username having the needed privileges to execute this action. It takes by default the value of MODEL_SERVICE_USER_NAME workflow variable." group="MaaS_ML Service Configuration" hidden="false" name="MODEL_SERVICE_USER_NAME" value="user"/>
    <variable advanced="false" description="The name of the node where the service will be deployed. If empty, the service will be deployed on an available node selected randomly. It takes by default the value of MODEL_SERVICE_NODE_NAME workflow variable." group="Resource Management" hidden="false" name="MODEL_SERVICE_NODE_NAME" value=""/>
    <variable advanced="false" description="If True, the service will be configured to use the GPU and the Nvidia Rapids library." group="MaaS_ML Service Configuration" hidden="false" model="PA:Boolean" name="USE_NVIDIA_RAPIDS" value="False"/>
  </variables>
  <description>
    <![CDATA[ Start a server to deploy, update and call ML models. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="model-as-a-service"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_as_service.png"/>
<info name="NS" value="$NATIVE_SCHEDULER"/>
<info name="NODE_ACCESS_TOKEN" value="$NODE_ACCESS_TOKEN"/>
<info name="Documentation" value="PML/PMLUserGuide.html#_start_a_generic_service_instance"/>
<info name="NS_BATCH" value="$NATIVE_SCHEDULER_PARAMS"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="MaaS_ML_Service_Start" onTaskError="cancelJob">
      <description>
        <![CDATA[ Start a model server to deploy ML models. ]]>
      </description>
      <variables>
        <variable advanced="false" description="The name of the service. Please keep the default value for this variable." group="Service Parameters" hidden="false" inherited="false" name="SERVICE_ID" value="MaaS_ML"/>
        <variable advanced="false" description="The name of the service that will be deployed." group="Service Parameters" hidden="false" inherited="false" name="INSTANCE_NAME" value="$MODEL_SERVICE_INSTANCE_NAME"/>
        <variable advanced="false" description="The container platform that will be used (no container, docker, singularity, or podman)." group="Service Parameters" hidden="false" inherited="false" name="ENGINE" value="$CONTAINER_PLATFORM"/>
        <variable advanced="false" description="True if a proxy is needed to protect the access to this model-service endpoint. It takes by default the value of MODEL_SERVICE_PROXYFIED workflow variable." group="Service Parameters" hidden="false" inherited="false" name="PROXYFIED" value="$MODEL_SERVICE_PROXYFIED"/>
        <variable advanced="false" description="This entry script starts the service and defines the different functions to deploy the model, scores the prediction requests based on the deployed model, and returns the results. This script is specific to your model. It takes by default the value of MODEL_SERVICE_ENTRYPOINT workflow variable." group="MaaS_ML Service Configuration" hidden="false" inherited="false" name="PYTHON_ENTRYPOINT" value="$MODEL_SERVICE_ENTRYPOINT"/>
        <variable advanced="false" description="A YAML file that describes the OpenAPI Specification ver. 2 (known as Swagger Spec) of the service. This file should be stored in the catalog under the model_as_service_resources bucket.   It takes by default the value of MODEL_SERVICE_ENTRYPOINT workflow variable." group="MaaS_ML Service Configuration" hidden="false" inherited="false" name="YAML_FILE" value="$MODEL_SERVICE_YAML_FILE"/>
        <variable advanced="false" description="A valid username having the needed privileges to execute this action. It takes by default the value of MODEL_SERVICE_USER_NAME workflow variable." group="MaaS_ML Service Configuration" hidden="false" inherited="false" name="USER_NAME" value="$MODEL_SERVICE_USER_NAME"/>
        <variable advanced="false" description="The name of the node where the service will be deployed. If empty, the service will be deployed on an available node selected randomly. It takes by default the value of MODEL_SERVICE_NODE_NAME workflow variable." group="Resource Management" hidden="false" inherited="false" name="NODE_NAME" value="$MODEL_SERVICE_NODE_NAME"/>
        <variable advanced="false" description="The name of the Native Scheduler node source to use when the workflow tasks must be deployed inside a cluster such as SLURM, LSF, etc." group="Resource Management" hidden="true" inherited="true" name="NATIVE_SCHEDULER" value=""/>
        <variable advanced="false" description="The parameters given to the native scheduler (SLURM, LSF, etc) while requesting a ProActive node used to deploy the workflow tasks." group="Resource Management" hidden="true" inherited="true" name="NATIVE_SCHEDULER_PARAMS" value=""/>
        <variable advanced="false" description="If True, the service will be configured to use the GPU and the Nvidia Rapids library." group="Resource Management" hidden="false" inherited="false" name="GPU_ENABLED" value="$USE_NVIDIA_RAPIDS"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_as_service.png"/>
        <info name="task.documentation" value="PML/PMLUserGuide.html#_start_a_generic_service_instance"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Service_Start/raw">
            <arguments>
              <argument value="true"/>
              <argument value="NATIVE_SCHEDULER"/>
              <argument value="NATIVE_SCHEDULER_PARAMS"/>
              <argument value="ENGINE"/>
              <argument value="PROXYFIED"/>
              <argument value="PYTHON_ENTRYPOINT"/>
              <argument value="YAML_FILE"/>
              <argument value="NODE_NAME"/>
              <argument value="GPU_ENABLED"/>
              <argument value="DRIFT_ENABLED"/>
              <argument value="DRIFT_NOTIFICATION"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <post>
        <script>
          <file language="cpython" url="${PA_CATALOG_REST_URL}/buckets/model-as-a-service/resources/MaaS_Start_Post_Script/raw"/>
        </script>
      </post>
      <metadata>
        <positionTop>
            69.5625
        </positionTop>
        <positionLeft>
            141.1875
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2817px;
            height:4160px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-64.5625px;left:-136.1875px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_4" style="top: 69.5625px; left: 141.188px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Start a model server to deploy ML models."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_as_service.png" width="20px">&nbsp;<span class="name">MaaS_ML_Service_Start</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 204px; top: 100px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
