<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.13" xsi:schemaLocation="urn:proactive:jobdescriptor:3.13 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.13/schedulerjob.xsd"  name="Azure_HDInsight_Spark_Submit_Application" projectName="06. Azure HDInsight Spark Basic Tasks" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="SPARK_SSH_HOST" value="activeeon-spark-cluster-31-ssh.azurehdinsight.net" model="PA:NOT_EMPTY_STRING"/>
    <variable name="SPARK_SSH_PORT" value="22" model="PA:INTEGER"/>
    <variable name="SPARK_SSH_USER" value="act-usr" model="PA:NOT_EMPTY_STRING"/>
    <variable name="SPARK_SSH_PASSWORD" value="" model="PA:HIDDEN"/>
  </variables>
  <description>
    <![CDATA[ A workflow that uses SSH to execute a shell script in the head node of a HDInsight Spark Cluster.
The shell script is implemented as a pre-script of the task "submit_spark_app_with_ssh".
The provided sample shell script downloads the executable jar of Spark application examples (from a Amazon S3 public repository), then submits the application "SparkPi" to the cluster.
The workflow requires as input: (i) SPARK_SSH_HOST, the address of the head node of Spark cluster, (ii) SPARK_SSH_PORT, the SSH port used by the head node of Spark cluster, (iii) SPARK_SSH_USER and SPARK_SSH_PASSWORD, which are the SSH credentials to be used to access the head node of Spark cluster.
The required inputs can be obtained, for example, from the workflow "Azure_HDInsight_Spark_Workflow" (available in the big-data bucket), which deploys a HDInsight Spark cluster. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="big-data"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark_ssh.png"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="submit_spark_app_with_ssh" 
    
    
    
    
    fork="true">
      <description>
        <![CDATA[ A task that uses the tool "sshpass" to execute a shell script in a remote machine via SSH.
The pre-script of this task implements a shell script that downloads the executable jar of Spark application examples (from a Amazon S3 public repository), then submits the application "SparkPi" to Spark platform available in the machine.
The tool "sshpass" is used in native mode (if it is already installed in the machine), or inside a docker container (if it is not available in the machine). ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark_ssh.png"/>
        <info name="PRE_SCRIPT_AS_FILE" value="scripts/script.sh"/>
      </genericInformation>
      <pre>
        <script>
          <code language="bash">
            <![CDATA[
mkdir -p /tmp/spark-application
cd /tmp/spark-application
wget -nc https://activeeon-spark-sample-apps.s3.eu-west-3.amazonaws.com/spark-examples_2.11-2.4.6.4.1.4.8.jar
spark-submit --class org.apache.spark.examples.SparkPi --master yarn spark-examples_2.11-2.4.6.4.1.4.8.jar
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
# Get the SSH user and password to log into the remote machine
sshUser=$variables_SPARK_SSH_USER
sshPassword=$variables_SPARK_SSH_PASSWORD
sshHost=$variables_SPARK_SSH_HOST
sshPort=$variables_SPARK_SSH_PORT

# Prepare the script to execute in the remote machine
chmod a+x scripts/script.sh

# Printing the script
echo "============== The script to be executed =========================="
cat scripts/script.sh
echo "============================================================"
echo "Executing the script remotely in the machine "$sshHost

if ! command -v sshpass &> /dev/null
then
    echo "sshpass could not be found, using a sshpass docker container"
    
    # copy the script to the remote machine
    docker run --rm -i -v $(pwd)/scripts:/scripts ictu/sshpass -p $sshPassword scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -P $sshPort /scripts/script.sh $sshUser@$sshHost:/tmp/script.sh
    
    # execute the script in the remote machine
    docker run --rm -i -v $(pwd)/scripts:/scripts ictu/sshpass -p $sshPassword ssh  -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no $sshUser@$sshHost -p $sshPort  "/tmp/script.sh"
    
else
    echo "sshpass found"
    
    # copy the script to the remote machine
    sshpass -p $sshPassword scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -P $sshPort scripts/script.sh $sshUser@$sshHost:/tmp/script.sh
    
    # execute the script in the remote machine
    sshpass -p $sshPassword ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no $sshUser@$sshHost -p $sshPort "/tmp/script.sh"
fi
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            425
        </positionTop>
        <positionLeft>
            559.5
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2732px;
            height:3128px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-420px;left:-554.5px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable active-task" id="jsPlumb_1_610" style="top: 425px; left: 559.5px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="- A task that uses the tool &quot;sshpass&quot; to execute a shell script in a remote machine via SSH.
- The pre-script of this task implements a shell script that 
downloads the executable jar of Spark application examples (from a Amazon S3 public repository), then submits the application &quot;SparkPi&quot; to Spark platform available in the machine.
- The tool &quot;sshpass&quot; is used in native mode (if it is already installed in the machine), or inside a docker container (if it is not available in the machine)."><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark_ssh.png" width="20px">&nbsp;<span class="name">submit_spark_app_with_ssh</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 630px; top: 455px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
