<?xml version="1.0" encoding="UTF-8"?>
<job
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="Iterative_ETL" tags="S3,Big Data,HDFS,Data Streaming,Spark,ETL" projectName="10. ETL Workflows" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="DATABASE" value="demo" model="PA:NOT_EMPTY_STRING" description="name of the database" group="Database Connection" advanced="false" hidden="false"/>
    <variable name="HOST" value="xx" model="PA:NOT_EMPTY_STRING" description="hostname or IP address of the mysql server" group="Database Connection" advanced="false" hidden="false"/>
    <variable name="PORT" value="3306" model="PA:INTEGER" description="port of the mysql server" group="Database Connection" advanced="false" hidden="false"/>
    <variable name="USER" value="demo" model="PA:NOT_EMPTY_STRING" description="database user name" group="Database Connection" advanced="false" hidden="false"/>
    <variable name="CREDENTIALS_KEY" value="mysql://${USER}@${HOST}:${PORT}" model="PA:CREDENTIAL" description="third-party credential storing the database password" group="Database Connection" advanced="false" hidden="false"/>
    <variable name="spark_hdfs_service_instance_id" value="0" model="PA:INTEGER" description="id of the spark-hdfs instance deployed by ProActive Service Automation." group="" advanced="false" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ Iterative workflow performing ETL (Extract Transform Load) operations. For each iteration, data are retrieved from a csv file stored in Amazon S3, and pushed to a MySQL server. Then a Scala Spark task retrieves data from the MySQL server, transforms data, and stores data in a HDFS (Hadoop File System). If a cumulative count of rows in the HDFS is reached, the loop condition is broken and a web notification is sent. This workflow requires a full running Spark HDFS platform, with a running MySQL server. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="data-big-data"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/etl.png"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="MySQL_to_CSV"




          fork="true"
    >
      <description>
        <![CDATA[ This task allows to import data from MySQL database.
It requires the following third-party credential:  {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials. ]]>
      </description>
      <variables>
        <variable name="DATABASE" value="" inherited="true" model="PA:NOT_EMPTY_STRING" description="Name of the database" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="HOST" value="" inherited="true" model="PA:NOT_EMPTY_STRING" description="Hostname or IP address of the mysql server" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="PORT" value="" inherited="true" model="PA:INTEGER" description="Port of the mysql server" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="USER" value="" inherited="true" model="PA:NOT_EMPTY_STRING" description="Database user name" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="CREDENTIALS_KEY" value="mysql://${USER}@${HOST}:${PORT}" inherited="true" model="PA:CREDENTIAL" description="Third-party credential storing the database password" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="SQL_QUERY" value="select * from iris" inherited="false" model="PA:NOT_EMPTY_STRING" description="SQL query to execute" group="SQL Parameters" advanced="false" hidden="false"/>
        <variable name="OUTPUT_TYPE" value="CSV" inherited="false" model="PA:LIST(CSV,HTML)" description="Format of the output (CSV or HTML). If set to HTML, it allows to preview the results in Scheduler Portal in an HTML format. If set to CSV, OUTPUT_FILE must contain the relative file path in the Global Space of the output CSV file." group="Output Parameters" advanced="false" hidden="false"/>
        <variable name="OUTPUT_FILE" value="mysql_extract_job_${PA_JOB_ID}_iter_${PA_TASK_ITERATION}.csv" inherited="false"  description="relative file path in the Global Space where the CSV output file will be stored" group="Output Parameters" advanced="false" hidden="false"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:BOOLEAN" description="the task will be executed inside a docker container" group="Docker Parameters" advanced="true" hidden="false"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" description="name of the docker image used to execute the task" group="Docker Parameters" advanced="true" hidden="false"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Feed_MySQL"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw" language="groovy"></file>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/data-connectors/resources/ImportFromSqlDB/raw" language="cpython">
            <arguments>
              <argument value="mysql"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
      <metadata>
        <positionTop>
          558.078125
        </positionTop>
        <positionLeft>
          551.140625
        </positionLeft>
      </metadata>
    </task>
    <task name="Feed_MySQL"




          fork="true"
    >
      <description>
        <![CDATA[ This task allows to export data to MySQL database.
It requires the following third-party credential: {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials. ]]>
      </description>
      <variables>
        <variable name="DATABASE" value="" inherited="true" model="PA:NOT_EMPTY_STRING" description="Name of the database" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="HOST" value="" inherited="true" model="PA:NOT_EMPTY_STRING" description="Hostname or IP address of the mysql server" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="PORT" value="" inherited="true" model="PA:INTEGER" description="Port of the mysql server" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="USER" value="" inherited="true" model="PA:NOT_EMPTY_STRING" description="Database user name" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="CREDENTIALS_KEY" value="mysql://${USER}@${HOST}:${PORT}" inherited="true" model="PA:CREDENTIAL" description="Third-party credential storing the database password" group="Database Connection" advanced="true" hidden="false"/>
        <variable name="TABLE" value="iris" inherited="false" model="PA:NOT_EMPTY_STRING" description="Name of the database table where data will be inserted" group="SQL Parameters" advanced="false" hidden="false"/>
        <variable name="INSERT_MODE" value="append" inherited="false" model="PA:LIST(fail, replace, append)" description="Indicates the behavior to follow when the table exists in the database:&lt;br/&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;fail&lt;/u&gt;:&lt;/b&gt; If table exists, do nothing&lt;/li&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;replace&lt;/u&gt;:&lt;/b&gt; If table exists, drop it, recreate it, and insert data.&lt;/li&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;append&lt;/u&gt;:&lt;/b&gt; (default) If table exists, insert data. Create if does not exist.&lt;/li&gt;&lt;/ul&gt;" group="SQL Parameters" advanced="false" hidden="false"/>
        <variable name="INPUT_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/iris.csv" inherited="false" model="PA:NOT_EMPTY_STRING" description="The relative path in the Global Space of the CSV file that contains data to be imported. The string could also be a URL. Valid URL schemes include http, ftp, s3, and file." group="Input Parameters" advanced="false" hidden="false"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:BOOLEAN" description="The task will be executed inside a docker container" group="Docker Parameters" advanced="true" hidden="false"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" description="Name of the docker image used to execute the task" group="Docker Parameters" advanced="true" hidden="false"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Start"/>
      </depends>
      <inputFiles>
        <files  includes="$INPUT_FILE" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw" language="groovy"></file>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/data-connectors/resources/ExportToSqlDB/raw" language="cpython">
            <arguments>
              <argument value="mysql"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
          431.078125
        </positionTop>
        <positionLeft>
          616.140625
        </positionLeft>
      </metadata>
    </task>
    <task name="MySQL_to_Spark_to_HDFS"




          fork="true"
    >
      <description>
        <![CDATA[ A task which submits a Spark job from a docker container, to read/write files from/to HDFS. This task requires a Spark/HDFS platform deployed with ProActive Service Automation. ]]>
      </description>
      <variables>
        <variable name="MYSQL_TABLE" value="iris" inherited="false" model="PA:NOT_EMPTY_STRING" description="name of the database table to be loaded into a Spark DataFrame" group="SQL Parameters" advanced="false" hidden="false"/>
        <variable name="output_hdfs_csv_file_name" value="iris_PROCESSED.csv" inherited="false"  description="the csv file name under the HDFS root dir where Spark writes the DataFrame" group="Output Parameters" advanced="false" hidden="false"/>
      </variables>
      <genericInformation>
        <info name="PRE_SCRIPT_AS_FILE" value="script.scala"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$spark_token_name"/>
      </genericInformation>
      <depends>
        <task ref="Feed_MySQL"/>
      </depends>
      <inputFiles>
        <files  includes="mysql-connector-java-8.0.22.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <pre>
        <script>
          <code language="scalaw">
            <![CDATA[
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.types.DoubleType

import org.apache.spark.sql.SQLContext

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path

import java.nio.file.{Paths, Files}
import java.net.URI


object Application {

  def main() {
    val spark = SparkSession
          .builder
          .appName("HDFS-MySQL-Spark app")
          .getOrCreate()
    val sc = spark.sparkContext

    // Get args
    val args = sc.getConf.get("spark.driver.args").split("\\s+")
    val mysql_host = args(0)
    val mysql_port = args(1)
    val mysql_user = args(2)
    val mysql_psswd = args(3)
    val mysql_db_name = args(4)
    val mysql_table_name = args(5)
    val hdfs_url = args(6)
    val output_hdfs_csv_file_name = args(7)

    // MySQL to DF
    val sqlcontext = new org.apache.spark.sql.SQLContext(sc)
    val df = sqlcontext.read.format("jdbc").option("url", "jdbc:mysql://" + mysql_host + ":" + mysql_port + "/" + mysql_db_name ).option("driver", "com.mysql.jdbc.Driver").option("dbtable", mysql_table_name).option("user", mysql_user).option("password", mysql_psswd).load()


    // process DF
    val processed_df = df.withColumn("CopiedColumn",col("species"))
  	println("PROCESSED DF")
    processed_df.show()

    // Write DF to HDFS as CSV
    processed_df.write.mode(SaveMode.Overwrite).csv(hdfs_url + "/" + output_hdfs_csv_file_name)

  }
}

Application.main()
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
// Retrieve variables
def mysql_host = variables.get("HOST")
def mysql_port = variables.get("PORT")
def mysql_user = variables.get("USER")
def mysql_cred_keys = variables.get("CREDENTIALS_KEY")
def mysql_passwd = credentials.get(mysql_cred_keys)
def mysql_database = variables.get("DATABASE")
def mysql_table = variables.get("MYSQL_TABLE")
def spark_network_name = variables.get("spark_network_name")
def spark_master_url = variables.get("spark_master_url")
def hdfs_url = "hdfs://" + variables.get("hdfs_namenode_host_port")
def output_hdfs_csv_file_name = variables.get("output_hdfs_csv_file_name")

// Submit the Spark job
def spark_shell_command = "/usr/local/spark/bin/spark-shell --driver-memory 800m --executor-memory 800m --master " + spark_master_url + " --jars /usr/local/spark/jars/*,mysql-connector-java-8.0.22.jar -I /localspace/script.scala --conf spark.driver.args='" + mysql_host + " " + mysql_port + " " + mysql_user + " " + mysql_passwd + " " + mysql_database + " " + mysql_table + " " + hdfs_url + " " + output_hdfs_csv_file_name + "'"
cmd = ["docker", "run", "--rm", "--net", spark_network_name, "-w", "/localspace", "-v", localspace + ":/localspace", "activeeon/hdfs-spark:latest", "bash", "-c", spark_shell_command]
println cmd

cmd.execute().waitForProcessOutput(System.out, System.err)
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
          558.078125
        </positionTop>
        <positionLeft>
          679.140625
        </positionLeft>
      </metadata>
    </task>
    <task name="HDFS_to_CSV"




          fork="true"
    >
      <description>
        <![CDATA[ A task which submits a Spark job from a docker container, to read/write files from/to HDFS. This workflow requires a Spark/HDFS platform. ]]>
      </description>
      <variables>
        <variable name="input_hdfs_csv_file_name" value="iris_PROCESSED.csv" inherited="false"  description="the csv file name under the HDFS root dir from which Spark load the DataFrame" group="Input Parameters" advanced="false" hidden="false"/>
        <variable name="OUTPUT_FILE" value="hdfs_extract_job_${PA_JOB_ID}_iter_${PA_TASK_ITERATION}.csv" inherited="false"  description="relative file path in the Global Space where the CSV output file will be stored" group="Output Parameters" advanced="false" hidden="false"/>
      </variables>
      <genericInformation>
        <info name="PRE_SCRIPT_AS_FILE" value="script.scala"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$spark_token_name"/>
      </genericInformation>
      <depends>
        <task ref="MySQL_to_Spark_to_HDFS"/>
      </depends>
      <pre>
        <script>
          <code language="scalaw">
            <![CDATA[
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.types.DoubleType

import org.apache.spark.sql.SQLContext

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path

import java.nio.file.{Paths, Files}
import java.net.URI


object Application {

  def main() {
    val spark = SparkSession
          .builder
          .appName("HDFS-to-CSV app")
          .getOrCreate()
    val sc = spark.sparkContext

    // Get args
    val args = sc.getConf.get("spark.driver.args").split("\\s+")
    val hdfs_url = args(0)
    val input_hdfs_csv_file_name = args(1)

    // HDFS CSV to DF
    val df = spark
      			.read
      			.options(Map("inferSchema"->"true","delimiter"->",","header"->"false"))
      			.csv(hdfs_url + "/" + input_hdfs_csv_file_name)


    println("CSV>>")
    df.collect.foreach(println)
  	println("<<CSV")
  }
}

Application.main()
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import org.apache.commons.lang3.StringUtils

def spark_network_name = variables.get("spark_network_name")
def spark_master_url = variables.get("spark_master_url")
def hdfs_url = "hdfs://" + variables.get("hdfs_namenode_host_port")
def input_hdfs_csv_file_name = variables.get("input_hdfs_csv_file_name")
def OUTPUT_FILE = variables.get("OUTPUT_FILE")
def output_file_path = new File(localspace, OUTPUT_FILE).getAbsolutePath()

// Submit the Spark job
def spark_shell_command = "/usr/local/spark/bin/spark-shell --driver-memory 800m --executor-memory 800m --master " + spark_master_url + " --jars /usr/local/spark/jars/* -I /localspace/script.scala --conf spark.driver.args='" + hdfs_url + " " + input_hdfs_csv_file_name + "'"
cmd = ["docker", "run", "--rm", "--net", spark_network_name, "-v", localspace + ":/localspace", "activeeon/hdfs-spark:latest", "bash", "-c", spark_shell_command]
println cmd

def output_with_csv = new StringBuilder()
cmd.execute().waitForProcessOutput(output_with_csv, System.err)

new File(output_file_path).text = StringUtils.substringBetween(output_with_csv.toString(), "CSV>>", "<<CSV").replaceAll("\\[","").replaceAll("\\]","").replaceFirst("\n","")
]]>
          </code>
        </script>
      </scriptExecutable>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
      <metadata>
        <positionTop>
          686.078125
        </positionTop>
        <positionLeft>
          679.140625
        </positionLeft>
      </metadata>
    </task>
    <task name="Start"




          fork="true"
    >
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
        <info name="Documentation" value="user/ProActiveUserGuide.html#_loop"/>
      </genericInformation>
      <pre>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Retrieve_variables_from_service_instance_id/raw" language="groovy">
            <arguments>
              <argument value="$spark_hdfs_service_instance_id"/>
              <argument value="spark_network_name"/>
              <argument value="spark_network_name"/>
              <argument value="spark_master_url"/>
              <argument value="spark_master_url"/>
              <argument value="spark_token_name"/>
              <argument value="INSTANCE_NAME"/>
              <argument value="hdfs_namenode_host_port"/>
              <argument value="hdfs_namenode_host_port"/>
            </arguments>
          </file>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="javascript">
            <![CDATA[
print('Loop block start ' + variables.get('PA_TASK_ITERATION'))
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="start"></controlFlow>
      <metadata>
        <positionTop>
          302.078125
        </positionTop>
        <positionLeft>
          615.140625
        </positionLeft>
      </metadata>
    </task>
    <task name="Loop"




          fork="true"
    >
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
      </genericInformation>
      <depends>
        <task ref="MySQL_to_CSV"/>
        <task ref="HDFS_to_CSV"/>
      </depends>
      <inputFiles>
        <files  includes="*_extract_job_${PA_JOB_ID}_iter_${PA_TASK_ITERATION}.csv" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <scriptExecutable>
        <script>
          <code language="R">
            <![CDATA[
# Get variables
job_id <- variables["PA_JOB_ID"]
iteration_id <- variables["PA_TASK_ITERATION"]

# Get SOURCE DATA as dataframe
mysql_csv_file_path <- file.path( localspace, paste("mysql_extract_job_", job_id, "_iter_", iteration_id, ".csv", sep=''))
mysql_csv <- read.table(file=mysql_csv_file_path, header=TRUE, sep=",")

# Get TARGET DATA as dataframe
hdfs_csv_file_path <- file.path( localspace, paste("hdfs_extract_job_", job_id, "_iter_", iteration_id, ".csv", sep=''))
hdfs_csv <- read.table(file=hdfs_csv_file_path, header=FALSE, sep=",")

# Print nb rows of SOURCE & DATA
nrow_mysql_csv <- nrow(mysql_csv)
nrow_hdfs_csv <- nrow(hdfs_csv)
print(paste ("!! CONTROL !! NB ROWS SOURCE/TARGET ==> MySQL", nrow_mysql_csv, "lines", "HDFS", nrow_hdfs_csv, "lines", sep=' '))

# Store hdfs nb rows for iteration control
variables[paste("nrow_hdfs_extract_job_", job_id, "_iter_", iteration_id, sep='')] <- nrow_hdfs_csv
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow  block="end">
        <loop target="Start">
          <script>
            <code language="groovy">
              <![CDATA[
// Get variables
def job_id = variables.get("PA_JOB_ID")
def iteration_id = variables.get("PA_TASK_ITERATION") as Integer
def nrow_hdfs_csv = variables.get("nrow_hdfs_extract_job_" + job_id + "_iter_" + iteration_id) as Integer

// Get accumulated nb rows
def accu_nrow = nrow_hdfs_csv
def accu_variable_name = "accu_nrow_hdfs_extract_job_" + job_id
if (iteration_id > 0) {
    accu_nrow += variables.get(accu_variable_name)
}

// Update accumulated nb rows
println("STORING NEW ACCU = " + accu_nrow)
variables.put(accu_variable_name, accu_nrow)


if(accu_nrow <= 300) {
    loop = true;
} else {
    loop = false;
}
]]>
            </code>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
          814.078125
        </positionTop>
        <positionLeft>
          615.140625
        </positionLeft>
      </metadata>
    </task>
    <task name="Web_Notification"




          fork="true"
    >
      <description>
        <![CDATA[ Task to send a message to the notification service ]]>
      </description>
      <variables>
        <variable name="MESSAGE" value="[ATRADIUS] NOTIFICATION" inherited="true"  description="Notification message sent to the notification portal" group="" advanced="false" hidden="false"/>
        <variable name="SEVERITY" value="INFO" inherited="true" model="PA:LIST(INFO,WARNING,ERROR,CRITICAL)"    />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/web_notification.png"/>
      </genericInformation>
      <depends>
        <task ref="Loop"/>
      </depends>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/control-notification/resources/Web_Notification_Script/raw" language="groovy"></file>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
          942.078125
        </positionTop>
        <positionLeft>
          615.140625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2832px;
            height:3312px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-297.078125px;left:-546.140625px"><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_785" style="top: 558.094px; left: 551.141px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task allows to import data from MySQL database.
It requires the following third-party credential:  {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials."><img src="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png" width="20px">&nbsp;<span class="name">MySQL_to_CSV</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_788" style="top: 431.078px; left: 616.141px; z-index: 24;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task allows to export data to MySQL database.
It requires the following third-party credential: {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials."><img src="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png" width="20px">&nbsp;<span class="name">Feed_MySQL</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_791" style="top: 558.094px; left: 679.141px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A task which submits a Spark job from a docker container, to read/write files from/to HDFS. This task requires a Spark/HDFS platform deployed with ProActive Service Automation."><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">MySQL_to_Spark_to_HDFS</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_794" style="top: 686.094px; left: 679.141px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A task which submits a Spark job from a docker container, to read/write files from/to HDFS. This workflow requires a Spark/HDFS platform."><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">HDFS_to_CSV</span></a></div><div class="task block-start ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_797" style="top: 302.094px; left: 615.141px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Start</span></a></div><div class="task block-end ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_800" style="top: 814.094px; left: 615.141px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Loop</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_ active-task" id="jsPlumb_1_803" style="top: 942.094px; left: 615.141px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Task to send a message to the notification service"><img src="/automation-dashboard/styles/patterns/img/wf-icons/web_notification.png" width="20px">&nbsp;<span class="name">Web_Notification</span></a></div><svg style="position:absolute;left:594.5px;top:470.578125px" width="82.140625" height="87.921875" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 86.921875 C -10 36.921875 71.140625 50 61.140625 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.770178542968748,63.03759774609375 L22.48052864796902,53.091603630455225 L13.267262298130222,52.75142255084027 L12.194353452715536,43.594519875293756 L3.770178542968748,63.03759774609375" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.770178542968748,63.03759774609375 L22.48052864796902,53.091603630455225 L13.267262298130222,52.75142255084027 L12.194353452715536,43.594519875293756 L3.770178542968748,63.03759774609375" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:654.5px;top:341.5px" width="22.140625" height="90.078125" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 1.140625 89.078125 C 11.140625 39.078125 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.8769804999999997,67.28913849999999 L9.949865529336158,46.988397936649484 L3.233183691549608,53.30394898861228 L-4.035323982051562,47.63219474509987 L3.8769804999999997,67.28913849999999" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.8769804999999997,67.28913849999999 L9.949865529336158,46.988397936649484 L3.233183691549608,53.30394898861228 L-4.035323982051562,47.63219474509987 L3.8769804999999997,67.28913849999999" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:655.640625px;top:470.578125px" width="114.359375" height="87.921875" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 93.359375 86.921875 C 103.359375 36.921875 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M84.18445068359375,61.314866699218754 L71.59314505541273,44.27200829524975 L72.61258943753663,53.43501725256044 L63.71329560875442,55.84386954130687 L84.18445068359375,61.314866699218754" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M84.18445068359375,61.314866699218754 L71.59314505541273,44.27200829524975 L72.61258943753663,53.43501725256044 L63.71329560875442,55.84386954130687 L84.18445068359375,61.314866699218754" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:719.5px;top:597.5px" width="50.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 39.5 50 29.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-0.19430087500000195,65.8307285 L14.119682683751062,50.20671254123305 L5.321872471305423,52.963256657656245 L1.2522108414072983,44.69053919492763 L-0.19430087500000195,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-0.19430087500000195,65.8307285 L14.119682683751062,50.20671254123305 L5.321872471305423,52.963256657656245 L1.2522108414072983,44.69053919492763 L-0.19430087500000195,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:594.5px;top:597.5px" width="81" height="217" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 60 216 C 70 166 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M53.0778975,167.0637915 L53.25038250621476,145.87487343128703 L48.56070781404513,153.81256524198514 L39.99915624819988,150.3920631172419 L53.0778975,167.0637915" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M53.0778975,167.0637915 L53.25038250621476,145.87487343128703 L48.56070781404513,153.81256524198514 L39.99915624819988,150.3920631172419 L53.0778975,167.0637915" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:654.5px;top:725.5px" width="86" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 88 C -10 38 75 50 65 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M4.508909999999998,63.554236 L23.50168446304822,54.158784004710824 L14.302278717405704,53.54974055220558 L13.497189015253797,44.36541528730511 L4.508909999999998,63.554236" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M4.508909999999998,63.554236 L23.50168446304822,54.158784004710824 L14.302278717405704,53.54974055220558 L13.497189015253797,44.36541528730511 L4.508909999999998,63.554236" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:701.4938960671528px;top:341.5px" width="14.006103932847157" height="473" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 0 C -10 50 -10 422 0 472 " transform="translate(13.506103932847157,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#316b31" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-6.048000000000002,103.59731200000003 L0.49019329683702306,123.75300668942207 L-6.369968722603548,117.5936092296842 L-13.506103932847157,123.43103796681852 L-6.048000000000002,103.59731200000003" class="" stroke="#316b31" fill="#316b31" transform="translate(13.506103932847157,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-6.048000000000002,103.59731200000003 L0.49019329683702306,123.75300668942207 L-6.369968722603548,117.5936092296842 L-13.506103932847157,123.43103796681852 L-6.048000000000002,103.59731200000003" class="" stroke="#316b31" fill="#316b31" transform="translate(13.506103932847157,0.5)"></path></svg><div class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_827" style="position: absolute; transform: translate(-50%, -50%); left: 707px; top: 577.5px;">loop</div><svg style="position:absolute;left:654.5px;top:853.5px" width="26" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 5 88 C 15 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M7.409531250000001,66.78168750000002 L12.520990380459518,46.21781175738666 L6.108748919827519,52.84224829573104 L-1.4184488238094648,47.518594087559144 L7.409531250000001,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M7.409531250000001,66.78168750000002 L12.520990380459518,46.21781175738666 L6.108748919827519,52.84224829573104 L-1.4184488238094648,47.518594087559144 L7.409531250000001,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 595px; top: 588px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 595px; top: 548px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 656.141px; top: 461.078px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 656.141px; top: 421.078px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 749.5px; top: 588px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 749.5px; top: 548px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 720px; top: 716px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 720px; top: 676px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 655px; top: 332px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint loop-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 705px; top: 332px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 655px; top: 844px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 655px; top: 804px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint loop-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 705px; top: 804px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 660px; top: 972px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 660px; top: 932px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
