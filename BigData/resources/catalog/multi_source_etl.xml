<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="Multi_Source_ETL" tags="S3,Azure,Big Data,Data Streaming,ETL" projectName="10. ETL Workflows" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="INPUT_DATA_URL" value="https://s3.us-east-1.amazonaws.com/activeeon-demo-etl/dataset/facturation_train.csv" model="PA:URL" description="URL of the input data file loaded from Amazon S3"/>
    <variable name="OUTPUT_PATH" value="results/ETL_results.csv" description="Relative file path inside the Global Space"/>
    <variable name="DATABASE" value="demo" group="PostgreSQL Connection" description="Database name"/>
    <variable name="HOST" value="xx" model="PA:NOT_EMPTY_STRING" group="PostgreSQL Connection" description="Hostname or IP address of the postgresql server" />
    <variable name="PORT" value="5432" model="PA:INTEGER" group="PostgreSQL Connection" description="Port of the postgresql server"/>
    <variable name="USER" value="demo" model="PA:NOT_EMPTY_STRING" group="PostgreSQL Connection" description="Database user name"/>
    <variable name="CREDENTIALS_KEY" value="postgresql://${USER}@${HOST}:${PORT}" group="PostgreSQL Connection" model="PA:CREDENTIAL" description="Third-party credential storing the database password"/>
    <variable name="ACCESS_KEY" value="my_access_key" model="PA:NOT_EMPTY_STRING" group="S3 Storage" description="Amazon S3 Access Key"/>
    <variable name="SECRET_KEY" value="${ACCESS_KEY}" model="PA:CREDENTIAL" group="S3 Storage" description="Third-party credential storing the Secret Key associated with the given S3 Access Key"/>
    <variable name="STORAGE_ACCOUNT" value="my_storage_account" model="PA:NOT_EMPTY_STRING" group="Azure Blob Storage" description="Name of the Azure Storage Account"/>
    <variable name="ACCOUNT_KEY" value="${STORAGE_ACCOUNT}" model="PA:CREDENTIAL" group="Azure Blob Storage" description="Third-party credential storing the account key associated with the given Azure Storage Account"/>
    <variable name="CONTAINER_NAME" value="my-container" model="PA:NOT_EMPTY_STRING" group="Azure Blob Storage" description="A new or existing container name under which your uploaded data will be stored."/>
    <variable name="BLOB_NAME" value="my-blob" model="PA:NOT_EMPTY_STRING" group="Azure Blob Storage" description="The blob name or the directory to which file(s) are uploaded. It can be empty if the INPUT_PATH contains a path to a directory."/>
    <variable name="DOCKER_ENABLED" value="True" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
  </variables>
  <description>
    <![CDATA[ This workflow shows an example ETL pipeline that extracts data from different sources (S3 + PostgreSQL), runs different Transformation operations (drop some columns, scale data, summarize data) then Loads data into a different source (Azure blob storage).
    This workflow is a kind of data processing and preparation step that precedes the training step in a machine learning process. The processed data correspond to a set of transactional bank information of several clients.
]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="data-big-data"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/etl.png"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Merge_Data"




    fork="true">
      <description>
        <![CDATA[ Merge two data frames by performing a database-style join operation by columns or indexes. ]]>
      </description>
      <variables>
        <variable name="REF_COLUMN" value="ID_CPTE" inherited="false" description="Name of the database colum that will be used to merge the two data frames."/>
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" model="PA:NOT_EMPTY_STRING" inherited="true" group="Docker Parameters" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_filter_data"/>
      </genericInformation>
      <depends>
        <task ref="Drop_Columns2"/>
        <task ref="Summarize_Data"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
  print("Task " + __file__ + " disabled")
  quit()

print("BEGIN " + __file__)

import sys, bz2, uuid
import pandas as pd
import numpy as np

REF_COLUMN = variables.get("REF_COLUMN")
assert REF_COLUMN is not None and REF_COLUMN is not ""

input_variables = {'task.dataframe_id': None}
dataframe_id1 = None
dataframe_id2 = None

for key in input_variables.keys():
  for res in results:
    value = res.getMetadata().get(key)
    if value is not None and dataframe_id1 is None:
      dataframe_id1 = value
      continue
    if value is not None and dataframe_id2 is None:
      dataframe_id2 = value
      continue

print("dataframe id1 (in): ", dataframe_id1)
print("dataframe id2 (in): ", dataframe_id2)

dataframe_json1 = variables.get(dataframe_id1)
dataframe_json2 = variables.get(dataframe_id2)

assert dataframe_json1 is not None
assert dataframe_json2 is not None

dataframe_json1 = bz2.decompress(dataframe_json1).decode()
dataframe_json2 = bz2.decompress(dataframe_json2).decode()

dataframe1 = pd.read_json(dataframe_json1, orient='split')
dataframe2 = pd.read_json(dataframe_json2, orient='split')

dataframe = pd.merge(dataframe1, dataframe2, on=[REF_COLUMN], how='outer')

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id (out): ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", __file__)
resultMetadata.put("task.dataframe_id", dataframe_id)

LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")
LIMIT_OUTPUT_VIEW = 5 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
  print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
  dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

#***************# HTML PREVIEW STYLING #***************#
styles = [
    dict(selector="th", props=[("font-weight", "bold"),
                               ("font-size", "17px"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "#FFFFFF"),
                              ("border-bottom", "1px solid #FF8C00")]),
    dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 2px"),
                               ("font-size", "13px"),
                               ("border", "1px solid #999999"),
                               ("border-bottom", "1px solid #0B6FA4")]),
    dict(selector="table", props=[("border-collapse", "collapse"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
]
#******************************************************#

with pd.option_context('display.max_colwidth', -1):
    result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <metadata>
        <positionTop>
            674.859375
        </positionTop>
        <positionLeft>
            529.546875
        </positionLeft>
      </metadata>
    </task>
    <task name="Export_Data_to_CSV"




    fork="true">
      <description>
        <![CDATA[ Export the data in a specified format (CSV, JSON, HTML). ]]>
      </description>
      <variables>
        <variable name="OUTPUT_FILE" value="$OUTPUT_PATH" inherited="false" description="Relative file path in the Global Space where results will be stored"/>
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" group="Docker Parameters" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_filter_data"/>
      </genericInformation>
      <depends>
        <task ref="Merge_Data"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
    print("Task " + __file__ + " disabled")
    quit()

print("BEGIN " + __file__)

import pandas as pd
import numpy as np
import bz2, os

OUTPUT_FILE = variables.get("OUTPUT_FILE")
assert OUTPUT_FILE is not None and OUTPUT_FILE is not ""

input_variables = {'task.dataframe_id': None}
for key in input_variables.keys():
    for res in results:
        value = res.getMetadata().get(key)
        if value is not None:
            input_variables[key] = value
            break

dataframe_id = input_variables['task.dataframe_id']
print("dataframe id (in): ", dataframe_id)

dataframe_json = variables.get(dataframe_id)
assert dataframe_json is not None
dataframe_json = bz2.decompress(dataframe_json).decode()

dataframe = pd.read_json(dataframe_json, orient='split')
print(dataframe.head())

os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
dataframe.to_csv(path_or_buf=OUTPUT_FILE, index=False)

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
      <metadata>
        <positionTop>
            802.625
        </positionTop>
        <positionLeft>
            438.90625
        </positionLeft>
      </metadata>
    </task>
    <task name="Preview_Results_in_HTML"




    fork="true">
      <description>
        <![CDATA[ Export the results in a specified format (CSV, JSON, HTML). ]]>
      </description>
      <variables>
        <variable name="OUTPUT_TYPE" value="HTML" inherited="false" model="PA:LIST(CSV,JSON,HTML)" description="Format of the output (CSV, JSON or HTML)"/>
        <variable name="LIMIT_OUTPUT_VIEW" value="100" inherited="false" model="PA:Integer" description="Maximum number of rows displayed in the output"/>
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" group="Docker Parameters" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_data.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_export_results"/>
      </genericInformation>
      <depends>
        <task ref="Merge_Data"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
    print("Task " + __file__ + " disabled")
    quit()

print("BEGIN " + __file__)

import pandas as pd
import numpy as np
import bz2

OUTPUT_TYPE = variables.get("OUTPUT_TYPE")
assert OUTPUT_TYPE is not None and OUTPUT_TYPE is not ""

input_variables = {'task.dataframe_id': None}
for key in input_variables.keys():
    for res in results:
        value = res.getMetadata().get(key)
        if value is not None:
            input_variables[key] = value
            break

dataframe_id = input_variables['task.dataframe_id']
print("dataframe id (in): ", dataframe_id)

dataframe_json = variables.get(dataframe_id)
assert dataframe_json is not None
dataframe_json = bz2.decompress(dataframe_json).decode()

dataframe = pd.read_json(dataframe_json, orient='split')
print(dataframe.head())

OUTPUT_TYPE = OUTPUT_TYPE.upper()
if OUTPUT_TYPE == "CSV":
    #result = dataframe.to_csv(encoding='utf-8', index=False)
    result = dataframe.to_csv(index=False)
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", "dataframe.csv")
    resultMetadata.put("content.type", "text/csv")

if OUTPUT_TYPE == "JSON":
    result = dataframe.to_json(orient='split', encoding='utf-8')
    resultMetadata.put("file.extension", ".json")
    resultMetadata.put("file.name", "dataframe.json")
    resultMetadata.put("content.type", "application/json")

if OUTPUT_TYPE == "HTML":
    LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")
    LIMIT_OUTPUT_VIEW = 5 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
    if LIMIT_OUTPUT_VIEW > 0:
        print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
        dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

    #***************# HTML PREVIEW STYLING #***************#
    styles = [
        dict(selector="th", props=[("font-weight", "bold"),
                               ("font-size", "17px"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "#FFFFFF"),
                               ("border-bottom", "1px solid #FF8C00")]),
        dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 2px"),
                               ("font-size", "13px"),
                               ("border", "1px solid #999999"),
                               ("border-bottom", "1px solid #0B6FA4")]),
        dict(selector="table", props=[("border-collapse", "collapse"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
    ]
    #******************************************************#

    with pd.option_context('display.max_colwidth', -1):
        result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
        resultMetadata.put("file.extension", ".html")
        resultMetadata.put("file.name", "output.html")
        resultMetadata.put("content.type", "text/html")

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <metadata>
        <positionTop>
            802.625
        </positionTop>
        <positionLeft>
            620.1875
        </positionLeft>
      </metadata>
    </task>
    <task name="Drop_Columns2"




    fork="true">
      <description>
        <![CDATA[ Remove the specified columns from your data. ]]>
      </description>
      <variables>
        <variable name="COLUMNS_NAME" value="PERIODID_MY" inherited="false" description="Comma-separated list of columns to remove"/>
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" group="Docker Parameters" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_filter_data"/>
      </genericInformation>
      <depends>
        <task ref="Import_from_PostgreSQL"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
  print("Task " + __file__ + " disabled")
  quit()

print("BEGIN " + __file__)

import sys, bz2, uuid
import pandas as pd
import numpy as np

COLUMNS_NAME = variables.get("COLUMNS_NAME")
assert COLUMNS_NAME is not None and COLUMNS_NAME is not ""

input_variables = {'task.dataframe_id': None}
for key in input_variables.keys():
  for res in results:
    value = res.getMetadata().get(key)
    if value is not None:
      input_variables[key] = value
      break

dataframe_id = input_variables['task.dataframe_id']
print("dataframe id (in): ", dataframe_id)

dataframe_json = variables.get(dataframe_id)
assert dataframe_json is not None
dataframe_json = bz2.decompress(dataframe_json).decode()

dataframe = pd.read_json(dataframe_json, orient='split')

columns = [x.strip() for x in COLUMNS_NAME.split(',')]
dataframe.drop(columns, axis=1, inplace=True)

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id (out): ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", __file__)
resultMetadata.put("task.dataframe_id", dataframe_id)

LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")
LIMIT_OUTPUT_VIEW = 5 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
    print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
    dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

#***************# HTML PREVIEW STYLING #***************#
styles = [
    dict(selector="th", props=[("font-weight", "bold"),
                               ("font-size", "17px"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "#FFFFFF"),
                              ("border-bottom", "1px solid #FF8C00")]),
    dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 2px"),
                               ("font-size", "13px"),
                               ("border", "1px solid #999999"),
                               ("border-bottom", "1px solid #0B6FA4")]),
    dict(selector="table", props=[("border-collapse", "collapse"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
]
#******************************************************#

with pd.option_context('display.max_colwidth', -1):
    result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")



print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
            547.078125
        </positionTop>
        <positionLeft>
            610.671875
        </positionLeft>
      </metadata>
    </task>
    <task name="Load_Data"




    fork="true">
      <description>
        <![CDATA[ Load data from a CSV file. ]]>
      </description>
      <variables>
        <variable name="INPUT_FILE" value="$S3_INPUT_FILE" inherited="false" description="Relative file path of the input file in the Global Space" />
        <variable name="FILE_DELIMITER" value="," inherited="false" description="CSV delimiter"/>
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" group="Docker Parameters" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_data.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_import_data"/>
      </genericInformation>
      <depends>
        <task ref="Import_from_S3"/>
      </depends>
      <inputFiles>
        <files  includes="$INPUT_FILE" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import sys, bz2, uuid
import pandas as pd
import numpy as np

__file__ = variables.get("PA_TASK_NAME")
print("BEGIN " + __file__)

FILE_URL = variables.get("INPUT_FILE")
FILE_DELIMITER = variables.get("FILE_DELIMITER")
LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")

assert FILE_URL is not None and FILE_URL is not ""
assert FILE_DELIMITER is not None and FILE_DELIMITER is not ""

FILE_URL = variables.get(FILE_URL[1:]) if FILE_URL.startswith("$") else FILE_URL
dataframe = pd.read_csv(FILE_URL, FILE_DELIMITER)

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id: ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", __file__)
resultMetadata.put("task.dataframe_id", dataframe_id)

LIMIT_OUTPUT_VIEW = 10 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
    print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
    dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

#***************# HTML PREVIEW STYLING #***************#
styles = [
    dict(selector="th", props=[("font-weight", "bold"),
                               ("font-size", "17px"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "#FFFFFF"),
                              ("border-bottom", "1px solid #FF8C00")]),
    dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 2px"),
                               ("font-size", "13px"),
                               ("border", "1px solid #999999"),
                               ("border-bottom", "1px solid #0B6FA4")]),
    dict(selector="table", props=[("border-collapse", "collapse"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
]
#******************************************************#

with pd.option_context('display.max_colwidth', -1):
    result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <metadata>
        <positionTop>
            163.75
        </positionTop>
        <positionLeft>
            448.390625
        </positionLeft>
      </metadata>
    </task>
    <task name="Drop_Columns1"




    fork="true">
      <description>
        <![CDATA[ Remove the specified columns from your data. ]]>
      </description>
      <variables>
        <variable name="COLUMNS_NAME" value="PERIODID_MY, StatementDate" inherited="false" description="Comma-separated list of columns to remove"/>
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" group="Docker Parameters" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_filter_data"/>
      </genericInformation>
      <depends>
        <task ref="Load_Data"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
    print("Task " + __file__ + " disabled")
    quit()

print("BEGIN " + __file__)

import sys, bz2, uuid
import pandas as pd
import numpy as np

COLUMNS_NAME = variables.get("COLUMNS_NAME")
assert COLUMNS_NAME is not None and COLUMNS_NAME is not ""

input_variables = {'task.dataframe_id': None}
for key in input_variables.keys():
    for res in results:
        value = res.getMetadata().get(key)
        if value is not None:
            input_variables[key] = value
            break

dataframe_id = input_variables['task.dataframe_id']
print("dataframe id (in): ", dataframe_id)

dataframe_json = variables.get(dataframe_id)
assert dataframe_json is not None
dataframe_json = bz2.decompress(dataframe_json).decode()

dataframe = pd.read_json(dataframe_json, orient='split')

columns = [x.strip() for x in COLUMNS_NAME.split(',')]
dataframe.drop(columns, axis=1, inplace=True)

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id (out): ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", __file__)
resultMetadata.put("task.dataframe_id", dataframe_id)

#***************# HTML PREVIEW STYLING #***************#
styles = [
    dict(selector="th", props=[("font-weight", "bold"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "white")]),
    dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 5px"),
                               ("border-bottom", "1px solid #999999")]),
    dict(selector="table", props=[("border", "1px solid #999999"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
]
#******************************************************#

LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")
LIMIT_OUTPUT_VIEW = 5 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
    print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
    dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

with pd.option_context('display.max_colwidth', -1):
    result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
            291.515625
        </positionTop>
        <positionLeft>
            448.390625
        </positionLeft>
      </metadata>
    </task>
    <task name="Scale_Data"




    fork="true">
      <description>
        <![CDATA[ Scale the specified columns from the data. ]]>
      </description>
      <variables>
        <variable name="COLUMNS_NAME" value="CurrentTotalBalance,CashBalance,CreditLimit" inherited="false" description="Comma-separated list of columns which will be rescaled"/>
        <variable name="SCALER_NAME" value="MinMaxScaler" inherited="false" description="Algorithm used to rescale columns"/>
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" group="Docker Parameters" model="PA:NOT_EMPTY_STRING" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_filter_data"/>
      </genericInformation>
      <depends>
        <task ref="Drop_Columns1"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
  print("Task " + __file__ + " disabled")
  quit()

print("BEGIN " + __file__)

import sys, bz2, uuid
import pandas as pd
import numpy as np

COLUMNS_NAME = variables.get("COLUMNS_NAME")
SCALER_NAME  = variables.get("SCALER_NAME")

assert COLUMNS_NAME is not None and COLUMNS_NAME is not ""
assert SCALER_NAME is not None and SCALER_NAME is not ""

input_variables = {'task.dataframe_id': None}
for key in input_variables.keys():
  for res in results:
    value = res.getMetadata().get(key)
    if value is not None:
      input_variables[key] = value
      break

dataframe_id = input_variables['task.dataframe_id']
print("dataframe id (in): ", dataframe_id)

dataframe_json = variables.get(dataframe_id)
assert dataframe_json is not None
dataframe_json = bz2.decompress(dataframe_json).decode()

dataframe = pd.read_json(dataframe_json, orient='split')

#-------------------------------------------------------------
def scale_columns(df, columns, scaler_name="RobustScaler"):
  from sklearn import preprocessing

  scaler = None
  if scaler_name == "StandardScaler":
    scaler = preprocessing.StandardScaler()
  if scaler_name == "RobustScaler":
    scaler = preprocessing.RobustScaler()
  if scaler_name == "MinMaxScaler":
    scaler = preprocessing.MinMaxScaler()
  assert scaler is not None

  data = df.filter(columns, axis=1)
  print(scaler.fit(data))

  scaled_data = scaler.transform(data)
  scaled_df = pd.DataFrame(scaled_data, columns=columns)

  dataframe_scaled = df.copy()
  dataframe_scaled = dataframe_scaled.reset_index(drop=True)
  for column in columns:
      dataframe_scaled[column] = scaled_df[column]

  return dataframe_scaled, scaler

def apply_scaler(df, columns, scaler):
  data = df.filter(columns, axis=1)
  scaled_data = scaler.transform(data)
  scaled_df = pd.DataFrame(scaled_data, columns=columns)

  dataframe_scaled = df.copy()
  dataframe_scaled = dataframe_scaled.reset_index(drop=True)
  for column in columns:
      dataframe_scaled[column] = scaled_df[column]

  return dataframe_scaled
#-------------------------------------------------------------

columns = [x.strip() for x in COLUMNS_NAME.split(',')]
dataframe, scaler = scale_columns(dataframe, columns, SCALER_NAME)
print(dataframe.head())

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id (out): ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", __file__)
resultMetadata.put("task.dataframe_id", dataframe_id)

LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")
LIMIT_OUTPUT_VIEW = 5 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
  print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
  dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

#***************# HTML PREVIEW STYLING #***************#
styles = [
    dict(selector="th", props=[("font-weight", "bold"),
                               ("font-size", "17px"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "#FFFFFF"),
                              ("border-bottom", "1px solid #FF8C00")]),
    dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 2px"),
                               ("font-size", "13px"),
                               ("border", "1px solid #999999"),
                               ("border-bottom", "1px solid #0B6FA4")]),
    dict(selector="table", props=[("border-collapse", "collapse"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
]
#******************************************************#

with pd.option_context('display.max_colwidth', -1):
    result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
            419.296875
        </positionTop>
        <positionLeft>
            448.390625
        </positionLeft>
      </metadata>
    </task>
    <task name="Summarize_Data"




    fork="true">
      <description>
        <![CDATA[ Create a set of statistical measures that describe each column in the input data. ]]>
      </description>
      <variables>
        <variable name="REF_COLUMN" value="ID_CPTE" inherited="false" description="Reference column in the dataset used to generate statistics"/>
        <variable name="GLOBAL_MODEL_TYPE" value="" inherited="false" description="Method used to calculate the statistics model"/>
        <variable name="LABEL_COLUMN" value="" inherited="false" description="Reference column used as label during the machine learning" />
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean" description="Enable or skip the execution of this task" advanced="true"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" group="Docker Parameters" description="If true, the workflow tasks will be executed inside a docker container" advanced="true"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" group="Docker Parameters" description="Name of the docker image used to execute the task" advanced="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_filter_data"/>
      </genericInformation>
      <depends>
        <task ref="Scale_Data"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
  print("Task " + __file__ + " disabled")
  quit()

print("BEGIN " + __file__)

import sys, bz2, uuid
import pandas as pd
import numpy as np

REF_COLUMN        = variables.get("REF_COLUMN")
LABEL_COLUMN      = variables.get("LABEL_COLUMN")
GLOBAL_MODEL_TYPE = variables.get("GLOBAL_MODEL_TYPE")

assert REF_COLUMN is not None and REF_COLUMN is not ""

IGNORE_COLUMNS = [REF_COLUMN]
if LABEL_COLUMN is not None and LABEL_COLUMN is not "":
  IGNORE_COLUMNS.append(LABEL_COLUMN)

input_variables = {'task.dataframe_id': None}
for key in input_variables.keys():
  for res in results:
    value = res.getMetadata().get(key)
    if value is not None:
      input_variables[key] = value
      break

dataframe_id = input_variables['task.dataframe_id']
print("dataframe id (in): ", dataframe_id)

dataframe_json = variables.get(dataframe_id)
assert dataframe_json is not None
dataframe_json = bz2.decompress(dataframe_json).decode()

dataframe = pd.read_json(dataframe_json, orient='split')

columns = dataframe.drop(IGNORE_COLUMNS, axis=1, inplace=False).columns.values
ncolumns = columns.shape[0]
#bins = [10] * ncolumns
#--- task specific
bins = [10, 10, 10, 5]
#---
print(columns, ncolumns, bins)

#-------------------------------------------------------------
def compute_global_model(df, columns, bins, model_type="KMeans"):
  from sklearn.cluster import KMeans
  from sklearn.preprocessing import PolynomialFeatures

  models = {}
  for j, column in enumerate(columns):
    column_df = df[column]
    X = column_df.values

    if model_type == "KMeans":
      model = KMeans(n_clusters=bins[j], random_state=0).fit(X.reshape(-1,1))

    if model_type == "PolynomialFeatures":
      model = PolynomialFeatures().fit(X.reshape(-1,1))

    models[column] = model
  return models

def compute_features(sub_df, columns, bins, model, model_type="KMeans"):
  import scipy.stats.stats as st
  row = []
  for j, column in enumerate(columns):
    column_df = sub_df[column]
    X = column_df.values

    if model is not None:
      if model_type == "KMeans":
        result = model[column].predict(X.reshape(-1,1))

      if model_type == "PolynomialFeatures":
        result = model[column].transform(X.reshape(-1,1)).tolist()
    else:
        result = X

    # compute feature histogram
    #counts, bin_edges = np.histogram(result, bins=bins[j], density=False)
    #column_hist = counts

    # compute normalized feature histogram
    #counts, bin_edges = np.histogram(result, bins=bins[j], density=True)
    #column_hist = counts * np.diff(bin_edges)

    #row.extend(column_hist)

    # add extra features
    kurtosis = st.kurtosis(X.reshape(-1,1))[0]
    skew = st.skew(X.reshape(-1,1))[0]
    min_value = column_df.min()
    max_value = column_df.max()
    mean_value = column_df.mean()
    median_value = column_df.median()
    row.extend([kurtosis, skew, min_value, max_value, mean_value, median_value])
  return row

def get_summary(df, columns, bins, model, model_type, ref_column, label_column=None):
  data = {}
  IDs = df[ref_column].unique()
  for i, ID in enumerate(IDs):
    sub_df = df.loc[df[ref_column] == ID]
    row = [ID]

    features = compute_features(sub_df, columns, bins, model, model_type)
    row.extend(features)

    if label_column is not None and label_column is not "":
        assert sub_df[label_column].unique().shape[0] == 1
        label = sub_df[label_column].unique()[0]
        row.extend([label])

    data[i] = row
  return data
#-------------------------------------------------------------

model = None
if GLOBAL_MODEL_TYPE is not None and GLOBAL_MODEL_TYPE is not "":
  print('Computing the global model using ', GLOBAL_MODEL_TYPE)
  model = compute_global_model(dataframe, columns, bins, GLOBAL_MODEL_TYPE)
  print(model)
  print('Finished')

print('Summarizing data...')
data = get_summary(dataframe, columns, bins, model, GLOBAL_MODEL_TYPE, REF_COLUMN, LABEL_COLUMN)
print('Finished')

dataframe = pd.DataFrame.from_dict(data, orient='index')
cols_len = len(dataframe.columns)
dataframe.columns = list(range(0, cols_len))

COLUMNS_NAME = {0: REF_COLUMN}
if LABEL_COLUMN is not None and LABEL_COLUMN is not "":
  COLUMNS_NAME = {0: REF_COLUMN, cols_len-1: LABEL_COLUMN}
dataframe.rename(index=str, columns=COLUMNS_NAME, inplace=True)

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id (out): ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", __file__)
resultMetadata.put("task.dataframe_id", dataframe_id)

LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")
LIMIT_OUTPUT_VIEW = 5 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
  print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
  dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

#***************# HTML PREVIEW STYLING #***************#
styles = [
    dict(selector="th", props=[("font-weight", "bold"),
                               ("font-size", "17px"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "#FFFFFF"),
                              ("border-bottom", "1px solid #FF8C00")]),
    dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 2px"),
                               ("font-size", "13px"),
                               ("border", "1px solid #999999"),
                               ("border-bottom", "1px solid #0B6FA4")]),
    dict(selector="table", props=[("border-collapse", "collapse"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
]
#******************************************************#

with pd.option_context('display.max_colwidth', -1):
    result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
            547.078125
        </positionTop>
        <positionLeft>
            448.390625
        </positionLeft>
      </metadata>
    </task>
    <task name="Import_from_PostgreSQL"




    fork="true">
      <description>
        <![CDATA[ This task allows to import data from PostgreSQL database.
It requires the following third-party credential: {key: postgres://<username>@<host>:<port>, value: POSTGRESQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
$POSTGRES_QUERY (required) is the user's sql query.
$OUTPUT_FILE (optional) is a relative path in the data space used to save the results in a CSV file.
$OUTPUT_TYPE (optional) if set to HTML, it allows to preview the results in Scheduler Portal in an HTML format. Default is CSV
This task uses also the task variable RMDB_DRIVER as a driver to connect to the database. The specified default driver "psycopg2" is already provided for this task. To use another driver, make sure you have it properly installed before. ]]>
      </description>
      <variables>
        <variable name="SQL_QUERY" value="performance" inherited="false" description="SQL query to run" />
        <variable name="OUTPUT_FILE" value="" inherited="false"  description="Relative file path in the Global Space where imported data will be stored" group="Output Parameters"/>
        <variable name="OUTPUT_TYPE" value="HTML" inherited="false" model="PA:List(CSV,HTML)" description="If set to HTML, it allows to preview the results in Scheduler Portal in an HTML format. Default is CSV" group="Output Parameters"/>
        <variable name="DATABASE" value="demo" inherited="true" group="PostgreSQL Connection" description="Database name"/>
        <variable name="HOST" value="xx" inherited="true" model="PA:NOT_EMPTY_STRING" group="PostgreSQL Connection" description="Hostname or IP address of the postgresql server" />
        <variable name="PORT" value="5432" inherited="true" model="PA:INTEGER" group="PostgreSQL Connection" description="Port of the postgresql server"/>
        <variable name="USER" value="demo" inherited="true" model="PA:NOT_EMPTY_STRING" group="PostgreSQL Connection" description="Database user name"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean" description="If true, the workflow tasks will be executed inside a docker container" advanced="true" group="Docker Parameters"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" model="PA:NOT_EMPTY_STRING" description="Name of the docker image used to execute the task" advanced="true" group="Docker Parameters"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import sys, bz2, uuid

DBMS_NAME = "postgresql"
if DBMS_NAME == "postgresql":
    DBMS_DATA=('psycopg2', 5432)
if DBMS_NAME == "mysql":
    DBMS_DATA=('mysqlconnector', 3306)
if DBMS_NAME == "greenplum":
    DBMS_DATA=('psycopg2', 5432)
if DBMS_NAME == "sqlserver":
    DBMS_DATA=('pyodbc', 1433)
if DBMS_NAME == "oracle":
    DBMS_DATA=('cx_oracle', 1521)

DBMS_DRIVER, DBMS_DEFAULT_PORT = DBMS_DATA

print("BEGIN importing data from " + DBMS_NAME + " database using " + DBMS_DRIVER + " driver")

CREDENTIALS_KEY_MSG = DBMS_NAME + "://<username>@<host>:<port>"

HOST = variables.get("HOST")
PORT = variables.get("PORT")
DATABASE = variables.get("DATABASE")
USER = variables.get("USER")

# This key is used for getting the password from 3rd party credentials.
CREDENTIALS_KEY = variables.get("CREDENTIALS_KEY")
PASSWORD=credentials.get(CREDENTIALS_KEY)

SQL_QUERY = variables.get("SQL_QUERY")
OUTPUT_FILE = variables.get("OUTPUT_FILE")
OUTPUT_TYPE = variables.get("OUTPUT_TYPE")
LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")

if not HOST:
    print("ERROR: HOST variable is not provided by the user.")
    sys.exit(1)
if not PORT:
    PORT = DBMS_DEFAULT_PORT
    print("WARNING: PORT variable is not provided by the user. Using the default value: {0}".format(DBMS_DEFAULT_PORT))
if not DATABASE:
    print("ERROR: DATABASE variable is not provided by the user.")
    sys.exit(1)
if not USER:
    print("ERROR: USER variable is not provided by the user.")
    sys.exit(1)
if not PASSWORD:
    print('ERROR: Please add your database password to 3rd-party credentials in the scheduler-portal under the key :"{0}"'.format(CREDENTIALS_KEY_MSG))
    sys.exit(1)
if not SQL_QUERY:
    print("ERROR: SQL_QUERY variable is not provided by the user.")
    sys.exit(1)

print("EXECUTING QUERY...")
print("INSERTING DATA IN {0} DATABASE...".format(DBMS_NAME))
print('HOST= ', HOST)
print('PORT= ', PORT)
print('USER= ', USER)
print('DATABASE= ', DATABASE)
print('QUERY= ', SQL_QUERY)
if OUTPUT_FILE:
    print('OUTPUT_FILE=' + OUTPUT_FILE)

# Please refer to SQLAlchemy doc for more info about database urls.
# http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urlsdatabase_url = '{0}+{1}://{2}:{3}@{4}:{5}/{6}'.format(DBMS_PROTOCOL,DBMS_DRIVER,USER,PASSWORD,HOST,PORT,DATABASE)
database_url = '{0}+{1}://{2}:{3}@{4}:{5}/{6}'.format(DBMS_NAME,DBMS_DRIVER,USER,PASSWORD,HOST,PORT,DATABASE)
engine = create_engine(database_url)

with engine.connect() as conn, conn.begin():
    #pd.read_sql() can take either a SQL query as a parameter or a table name
    dataframe = pd.read_sql(SQL_QUERY, conn)

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id: ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", variables.get("PA_TASK_NAME"))
resultMetadata.put("task.dataframe_id", dataframe_id)

LIMIT_OUTPUT_VIEW = 10 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
    print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
    dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

#***************# HTML PREVIEW STYLING #***************#
styles = [
    dict(selector="th", props=[("font-weight", "bold"),
                               ("text-align", "center"),
                               ("background", "#0B6FA4"),
                               ("color", "white")]),
    dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 5px"),
                               ("border-bottom", "1px solid #999999")]),
    dict(selector="table", props=[("border", "1px solid #999999"),
                                  ("text-align", "center"),
                                  ("width", "100%"),
                                  ("border", "1px solid #999999")])
]
#******************************************************#

if OUTPUT_TYPE == "HTML":
    result = dataframe.style.set_table_styles(styles).render().encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")
else:
    # Write results to the task result in CSV format
    result = dataframe.to_csv(index=False).encode('utf-8')
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", "result.csv")
    resultMetadata.put("content.type", "text/csv")

# If an OUTPUT_FILE path in the dataspace is designated, then write to this file.
if OUTPUT_FILE:
    dataframe.to_csv(path_or_buf=OUTPUT_FILE, index=False)

print("END Import_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
      <metadata>
        <positionTop>
            419.296875
        </positionTop>
        <positionLeft>
            610.671875
        </positionLeft>
      </metadata>
    </task>
    <task name="Import_from_S3"




    fork="true">
      <description>
        <![CDATA[ This task allows to export data to S3.
The task requires the following third-party credential: {key: ACCESS_KEY, value: SECRET_KEY} Please refer to the User documentation to learn how to add third-party credentials.
]]>
      </description>
      <variables>
        <variable name="LOCAL_RELATIVE_PATH" value="datasets/" inherited="false" description="Local relative path to which we download file(s). LOCAL_RELATIVE_PATH can contain either a path to a file, a directory terminated by / or an empty value if you want to download file(s) to the root of the localspace (user or global)."/>
        <variable name="URL" value="${INPUT_DATA_URL}" inherited="false" description="URL of a file stored in S3 which will be used as input file" group="S3 Storage"/>
        <variable name="ACCESS_KEY" value="" inherited="true" description="S3 user access key" group="S3 Storage"/>
        <variable name="TRANSFER_DIRECTIVE" value="" inherited="false" model="PA:SPEL(! ( variables[&#39;LOCAL_RELATIVE_PATH&#39;].endsWith(&#39;/&#39;) || variables[&#39;LOCAL_RELATIVE_PATH&#39;].isEmpty() ? variables[&#39;TRANSFER_DIRECTIVE&#39;] = variables[&#39;LOCAL_RELATIVE_PATH&#39;] + &#39;**&#39; : variables[&#39;TRANSFER_DIRECTIVE&#39;] = variables[&#39;LOCAL_RELATIVE_PATH&#39;]).isEmpty())" hidden="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/aws_s3.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_amazon_s3"/>
      </genericInformation>
      <inputFiles>
        <files  includes="aws-java-sdk-fat-1.11.228.jar" accessMode="cacheFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="$cachespace/aws-java-sdk-fat-1.11.228.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import java.io.File
import java.io.FileNotFoundException
import java.io.IOException
import java.io.Serializable
import java.nio.file.Paths
import java.util.ArrayList
import java.util.List
import java.util.Map

import com.amazonaws.auth.AWSStaticCredentialsProvider
import com.amazonaws.auth.BasicAWSCredentials
import com.amazonaws.client.builder.AwsClientBuilder
import com.amazonaws.services.s3.AmazonS3
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.services.s3.AmazonS3ClientBuilder
import com.amazonaws.services.s3.internal.ServiceUtils
import com.amazonaws.services.s3.model.Bucket
import com.amazonaws.services.s3.transfer.MultipleFileUpload
import com.amazonaws.services.s3.transfer.TransferManager
import com.amazonaws.services.s3.transfer.TransferManagerBuilder
import com.amazonaws.services.s3.transfer.Upload
import com.amazonaws.services.s3.transfer.Transfer
import com.amazonaws.services.s3.transfer.TransferProgress
import com.amazonaws.services.s3.AmazonS3URI
import com.amazonaws.services.s3.transfer.Download
import com.amazonaws.services.s3.transfer.MultipleFileDownload
import com.amazonaws.AmazonClientException
import com.amazonaws.util.AwsHostNameUtils
import com.amazonaws.AmazonServiceException
import com.amazonaws.regions.RegionUtils

//Set S3 connection parameters and retrieve the S3 secret key
url = variables.get("URL")
localRelativePath = variables.get("LOCAL_RELATIVE_PATH")
accessKey = variables.get("ACCESS_KEY")
scheme = ""
host = ""
bucket = ""
remoteRelativePath = ""
secretKey = checkParametersAndReturnSecretKey()

File file = new File(localRelativePath)
createDirIfNotExists(file)
AmazonS3 amazonS3 = getS3Client(accessKey, secretKey, scheme, host)

// Check that the key name (remoteRelativePath) is either a path to a directory terminated by / or a path for a file
if (isDirectoryPath(remoteRelativePath)) {
    downloadDir(bucket, remoteRelativePath, localRelativePath, false, amazonS3)
    println(listDirectoryContents(file, new ArrayList<>()))
} else {
    localRelativePath = Paths.get(localRelativePath, Paths.get(url).getFileName().toString()).toString()
    downloadFile(bucket, remoteRelativePath, localRelativePath, false, amazonS3)
    variables.put("S3_INPUT_FILE", localRelativePath)
    println(localRelativePath)
}

/**
     * Download a list of files from S3. <br>
     * Requires a bucket name. <br>
     * Requires a key prefix. <br>
     *
     * @param bucketName
     * @param keyPrefix
     * @param dirPath
     * @param pause
     * @param s3Client
     */
def downloadDir(String bucketName, String keyPrefix, String dirPath, boolean pause, AmazonS3 s3Client) {
    println("downloading to directory: " + dirPath + (pause ? " (pause)" : ""));
    TransferManager transferManager = TransferManagerBuilder.standard().withS3Client(s3Client).build()

    try {
        MultipleFileDownload xfer = transferManager.downloadDirectory(bucketName, keyPrefix, new File(dirPath))
        // loop with Transfer.isDone()
        showTransferProgress(xfer);
        // or block with Transfer.waitForCompletion()
        waitForCompletion(xfer);
    } catch (AmazonServiceException e) {
        throw new Exception(e.getMessage())
    } finally {
        transferManager.shutdownNow();
    }
}

/**
     * Download a file from S3. <br>
     * Requires a bucket name. <br>
     * Requires a key prefix. <br>
     *
     * @param bucketName
     * @param keyName
     * @param filePath
     * @param pause
     * @param s3Client
     */
def downloadFile(String bucketName, String keyName, String filePath, boolean pause, AmazonS3 s3Client) {
    println("Downloading to file: " + filePath + (pause ? " (pause)" : ""));
    File f = new File(filePath);
    TransferManager xferMgr = TransferManagerBuilder.standard().withS3Client(s3Client).build();
    try {
        Download xfer = xferMgr.download(bucketName, keyName, f);
        // loop with Transfer.isDone()
        showTransferProgress(xfer);
        // or block with Transfer.waitForCompletion()
        waitForCompletion(xfer);
    } catch (AmazonServiceException e) {
        throw new Exception(e.getMessage())
    } finally {
        xferMgr.shutdownNow();
    }
}

/**
* waits for the transfer to complete, catching any exceptions that occur.
* @param xfer
*/
def waitForCompletion(Transfer xfer) {
    try {
        xfer.waitForCompletion();
    } catch (AmazonServiceException e) {
        println("Amazon service error: " + e.getMessage());
        throw new IllegalStateException("Amazon service error: " + e.getMessage())
    } catch (AmazonClientException e) {
        println("Amazon client error: " + e.getMessage());
        throw new IllegalStateException("Amazon client error: " + e.getMessage())
    } catch (InterruptedException e) {
        println("Transfer interrupted: " + e.getMessage());
        Thread.currentThread().interrupt();
        throw new IllegalStateException("Transfer interrupted: " + e.getMessage())
    }
}

/**
* Prints progress while waiting for the transfer to finish.
* @param xfer
*/
def showTransferProgress(Transfer xfer) {
    // print the transfer's human-readable description
    println(xfer.getDescription());
    // print an empty progress bar...
    printProgressBar(0.0);
    // update the progress bar while the xfer is ongoing.
    while ({
        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            return;
        }
        // Note: so_far and total aren't used, they're just for
        // documentation purposes.
        TransferProgress progress = xfer.getProgress();
        double pct = progress.getPercentTransferred();
        eraseProgressBar();
        printProgressBar(pct);
        !xfer.isDone();
    }()) continue;
    // print the final state of the transfer.
    Transfer.TransferState xferState = xfer.getState();
    println(": " + xferState);
}

/**
 * prints a simple text progressbar: [#####     ]
 * @param pct
 */
def printProgressBar(double pct) {
    // if bar_size changes, then change erase_bar (in eraseProgressBar) to
    // match.
    final int bar_size = 40;
    final String empty_bar = "                                        ";
    final String filled_bar = "########################################";
    int amtFull = (int) (bar_size * (pct / 100.0));
    final String logMsg = String.format("  [%s%s]",
                                        filled_bar.substring(0, amtFull),
                                        empty_bar.substring(0, bar_size - amtFull));
    println(logMsg);
}

/**
* erases the progress bar.
*/
def eraseProgressBar() {
    // erase_bar is bar_size (from printProgressBar) + 4 chars.
    final String erase_bar = "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b";
    println(erase_bar)
}

/**
* Get or initialize the S3 client.
* Note: this method must be synchronized because we're accessing the
* field and we're calling this method from a worker thread.
*
* @return the S3 client
*/
def getS3Client(String accessKey, String secretKey, String... args) {
    BasicAWSCredentials credentials = new BasicAWSCredentials(accessKey, secretKey)
    AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard().withCredentials(new AWSStaticCredentialsProvider(credentials))

    if (args.length == 1) {
        builder = builder.withRegion(args[0])

    } else {
        String endpoint = args[0] + "://" + args[1]
        String clientRegion = null
        if (!ServiceUtils.isS3USStandardEndpoint(endpoint) &&
            (clientRegion = AwsHostNameUtils.parseRegion(args[1], AmazonS3Client.S3_SERVICE_NAME)) == null) {
            throw new IllegalArgumentException("Invalid region in " + args[1])
        }
        builder = builder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpoint,
                                                                                               clientRegion))
    }
    builder = builder.withPathStyleAccessEnabled(true)
    return builder.build()
}


/**
* List recursively the contents of a directory
* @param dir
* @param filesRelativePathName
* @return List of Files' names of the given directory
* @throws IOException
*/
def listDirectoryContents(File dir, List<String> filesRelativePathName) throws IOException {
    File[] files = dir.listFiles();
    if (files == null) {
        throw new IOException("Download failed. The resource in the given S3 URL does not exist or is not accessible");
    }
    for (File file : files) {
        if (file.isDirectory()) {
            listDirectoryContents(file, filesRelativePathName);
        } else {
            filesRelativePathName.add(file.getCanonicalPath());
        }
    }
    return filesRelativePathName;
}

/**
* Creates a directory if it does not exist
* @param file
*/
def createDirIfNotExists(File file) {
    // If the path already exists, print a warning.
    if (!file.exists()) {
        try {
            file.mkdir()
            println("The " + file.getName() + " directory is created")
        } catch (Exception e) {
            throw new Exception("Couldn't create destination directory! " + file.getName())
        }
    } else {
        println("The given local path " + file.getName() + " already exists");
    }
}

/**
* check whether or not the given file path is a path to a directory terminated by /
* @param filePath
* @return
*/
def isDirectoryPath(String filePath) {
    return filePath.endsWith(File.separator)
}

/**
* Parse an Amazon S3 Uri to extract four elements: scheme, host, bucket name and key name.
*
* @param s3Uri
*/
def parseAmazonS3URI(String s3Uri) {
    AmazonS3URI amazonS3URI = new AmazonS3URI(s3Uri);
    if ((scheme = amazonS3URI.getURI().getScheme()) == null) {
        throw new IllegalArgumentException("You have to specify a valid scheme in the provided s3 uri. Empty value is not allowed.");
    }
    if ((host = amazonS3URI.getURI().getHost()) == null) {
        throw new IllegalArgumentException("You have to specify a valid host in the provided s3 uri. Empty value is not allowed.");
    }
    if ((bucket = amazonS3URI.getBucket()) == null) {
        throw new IllegalArgumentException("You have to specify a valid bucket name in the provided s3 uri. Empty value is not allowed.");
    }
    if ((remoteRelativePath = amazonS3URI.getKey()) == null) {
        throw new IllegalArgumentException("You have to specify a valid key name in the provided s3 uri. Empty value is not allowed.");
    }
}

/**
* Checks and initialize parameters
* returns the S3 secret key using the third party credentials mechanism
*/
def checkParametersAndReturnSecretKey() {
    if (url.isEmpty()) {
        throw new IllegalArgumentException("URL variable is not provided by the user. Empty value is not allowed.")
    } else {
        parseAmazonS3URI(url)
    }
    if (localRelativePath.isEmpty()) {
        //Default value is getLocalSpace() because it will always be writable and moreover can be used to transfer files to another data space (global, user)
        localRelativePath = localspace
    }
    if (accessKey.isEmpty()) {
        throw new IllegalArgumentException("ACCESS_KEY variable is not provided by the user. Empty value is not allowed.")
    }
    def secretKey = credentials.get(accessKey)
    if (secretKey == null || secretKey.isEmpty()) {
        throw new IllegalArgumentException("Please add your secret key to 3rd-party credentials under the key :\"" +
                                           accessKey + "\"")
    }
    return secretKey
}
]]>
          </code>
        </script>
      </scriptExecutable>
      <outputFiles>
        <files  includes="$TRANSFER_DIRECTIVE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
      <metadata>
        <positionTop>
            35.953125
        </positionTop>
        <positionLeft>
            448.390625
        </positionLeft>
      </metadata>
    </task>
    <task name="Export_to_Azure_Blob"




    fork="true">
      <description>
        <![CDATA[ This task allows to export data to Azure Blob Storage.
The task requires the following third-party credential: {key: STORAGE_ACCOUNT, value: ACCOUNT_KEY}. Please refer to the User documentation to learn how to add third-party credentials.
]]>
      </description>
      <variables>
        <variable name="INPUT_PATH" value="$OUTPUT_PATH" inherited="false" description="Local relative path in the data space from which we upload file(s). INPUT_PATH can contain either a path to a file, a directory terminated by / or an empty value for the root."/>
        <variable name="STORAGE_ACCOUNT" value="" inherited="true" description="Azure Storage Account name" group="Azure Blob Storage"/>
        <variable name="CONTAINER_NAME" value="" inherited="true" description="A new or existing container name under which your uploaded data will be stored." group="Azure Blob Storage"/>
        <variable name="BLOB_NAME" value="" inherited="true" description="The blob name or the directory to which file(s) are uploaded. It can be empty if the INPUT_PATH contains a path to a directory." group="Azure Blob Storage"/>
        <variable name="TRANSFER_DIRECTIVE" value="" inherited="false" model="PA:SPEL(! ( variables[&#39;INPUT_PATH&#39;].endsWith(&#39;/&#39;) || variables[&#39;INPUT_PATH&#39;].isEmpty() ? variables[&#39;TRANSFER_DIRECTIVE&#39;] = variables[&#39;INPUT_PATH&#39;] + &#39;**&#39; : variables[&#39;TRANSFER_DIRECTIVE&#39;] = variables[&#39;INPUT_PATH&#39;]).isEmpty())" hidden="true"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure_blob_storage.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_azure_blob_storage"/>
      </genericInformation>
      <depends>
        <task ref="Export_Data_to_CSV"/>
      </depends>
      <inputFiles>
        <files  includes="$TRANSFER_DIRECTIVE" accessMode="transferFromGlobalSpace"/>
        <files  includes="azure-storage-blob-fat-10.0.3-Preview.jar" accessMode="cacheFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <envScript>
          <script>
            <code language="groovy">
              <![CDATA[
def jarFile = new File(cachespace, "azure-storage-blob-fat-10.0.3-Preview.jar")

forkEnvironment.addAdditionalClasspath(jarFile.getAbsolutePath())
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="bash">
            <![CDATA[

]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import java.io.File
import java.io.FileNotFoundException
import java.io.IOException
import java.io.Serializable
import java.nio.channels.FileChannel
import java.nio.file.Paths
import java.util.ArrayList
import java.util.List
import java.util.Map
import java.util.concurrent.ExecutionException

import com.microsoft.azure.storage.blob.BlockBlobURL
import com.microsoft.azure.storage.blob.ContainerURL
import com.microsoft.azure.storage.blob.TransferManager
import java.net.MalformedURLException
import java.net.URL
import java.security.InvalidKeyException;
import com.microsoft.azure.storage.blob.PipelineOptions
import com.microsoft.azure.storage.blob.ServiceURL
import com.microsoft.azure.storage.blob.SharedKeyCredentials
import com.microsoft.azure.storage.blob.StorageURL
import com.microsoft.rest.v2.RestException


//Set Azure Blob Storage connection parameters and retrieve the account key
containerName = variables.get("CONTAINER_NAME")
inputPath = variables.get("INPUT_PATH")
blobName = variables.get("BLOB_NAME")
storageAccount = variables.get("STORAGE_ACCOUNT")
accountKey = checkParametersAndReturnAccountKey()

File file = new File(inputPath)
containerURL = createContainerURL(storageAccount, accountKey, containerName)
if (file.exists()) {
    uploadResources(file)
} else {
    throw new FileNotFoundException("The input file cannot be found at " + inputPath)
}


/**
* This method uploads resources (file or folder to an Azure Blob Storage
* @param file
* @throws InterruptedException
* @throws ExecutionException
* @throws IOException
*/

def uploadResources(File file) throws InterruptedException, ExecutionException, IOException {
    List<String> filesRelativePathName = new ArrayList<>()
    if (file.isDirectory()) {
        if (blobName) {
            filesRelativePathName = recursiveFolderUpload(inputPath, blobName, true)
        } else {
            filesRelativePathName = recursiveFolderUpload(inputPath, "", false)
        }

    } else {
        if (blobName) {
            //remove all white spaces from the blob name
            blobName = blobName.replaceAll("\\s+", "")
        }
        //this condition is true in the case where the blob name is initially a white spaces string and becomes an empty string
        if (blobName.isEmpty()) {
            uploadFile(file, file.getName())
        } else {
            uploadFile(file, blobName)
        }
        filesRelativePathName.add(file.getPath())
    }
    return filesRelativePathName;
}

/**
* This method uploads a local file to an Azure Storage blob
*
* @param file
* @param blobName
* @throws IOException
*/
def uploadFile(File file, String blobName) throws IOException, ExecutionException, InterruptedException {
    println("Uploading " + file.getName() + " file into the container: " + containerURL);
    FileChannel fileChannel = FileChannel.open(file.toPath());

    // Create a BlockBlobURL to run operations on Blobs
    final BlockBlobURL blob = containerURL.createBlockBlobURL(blobName);

    // Uploading a file to the blobURL using the high-level methods available in TransferManager class
    // Alternatively call the PutBlob/PutBlock low-level methods from BlockBlobURL type
    TransferManager.uploadFileToBlockBlob(fileChannel, blob, 8 * 1024 * 1024, null).toFuture().get();
    println("Completed upload request.");

}

/**
* This method recursively uploads a local file/folder to an Azure Storage blob
*
* @param sourcePath
* @param destinationPath
* @throws IOException
*/
def recursiveFolderUpload(String sourcePath, String destinationPath, boolean ignoreRoot)
throws IOException, ExecutionException, InterruptedException {
    List<String> filesRelativePathName = new ArrayList<>()

    File sourceFile = new File(sourcePath)
    String remoteFilePath = Paths.get(destinationPath, sourceFile.getName()).toString()

    if (sourceFile.isFile()) {
        // copy if it is a file
        println("Uploading " + sourcePath)
        uploadFile(sourceFile, remoteFilePath)
        println("File uploaded successfully to " + remoteFilePath)
        filesRelativePathName.add(remoteFilePath)

    } else {
        if (ignoreRoot) {
            remoteFilePath = destinationPath
        }

        File[] files = sourceFile.listFiles()
        if (!sourceFile.isHidden()) {
            for (File f : files) {
                filesRelativePathName.addAll(recursiveFolderUpload(f.getAbsolutePath(), remoteFilePath, false))
            }
        }
    }
    return filesRelativePathName
}

def createContainerURL(String accountName, String accountKey, String containerName) throws MalformedURLException {
    // Create a ServiceURL to call the Blob service. We will also use this to construct the ContainerURL
    SharedKeyCredentials creds = null
    try {
        creds = new SharedKeyCredentials(accountName, accountKey)
    } catch (InvalidKeyException e) {
        throw new Exception(e.getMessage())
    }
    // We are using a default pipeline here, you can learn more about it at https://github.com/Azure/azure-storage-java/wiki/Azure-Storage-Java-V10-Overview
    final ServiceURL serviceURL = new ServiceURL(new URL("https://" + accountName + ".blob.core.windows.net"),
                                                 StorageURL.createPipeline(creds, new PipelineOptions()))

    // Let's create a container using a blocking call to Azure Storage
    // If container exists, we'll catch and continue
    ContainerURL containerURL = serviceURL.createContainerURL(containerName)

    try {
        containerURL.create(null, null).blockingGet()
        println("The " + containerName + " container is created")
    } catch (RestException e) {
        if (e.response().statusCode() != 409) {
            throw new Exception(e.getMessage())
        } else {
            println("The " + containerName + " container already exists, resuming...")
        }
    }
    return containerURL
}

/**
* Checks and initialize parameters
* returns the Azure Blob account key using the third party credentials mechanism
*/
def checkParametersAndReturnAccountKey() {
    if (containerName.isEmpty()) {
        throw new IllegalArgumentException("CONTAINER_NAME variable is not provided by the user. Empty value is not allowed.")
    }
    if (inputPath.isEmpty()) {
        //Default value is getLocalSpace() because it will always be writable and moreover can be used to transfer files to another data space (global, user)
        inputPath = localspace
    }
    if (storageAccount.isEmpty()) {
        throw new IllegalArgumentException("STORAGE_ACCOUNT variable is not provided by the user. Empty value is not allowed.")
    }
    def accountKey = credentials.get(storageAccount)
    if (accountKey == null || accountKey.isEmpty()) {
        throw new IllegalArgumentException("Please add your secret key to 3rd-party credentials under the key :\"" +
                                           storageAccount + "\"")
    }
    return accountKey
}
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            926.015625
        </positionTop>
        <positionLeft>
            442.46875
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2830px;
            height:3392px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-30.953125px;left:-433.90625px"><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_62" style="top: 674.861px; left: 529.549px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Merge two data frames by performing a database-style join operation by columns or indexes."><img src="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png" width="20px">&nbsp;<span class="name">Merge_Data</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_65" style="top: 802.639px; left: 438.906px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Export the data in a specified format (CSV, JSON, HTML)."><img src="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png" width="20px">&nbsp;<span class="name">Export_Data_to_CSV</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_68" style="top: 802.639px; left: 620.191px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Export the results in a specified format (CSV, JSON, HTML)."><img src="/automation-dashboard/styles/patterns/img/wf-icons/export_data.png" width="20px">&nbsp;<span class="name">Preview_Results_in_HTML</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_71" style="top: 547.083px; left: 610.677px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Remove the specified columns from your data."><img src="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png" width="20px">&nbsp;<span class="name">Drop_Columns2</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_74" style="top: 163.75px; left: 448.403px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Load data from a CSV file."><img src="/automation-dashboard/styles/patterns/img/wf-icons/import_data.png" width="20px">&nbsp;<span class="name">Load_Data</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_77" style="top: 291.528px; left: 448.403px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Remove the specified columns from your data."><img src="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png" width="20px">&nbsp;<span class="name">Drop_Columns1</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_80" style="top: 419.306px; left: 448.403px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Scale the specified columns from the data."><img src="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png" width="20px">&nbsp;<span class="name">Scale_Data</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_83" style="top: 547.083px; left: 448.403px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Create a set of statistical measures that describe each column in the input data."><img src="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png" width="20px">&nbsp;<span class="name">Summarize_Data</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_86" style="top: 419.306px; left: 610.677px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task allows to import data from PostgreSQL database.
It requires the following third-party credential: {key: postgres://<username>@<host>:<port>, value: POSTGRESQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
$POSTGRES_QUERY (required) is the user's sql query.
$OUTPUT_FILE (optional) is a relative path in the data space used to save the results in a CSV file.
$OUTPUT_TYPE (optional) if set to HTML, it allows to preview the results in Scheduler Portal in an HTML format. Default is CSV
This task uses also the task variable RMDB_DRIVER as a driver to connect to the database. The specified default driver &quot;psycopg2&quot; is already provided for this task. To use another driver, make sure you have it properly installed before."><img src="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png" width="20px">&nbsp;<span class="name">Import_from_PostgreSQL</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_89" style="top: 35.9549px; left: 448.403px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task allows to export data to S3.
The task requires the following third-party credential: {key: ACCESS_KEY, value: SECRET_KEY} Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
$URL (required) is the s3 Url that should respect a certain format.
$LOCAL_RELATIVE_PATH (required) is the local relative path to which we download file(s). LOCAL_RELATIVE_PATH can contain either a path to a file, a directory terminated by / or an empty value for the root.
$LOCAL_RELATIVE_PATH (optional) is the local relative path to which we download file(s). LOCAL_RELATIVE_PATH can contain either a path to a file, a directory terminated by / or an empty value if you want to download file(s) to the root of the localspace (user or global).
$ACCESS_KEY (required) is the s3 user access key."><img src="/automation-dashboard/styles/patterns/img/wf-icons/aws_s3.png" width="20px">&nbsp;<span class="name">Import_from_S3</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_92" style="top: 926.024px; left: 442.483px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task allows to export data to Azure Blob Storage.
The task requires the following third-party credential: {key: STORAGE_ACCOUNT, value: ACCOUNT_KEY}. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
INPUT_PATH (optional) is the local relative path in the data space from which we upload file(s). INPUT_PATH can contain either a path to a file, a directory terminated by / or an empty value for the root.
CONTAINER_NAME (required) is a new or an existing container name under which your uploaded data will be stored.
BLOB_NAME (optional) is the blob name or the directory to which file(s) are uploaded. It can be empty if the INPUT_PATH contains a path to a directory.
STORAGE_ACCOUNT (required) is the storage account name."><img src="/automation-dashboard/styles/patterns/img/wf-icons/azure_blob_storage.png" width="20px">&nbsp;<span class="name">Export_to_Azure_Blob</span></a></div><svg style="position:absolute;left:569.5px;top:586.5px" width="105.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 94.5 50 84.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M7.587007999999997,62.682047999999995 L27.694799236931864,55.99798250271722 L18.6675049434459,54.125028221695985 L19.137779458627854,44.91748555927131 L7.587007999999997,62.682047999999995" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M7.587007999999997,62.682047999999995 L27.694799236931864,55.99798250271722 L18.6675049434459,54.125028221695985 L19.137779458627854,44.91748555927131 L7.587007999999997,62.682047999999995" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:493.5px;top:586.5px" width="97" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 76 88 C 86 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M69.8573855,63.115491500000005 L59.3333862576084,44.72404131394505 L59.28036566458122,53.94343331215811 L50.161328069766505,55.30106114936383 L69.8573855,63.115491500000005" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M69.8573855,63.115491500000005 L59.3333862576084,44.72404131394505 L59.28036566458122,53.94343331215811 L50.161328069766505,55.30106114936383 L69.8573855,63.115491500000005" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:493.5px;top:714.5px" width="97" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 86 50 76 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M6.142614499999997,63.115491500000005 L25.838671930233495,55.301061149363825 L16.71963433541878,53.943433312158106 L16.666613742391597,44.724041313945044 L6.142614499999997,63.115491500000005" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M6.142614499999997,63.115491500000005 L25.838671930233495,55.301061149363825 L16.71963433541878,53.943433312158106 L16.666613742391597,44.724041313945044 L6.142614499999997,63.115491500000005" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:569.5px;top:714.5px" width="139" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 118 88 C 128 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M104.371551,61.4125935 L90.06992443409666,45.77726563105373 L92.04033146857535,54.78379082773624 L83.4411217618329,58.10848516247838 L104.371551,61.4125935" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M104.371551,61.4125935 L90.06992443409666,45.77726563105373 L92.04033146857535,54.78379082773624 L83.4411217618329,58.10848516247838 L104.371551,61.4125935" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:654px;top:458.5px" width="41.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 88 C -10 38 30.5 50 20.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-1.080432000000001,66.303232 L11.3951921061979,49.175511685817675 L2.9611229197005473,52.899283558177174 L-2.0087563356249163,45.13395676611713 L-1.080432000000001,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-1.080432000000001,66.303232 L11.3951921061979,49.175511685817675 L2.9611229197005473,52.899283558177174 L-2.0087563356249163,45.13395676611713 L-1.080432000000001,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:487.5px;top:75.5px" width="24.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 13.5 50 3.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.531265625,66.78168750000002 L5.922688671570663,47.35153935976458 L-1.5001906020674536,52.819707543808825 L-8.03929128462053,46.32046433683204 L-2.531265625,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.531265625,66.78168750000002 L5.922688671570663,47.35153935976458 L-1.5001906020674536,52.819707543808825 L-8.03929128462053,46.32046433683204 L-2.531265625,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:487.5px;top:203.5px" width="24.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 3.5 88 C 13.5 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M6.031265625,66.78168750000002 L11.53929128462053,46.32046433683204 L5.000190602067454,52.819707543808825 L-2.422688671570663,47.35153935976458 L6.031265625,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M6.031265625,66.78168750000002 L11.53929128462053,46.32046433683204 L5.000190602067454,52.819707543808825 L-2.422688671570663,47.35153935976458 L6.031265625,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:487.5px;top:331.5px" width="24.5" height="88" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 87 C -10 37 13.5 50 3.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.531265625,65.86284375000001 L5.94144883702875,46.44086883381477 L-1.486707701655021,51.901866035499594 L-8.019528877471666,45.396310910469786 L-2.531265625,65.86284375000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.531265625,65.86284375000001 L5.94144883702875,46.44086883381477 L-1.486707701655021,51.901866035499594 L-8.019528877471666,45.396310910469786 L-2.531265625,65.86284375000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:487.5px;top:458.5px" width="27" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 6 88 C 16 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M8.328375,66.78168750000002 L13.175164621094911,46.153826962153474 L6.848577140751924,52.86011437424426 L-0.7464085046608426,47.633624821401554 L8.328375,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M8.328375,66.78168750000002 L13.175164621094911,46.153826962153474 L6.848577140751924,52.86011437424426 L-0.7464085046608426,47.633624821401554 L8.328375,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:493.5px;top:842.5px" width="26.5" height="84" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 5.5 83 C 15.5 33 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M7.870972999999999,62.650558000000004 L12.80384769174033,42.04311402274781 L6.449314656829793,48.72292763603304 L-1.1237826722266346,43.464772365918016 L7.870972999999999,62.650558000000004" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M7.870972999999999,62.650558000000004 L12.80384769174033,42.04311402274781 L6.449314656829793,48.72292763603304 L-1.1237826722266346,43.464772365918016 L7.870972999999999,62.650558000000004" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 570px; top: 705px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 570px; top: 665px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 494px; top: 833px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 494px; top: 793px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 688px; top: 833px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 688px; top: 793px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 654.5px; top: 577px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 654.5px; top: 537px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 488px; top: 194px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 488px; top: 154px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 491.5px; top: 322px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 491.5px; top: 282px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 488px; top: 449px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 488px; top: 409px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 494px; top: 577px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 494px; top: 537px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 675px; top: 449px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 491.5px; top: 66px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 499.5px; top: 956px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 499.5px; top: 916px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
