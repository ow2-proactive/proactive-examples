<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.13" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="CIFAR_100_Image_Classification" onTaskError="continueJobExecution" priority="normal" projectName="3.  Hyperparameter Optimization" xsi:schemaLocation="urn:proactive:jobdescriptor:3.13 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.13/schedulerjob.xsd">
  <variables>
    <variable name="NATIVE_SCHEDULER" value="" advanced="true" description="Name of the Native Scheduler node source to use when the workflow tasks must be deployed inside a cluster such as SLURM, LSF, etc." group="Resource Management" hidden="false"/>
    <variable name="NATIVE_SCHEDULER_PARAMS" value="" advanced="true" description="Parameters given to the native scheduler (SLURM, LSF, etc) while requesting a ProActive node used to deploy the workflow tasks." group="Resource Management" hidden="false"/>
    <variable name="NODE_SOURCE_NAME" value="" advanced="true" description="If not empty, the workflow tasks will be run only on nodes belonging to the specified node source." group="Resource Management" hidden="false"/>
    <variable name="NODE_ACCESS_TOKEN" value="" advanced="true" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." group="Resource Management" hidden="false"/>
    <variable name="CONTAINER_PLATFORM" model="PA:LIST(no-container,docker,podman,singularity)" value="docker" advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false"/>
    <variable name="CONTAINER_IMAGE" model="PA:LIST(,docker://activeeon/dlm3,docker://activeeon/cuda,docker://activeeon/cuda2,docker://activeeon/nvidia:tensorflow,docker://activeeon/tensorflow:latest,docker://activeeon/tensorflow:latest-gpu)" value="" advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false"/>
    <variable name="CONTAINER_GPU_ENABLED" model="PA:Boolean" value="True" description="If True, containers will run based on images containing libraries that are compatible with GPU." hidden="false" group="Container Parameters" advanced="true"/>
    <variable name="CONTAINER_ROOTLESS_ENABLED" model="PA:Boolean" value="False" advanced="true" description="If true, the container will run in rootless mode." group="Container Parameters"/>
    <variable name="NUM_EPOCHS" model="PA:Integer" value="5" description="Number of times data is passed forward and backward through the training algorithm." advanced="false" hidden="false"/>
    <variable name="INPUT_VARIABLES" model="PA:JSON" value="{&quot;OPTIMIZER&quot;: &quot;Adam&quot;,&quot;LEARNING_RATE&quot;: 0.0001, &quot;BATCH_SIZE&quot;: 64, &quot;WEIGHT_DECAY&quot;: 0.0005}" description="A set of specific variables (usecase-related) that are used in the model training process." advanced="false" hidden="false"/>
    <variable name="SEARCH_SPACE" model="PA:JSON" value="{&quot;OPTIMIZER&quot;:{&quot;choice&quot;: [&quot;Adam&quot;, &quot;SGD&quot;, &quot;RMSprop&quot;]}, &quot;LEARNING_RATE&quot;:{&quot;choice&quot;: [0.0001, 0.00025]}, &quot;BATCH_SIZE&quot;:{&quot;choice&quot;: [32, 64]}, &quot;WEIGHT_DECAY&quot;:{&quot;choice&quot;: [0.0005, 0.005]}}" description="Specifies the representation of the search space which has to be defined using dictionaries or by entering the path of a json file stored in the catalog." advanced="false" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ Train a simple deep CNN on the CIFAR100 images dataset using the Keras deeplearning library. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="auto-ml-optimization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/keras.png"/>
<info name="NODESOURCENAME" value="$NODE_SOURCE_NAME"/>
<info name="PYTHON_COMMAND" value="python3"/>
<info name="NS" value="$NATIVE_SCHEDULER"/>
<info name="Documentation" value="PML/PMLUserGuide.html#_hyperparameter_optimization"/>
<info name="NODE_ACCESS_TOKEN" value="$NODE_ACCESS_TOKEN"/>
<info name="NS_BATCH" value="$NATIVE_SCHEDULER_PARAMS"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="CIFAR_100_Image_Classification">
      <description>
        <![CDATA[ Train a simple deep CNN on the CIFAR100 images dataset using the Keras deeplearning library. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/keras.png"/>
        <info name="task.documentation" value="PML/PMLUserGuide.html#_hyperparameter_optimization"/>
      </genericInformation>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_source_name/raw"/>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/auto-ml-optimization/resources/get_automl_variables/raw"/>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")
print("BEGIN " + __file__)

import os, json

######################### CHECK GPU AVAILABILITY ##########################
try:
    import GPUtil as GPU
    from random import randint, uniform
    # Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # Show the utilization of all GPUs in a nice table
    print("Current GPU utilization:")
    GPU.showUtilization()
    # Get the first available GPU
    # order - The order in which the available GPU device ids are returned: first, last, random, load, memory
    # limit - Limits the number of GPU device ids returned. Must be positive integer. (default = 1)
    # maxLoad - Maximum current relative load for a GPU to be considered available. (default = 0.5)
    # maxMemory - Maximum current relative memory usage for a GPU to be considered available. (default = 0.5)
    # attempts - Number of attempts before giving up finding an available GPU. (default = 1)
    # interval - Interval in seconds between each attempt to find an available GPU. (default = 900 --> 15 min)
    # verbose - If True, prints the attempt number before each attempt and the GPU id if an available is found.
    # NOTE: this step will fail if all GPUs are busy
    # deviceIDs = GPUtil.getAvailable(order='first', limit=1, maxLoad=0.5, maxMemory=0.5)
    maxMemory = 0.4  # round(uniform(0.2, 0.5), 2)
    maxLoad = 0.4    # round(uniform(0.2, 0.5), 2)
    attempts = 10    # internal attempts
    interval = 10    # interval = randint(3, 30)  # 3sec to 30sec
    print('Looking for available GPU id with memory < ' +
          str(maxMemory * 100) + '%, and load < ' + str(maxLoad * 100) + '%)')
    print('# of attempts: ' + str(attempts) + ', interval:' + str(interval))
    DEVICE_ID_LIST = GPU.getFirstAvailable(
        order='random', maxMemory=maxMemory, maxLoad=maxLoad, attempts=attempts, interval=interval
    )
    DEVICE_ID = DEVICE_ID_LIST[0]  # grab first element from list
    print('First available GPU id (memory < ' +
          str(maxMemory * 100) + '%, load < ' + str(maxLoad * 100) + '%):')
    print(DEVICE_ID)
    # Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id
    os.environ["CUDA_VISIBLE_DEVICES"] = str(DEVICE_ID)
except:
    pass
###########################################################################


################### IMPORT TENSORFLOW + KERAS LIBRARIES ###################
# import tensorflow.keras as keras
# from tensorflow.keras.callbacks import TensorBoard
# from tensorflow.keras.datasets import cifar100
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
# from tensorflow.keras.layers import Conv2D, MaxPooling2D

import keras
from keras.callbacks import TensorBoard
from keras.datasets import cifar100
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D

# https://github.com/tensorflow/tensorflow/issues/24828
try:
    from tensorflow.compat.v1 import ConfigProto
    from tensorflow.compat.v1 import InteractiveSession
    config = ConfigProto()
    config.gpu_options.allow_growth = True
    session = InteractiveSession(config=config)
except:
    pass


######################### CIFAR100 IMAGE DATASET ##########################
def load_data(label_mode='fine'):
    """Loads [CIFAR100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).
    This is a dataset of 50,000 32x32 color training images and
    10,000 test images, labeled over 100 fine-grained classes that are
    grouped into 20 coarse-grained classes. See more info at the
    [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html).
    Arguments:
        label_mode: one of "fine", "coarse". If it is "fine" the category labels
        are the fine-grained labels, if it is "coarse" the output labels are the
        coarse-grained superclasses.
    Returns:
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.
        **x_train, x_test**: uint8 arrays of RGB image data with shape
            `(num_samples, 3, 32, 32)` if `tf.keras.backend.image_data_format()` is
            `'channels_first'`, or `(num_samples, 32, 32, 3)` if the data format
            is `'channels_last'`.
        **y_train, y_test**: uint8 arrays of category labels with shape
            (num_samples, 1).
    Raises:
        ValueError: in case of invalid `label_mode`.
    """
    import os
    import numpy as np
    from tensorflow.python.keras import backend as K
    from tensorflow.python.keras.datasets.cifar import load_batch
    from tensorflow.python.keras.utils.data_utils import get_file
    from tensorflow.python.util.tf_export import keras_export
    if label_mode not in ['fine', 'coarse']:
        raise ValueError('`label_mode` must be one of `"fine"`, `"coarse"`.')
    dirname = 'cifar-100-python'
    origin = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'
    path = get_file(
        dirname,
        origin=origin,
        untar=True,
        file_hash=
        '85cd44d02ba6437773c5bbd22e183051d648de2e7d6b014e1ef29b855ba677a7',
        cache_dir='/tmp/'
    )
    fpath = os.path.join(path, 'train')
    x_train, y_train = load_batch(fpath, label_key=label_mode + '_labels')
    fpath = os.path.join(path, 'test')
    x_test, y_test = load_batch(fpath, label_key=label_mode + '_labels')
    y_train = np.reshape(y_train, (len(y_train), 1))
    y_test = np.reshape(y_test, (len(y_test), 1))
    if K.image_data_format() == 'channels_last':
        x_train = x_train.transpose(0, 2, 3, 1)
        x_test = x_test.transpose(0, 2, 3, 1)
    return (x_train, y_train), (x_test, y_test)


########################### GENERAL PARAMETERS ############################
NUM_CLASSES = 100
DATA_AUGMENTATION = False
NUM_EPOCHS    = int(variables.get("NUM_EPOCHS"))
LEARNING_RATE = 0.0001
BATCH_SIZE    = 32
WEIGHT_DECAY  = 1e-6
OPTIMIZER     = 'Adam' # Adam, RMSPROP, SGD


############################ INPUT FROM AUTOML ############################
"""
SEARCH_SPACE:
{
	"OPTIMIZER": {
		"choice": ["Adam", "SGD", "RMSprop"]
	},
	"LEARNING_RATE": {
		"choice": [0.0001, 0.00025]
	},
	"BATCH_SIZE": {
		"choice": [32, 64]
	},
	"WEIGHT_DECAY": {
		"choice": [0.0005, 0.005]
	}
}

INPUT_VARIABLES:
{"OPTIMIZER": "Adam", "LEARNING_RATE": 0.0001, "BATCH_SIZE": 64, "WEIGHT_DECAY": 0.0005}
"""
input_variables = variables.get("INPUT_VARIABLES")
if input_variables is not None and input_variables != '':
    input_variables = json.loads(input_variables)
    LEARNING_RATE = input_variables["LEARNING_RATE"]
    BATCH_SIZE = input_variables["BATCH_SIZE"]
    WEIGHT_DECAY = input_variables["WEIGHT_DECAY"]
    OPTIMIZER = input_variables["OPTIMIZER"]
###########################################################################

print('-' * 30)
print('NUM_CLASSES:       ', NUM_CLASSES)
print('DATA_AUGMENTATION: ', DATA_AUGMENTATION)
print('NUM_EPOCHS:        ', NUM_EPOCHS)
print('LEARNING_RATE:     ', LEARNING_RATE)
print('BATCH_SIZE:        ', BATCH_SIZE)
print('WEIGHT_DECAY:      ', WEIGHT_DECAY)
print('OPTIMIZER:         ', OPTIMIZER)
print('-' * 30)

# The data, split between train and test sets:
# (x_train, y_train), (x_test, y_test) = cifar100.load_data()
(x_train, y_train), (x_test, y_test) = load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)
y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)

# Implementing above stack into code.
model = Sequential()
# Adding layers to the sequential model 
# .add() will push the layer into the stack(sequential model)
# filter -> 32
# strides -> (3,3)
model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))
# Relu is an activation function faster than sigmoid and reduce the likelihood of vanishing gradient
model.add(Activation('relu'))
# We don't have to define parameters again.
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
# Maxpooling is done for downsampling the input and reducing its dimensionality
# and also help over-fitting by providing an abstracted form of the representation
model.add(MaxPooling2D(pool_size=(2, 2)))
# Drop put is a regularization technique where randomly selected neurons are ignored during training
# Here 25% connection will drop
model.add(Dropout(0.25))
# Depth is changed into 64
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
# Convert matrix into a vector by stretching it.
model.add(Flatten())
# Dense layer is use to collect all the info 
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
# Number of target classes
model.add(Dense(NUM_CLASSES))
# Softmax convert probability into binary number.
model.add(Activation('softmax'))

# Initiate the selected optimizer
if OPTIMIZER.lower() == 'rmsprop':
    opt = keras.optimizers.RMSprop(lr=LEARNING_RATE, decay=WEIGHT_DECAY)
elif OPTIMIZER.lower() == 'sgd':
    opt = keras.optimizers.SGD(lr=LEARNING_RATE, decay=WEIGHT_DECAY)
elif OPTIMIZER.lower() == 'adam':
    opt = keras.optimizers.Adam(lr=LEARNING_RATE, decay=WEIGHT_DECAY)
else:
    raise Exception("Optimizer not defined!")

# Let's train the model using the selected optimizer
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

if not DATA_AUGMENTATION:
    print('Not using data augmentation.')
    model.fit(x_train, y_train,
            batch_size=BATCH_SIZE,
            epochs=NUM_EPOCHS,
            validation_data=(x_test, y_test),
            shuffle=True)
else:
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images
    
    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(x_train)

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(
      datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), 
      steps_per_epoch=x_train.shape[0], 
      epochs=NUM_EPOCHS, 
      validation_data=(x_test, y_test)
    )

# Score trained model.
scores = model.evaluate(x_test, y_test, verbose=1)
loss = scores[0]
acc = scores[1]
print('Test loss:', loss)
print('Test accuracy:', acc)

############################ OUTPUT FOR AUTOML ############################
# Convert from JSON to dict
token = variables.get("TOKEN")
token = json.loads(token)

# Return the loss value
result_map = {'token': token, 'loss':  loss}
print('result_map: ', result_map)
resultMap.put("RESULT_JSON", json.dumps(result_map))

# To appear in Job Analytics
resultMap.put("LOSS", str(loss))
resultMap.put("ACCURACY", str(acc))
###########################################################################

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            81.515625
        </positionTop>
        <positionLeft>
            183.046875
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2834px;
            height:3568px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-76.515625px;left:-178.046875px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_13" style="top: 81.5156px; left: 183.047px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Train a simple deep CNN on the CIFAR100 images dataset using the Keras deeplearning library.
You can see more details in: https://github.com/prakritidev/DeepLearning/blob/master/Keras/Convolutional%20Neural%20Network/CIFAR10_cnn.py"><img src="/automation-dashboard/styles/patterns/img/wf-icons/keras.png" width="20px">&nbsp;<span class="name">CIFAR_100_Image_Classification</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 265px; top: 112px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
