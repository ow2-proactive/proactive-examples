<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.13" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Text_Generation" onTaskError="continueJobExecution" priority="normal" projectName="3.  Hyperparameter Optimization" xsi:schemaLocation="urn:proactive:jobdescriptor:3.13 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.13/schedulerjob.xsd">
  <variables>
    <variable name="NATIVE_SCHEDULER" value="" description="Name of the Native Scheduler node source to use when the workflow tasks must be deployed inside a cluster such as SLURM, LSF, etc." group="Resource Management" advanced="true" hidden="false"/>
    <variable name="NATIVE_SCHEDULER_PARAMS" value="" description="Parameters given to the native scheduler (SLURM, LSF, etc) while requesting a ProActive node used to deploy the workflow tasks." group="Resource Management" advanced="true" hidden="false"/>
    <variable name="NODE_ACCESS_TOKEN" value="" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." group="Resource Management" advanced="true" hidden="false"/>
    <variable name="CONTAINER_PLATFORM" model="PA:LIST(no-container,docker,podman,singularity)" value="docker" description="Container platform used for executing the workflow tasks." group="Container Parameters" advanced="true" hidden="false"/>
    <variable name="CONTAINER_IMAGE" model="PA:LIST(,docker://activeeon/dlm3,docker://activeeon/cuda,docker://activeeon/cuda2,docker://activeeon/nvidia:tensorflow,docker://activeeon/tensorflow:latest,docker://activeeon/tensorflow:latest-gpu)" value="" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" advanced="true" hidden="false"/>
    <variable name="CONTAINER_GPU_ENABLED" model="PA:Boolean" value="True" description="If true, the container will run in rootless mode." group="Container Parameters" advanced="true" />
    <variable name="INPUT_VARIABLES" model="PA:JSON" value="" description="A set of specific variables (usecase-related) that are used in the model training process." advanced="false" hidden="false"/>
    <variable name="SEARCH_SPACE" model="PA:JSON" value="{&quot;OPTIMIZER&quot;:{&quot;choice&quot;:[&quot;Adam&quot;,&quot;RMSprop&quot;]},&quot;EPOCHS&quot;:{&quot;choice&quot;:[500,1000,2000]},&quot;MAXLEN&quot;:{&quot;choice&quot;:[50,100,150,200]},&quot;UNIT&quot;:{&quot;choice&quot;:[256,600,700]},&quot;DROPOUT&quot;:{&quot;choice&quot;:[0.0025,0.025,0.25]}}" description="Specifies the representation of the search space which has to be defined using dictionaries or by entering the path of a json file stored in the catalog." advanced="false" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ Train a simple Long Short-Term Memory to learn sequences of characters from 'The Alchemist' book. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="auto-ml-optimization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/keras.png"/>
<info name="NODESOURCENAME" value="$NODE_SOURCE_NAME"/>
<info name="NS" value="$NATIVE_SCHEDULER"/>
<info name="Documentation" value="PML/PMLUserGuide.html#_hyperparameter_optimization"/>
<info name="NODE_ACCESS_TOKEN" value="$NODE_ACCESS_TOKEN"/>
<info name="NS_BATCH" value="$NATIVE_SCHEDULER_PARAMS"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="Text_Generation">
      <description>
        <![CDATA[ Train a simple Long Short-Term Memory to learn sequences of characters from 'The Alchemist' book. It's a novel by Brazilian author Paulo Coelho that was first published in 1988. ]]>
      </description>
      <variables>
        <variable name="DATA_PATH" value="https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/alchemist.txt" description="Path or name of the file that contains the dataset." inherited="false"/>
        <variable name="EPOCHS" value="10" description="Number of times all the training vectors are used once to update the weights." inherited="false"/>
        <variable name="OPTIMIZER" value="RMSprop" description="Optimization algorithm that helps you minimize the Loss function." inherited="false"/>
        <variable name="MAXLEN" model="PA:Integer" value="100" description="Size of the slide window allowing each character a chance to be learned from the past characters that preceded it." inherited="false"/>
        <variable name="UNIT" model="PA:Integer" value="256" description="Number of LSTM units in the hidden layer of the generator model." inherited="false"/>
        <variable name="DROPOUT" model="PA:DOUBLE" value="0.5" description="Percentage of the neurons that will be ignored during the training." inherited="false"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/keras.png"/>
        <info name="task.documentation" value="PML/PMLUserGuide.html#_hyperparameter_optimization"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/auto-ml-optimization/resources/get_automl_variables/raw"/>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")
print("BEGIN " + __file__)

import re
import sys
import json
import wget
import uuid
import random
import shutil
import zipfile
import numpy as np
import unicodedata

from os import remove, listdir, makedirs
from os.path import basename, splitext, exists, join
from sklearn.model_selection import train_test_split
from random import randrange

######################### CHECK GPU AVAILABILITY ##########################
try:
    import GPUtil as GPU
    from random import randint, uniform
    # Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # Show the utilization of all GPUs in a nice table
    print("Current GPU utilization:")
    GPU.showUtilization()
    # Get the first available GPU
    # order - The order in which the available GPU device ids are returned: first, last, random, load, memory
    # limit - Limits the number of GPU device ids returned. Must be positive integer. (default = 1)
    # maxLoad - Maximum current relative load for a GPU to be considered available. (default = 0.5)
    # maxMemory - Maximum current relative memory usage for a GPU to be considered available. (default = 0.5)
    # attempts - Number of attempts before giving up finding an available GPU. (default = 1)
    # interval - Interval in seconds between each attempt to find an available GPU. (default = 900 --> 15 min)
    # verbose - If True, prints the attempt number before each attempt and the GPU id if an available is found.
    # NOTE: this step will fail if all GPUs are busy
    # deviceIDs = GPUtil.getAvailable(order='first', limit=1, maxLoad=0.5, maxMemory=0.5)
    maxMemory = 0.4  # round(uniform(0.2, 0.5), 2)
    maxLoad = 0.4    # round(uniform(0.2, 0.5), 2)
    attempts = 10    # internal attempts
    interval = 10    # interval = randint(3, 30)  # 3sec to 30sec
    print('Looking for available GPU id with memory < ' +
          str(maxMemory * 100) + '%, and load < ' + str(maxLoad * 100) + '%)')
    print('# of attempts: ' + str(attempts) + ', interval:' + str(interval))
    DEVICE_ID_LIST = GPU.getFirstAvailable(
        order='random', maxMemory=maxMemory, maxLoad=maxLoad, attempts=attempts, interval=interval
    )
    DEVICE_ID = DEVICE_ID_LIST[0]  # grab first element from list
    print('First available GPU id (memory < ' +
          str(maxMemory * 100) + '%, load < ' + str(maxLoad * 100) + '%):')
    print(DEVICE_ID)
    # Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id
    os.environ["CUDA_VISIBLE_DEVICES"] = str(DEVICE_ID)
except:
    pass
###########################################################################

from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, GRU

if sys.version_info[0] >= 3:
    unicode = str

DATA_PATH = str(variables.get("DATA_PATH"))
MAXLEN = int(variables.get("MAXLEN"))
UNIT = int(variables.get("UNIT"))
DROPOUT = float(variables.get("DROPOUT"))
OPTIMIZER = (variables.get("OPTIMIZER"))
EPOCHS = int(variables.get("EPOCHS"))

############################ INPUT FROM AUTOML ############################
"""
SEARCH_SPACE:
{
	"OPTIMIZER": {
		"choice": ["Adam", "RMSprop"]
	},
	"EPOCHS": {
		"choice": [500, 1000, 2000]
	},
	"MAXLEN": {
		"choice": [50, 100, 150, 200]
	},
	"UNIT": {
		"choice": [256, 600, 700]
	},
	"DROPOUT": {
		"choice": [0.0025, 0.025, 0.25]
	}
}
"""
input_variables = variables.get("INPUT_VARIABLES")
if input_variables is not None and input_variables != '':
    input_variables = json.loads(input_variables)
    MAXLEN = input_variables["MAXLEN"]
    UNIT = input_variables["UNIT"]
    DROPOUT = input_variables["DROPOUT"]
    OPTIMIZER = input_variables["OPTIMIZER"]
    EPOCHS = input_variables["EPOCHS"]
###########################################################################

# Get current job ID
PA_JOB_ID = variables.get("PA_JOB_ID")

# Check parent job ID
PARENT_JOB_ID = genericInformation.get('PARENT_JOB_ID')

# Define the path to save the model
OUTPUT_PATH = '/tmp'
MODEL_PATH = join(OUTPUT_PATH, 'model')
os.makedirs(MODEL_PATH, exist_ok=True)
print("DATA_PATH: " + DATA_PATH)

if DATA_PATH is not None and DATA_PATH.startswith("http"):
    # Get an unique ID
    ID = str(uuid.uuid4())
    # Define localspace
    LOCALSPACE = join('data', ID)
    os.makedirs(LOCALSPACE, exist_ok=True)
    print("LOCALSPACE:  " + LOCALSPACE)
    DATASET_NAME = splitext(DATA_PATH[DATA_PATH.rfind("/") + 1:])[0]
    DATASET_PATH = join(LOCALSPACE, DATASET_NAME)
    os.makedirs(DATASET_PATH, exist_ok=True)
    print("Dataset information: ")
    print("DATASET_NAME: " + DATASET_NAME)
    print("DATASET_PATH: " + DATASET_PATH)
    print("Downloading...")
    filename = wget.download(DATA_PATH, DATASET_PATH)
    print("FILENAME: " + filename)
    print("OK")


def load_text(filename, MAXLEN):
    # read text
    raw_text = open(filename, 'r', encoding='utf-8').read()
    dataX = []
    dataY = []
    # We cannot model the characters directly, instead we must convert the characters to integers
    # We can do this easily by first creating a set of all of the distinct characters in the equations, then creating a map of each character to a unique integer.
    # Also, when preparing the mapping of unique characters to integers
    chars = sorted(list(set(raw_text)))
    char_to_int = dict((c, i) for i, c in enumerate(chars))
    int_to_char = dict((i, c) for i, c in enumerate(chars))
    # Summary equation dataset
    n_chars = len(raw_text)
    n_vocab = len(chars)
    #print("Total Characters: ", n_chars)
    #print("Total Vocab: ", n_vocab)
    # Each training pattern of the network is comprised of 'maxlen' time steps of one character (X) followed by one character output (y).
    # When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course).
    #  Now, we convert the characters to integers using our lookup table we prepared earlier.
    for i in range(0, n_chars - MAXLEN, 1):
        seq_in = raw_text[i:i + MAXLEN]
        seq_out = raw_text[i + MAXLEN]
        dataX.append([char_to_int[char] for char in seq_in])
        dataY.append(char_to_int[seq_out])
    n_patterns = len(dataX)
    # print("Total Patterns: ", n_patterns)
    # Now that we have prepared our training data we need to transform it so that
    # it is suitable for use with Keras.
    # First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.
    # Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.
    # Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding.
    # reshape X to be [samples, time steps, features]
    X = np.reshape(dataX, (n_patterns, MAXLEN, 1))
    # normalize
    X = X / float(n_vocab)
    # one hot encode the output variable
    y = np_utils.to_categorical(dataY)
    return X, y, int_to_char, n_vocab, dataX, raw_text


X, y, int_to_char, n_vocab, dataX, raw_text = load_text(filename, MAXLEN)


# define the generator model
def build_generator():
    print('Building generator model...')
    generator = Sequential()
    generator.add(LSTM(UNIT, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
    generator.add(Dropout(DROPOUT))
    generator.add(LSTM(UNIT))
    generator.add(Dropout(DROPOUT))
    generator.add(Dense(y.shape[1], activation='softmax'))
    generator.summary()
    return generator


generator = build_generator()
generator.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER)


def train_text(epochs, batch_size, X, y):
    for epoch in range(epochs):
        # Select a random batch of images
        idx = np.random.randint(0, X.shape[0], batch_size)
        equation = X[idx]
        label = y[idx]
        # ---------------------
        #  Train Generator
        # ---------------------
        #g_loss = generator.train_on_batch(X, y, sample_weight=weights)
        loss = generator.train_on_batch(equation, label)
        print("%d[G loss: %f]" % (epoch, loss))
    return loss, generator


batch_size = 256
loss, generator = train_text(EPOCHS, batch_size, X, y)

#save model
file_path = join(MODEL_PATH, 'weights-'+str(PA_JOB_ID)+'.hdf5')
generator.save_weights(file_path)

def generate_text(generator, dataX, n_vocab, int_to_char, max_character):
    generated = ''
    data = dataX.copy()
    start = np.random.randint(0, len(data) - 1)
    pattern = data[start]
    for i in range(max_character):
        x = np.reshape(pattern, (1, len(pattern), 1))
        x = x / float(n_vocab)
        prediction = generator.predict(x, verbose=0)
        index = np.argmax(prediction)
        result = int_to_char[index]
        generated += result
        pattern.append(index)
        pattern = pattern[1:len(pattern)]
    return generated


generated = generate_text(generator, dataX, n_vocab, int_to_char, 400)
print('***********Text Generation************')
print(unicode(generated).encode('utf-8'))


############################ OUTPUT FOR AUTOML ############################
# Convert from JSON to dict
token = variables.get("TOKEN")
token = json.loads(token)

# Return the loss value
result_map = {'token': token, 'loss': str(loss)}
print('result_map: ', result_map)
resultMap.put("RESULT_JSON", json.dumps(result_map))

# To appear in Job Analytics
resultMap.put("LOSS", str(loss))
###########################################################################

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            137.53125
        </positionTop>
        <positionLeft>
            50.53125
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2144px;
            height:2820px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-132.53125px;left:-45.53125px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_171" style="top: 137.531px; left: 50.5312px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Train a simple Long Short-Term Memory to learn sequences of characters from 'The Alchemist' book. It's a novel by Brazilian author Paulo Coelho that was first published in 1988."><img src="/automation-dashboard/styles/patterns/img/wf-icons/keras.png" width="20px">&nbsp;<span class="name">Text_Generation</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 95px; top: 168px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
