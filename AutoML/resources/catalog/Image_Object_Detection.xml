<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Image_Object_Detection" onTaskError="continueJobExecution" priority="normal" tags="DistributedAutoML,HyperParameterOptimization,AutoML,TunningAlgorithms,Detection" projectName="3.  Hyperparameter Optimization" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable name="CONTAINER_PLATFORM" model="PA:LIST(no-container,docker,podman,singularity)" value="docker" advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false"/>
    <variable name="CONTAINER_IMAGE" model="PA:LIST(,docker://activeeon/dlm3,docker://activeeon/cuda,docker://activeeon/cuda2,docker://activeeon/nvidia:pytorch)" value="" advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false"/>
    <variable name="CONTAINER_GPU_ENABLED" model="PA:Boolean" value="True" description="If True, containers will run based on images containing libraries that are compatible with GPU." hidden="false" group="Container Parameters" advanced="true"/>
    <variable name="INPUT_VARIABLES" model="PA:JSON" value="{&quot;LEARNING_RATE&quot; : 0.0025, &quot;CONF_THRESHOLD&quot; : 0.1, &quot;NMS_THRESHOLD&quot; : 0.35, &quot;MOMENTUM&quot; : 0.09, &quot;WEIGHT_DECAY&quot; : 0.005}" description="A set of specific variables (usecase-related) that are used in the model training process." advanced="false" hidden="false"/>
    <variable name="SEARCH_SPACE" model="PA:JSON" value="{&quot;LEARNING_RATE&quot;:{&quot;choice&quot;: [0.0001, 0.00025]}, &quot;CONF_THRESHOLD&quot;:{&quot;choice&quot;: [0.1, 0.5]}, &quot;NMS_THRESHOLD&quot;:{&quot;choice&quot;: [0.35, 0.45]}, &quot;MOMENTUM&quot;:{&quot;choice&quot;: [0.0009, 0.009]}, &quot;WEIGHT_DECAY&quot;:{&quot;choice&quot;: [0.0005, 0.005]}}" description="Specifies the representation of the search space which has to be defined using dictionaries or by entering the path of a json file stored in the catalog." advanced="false" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ Train a YOLO model using the PyTorch library. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-auto-ml-optimization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/pytorch-logo-dark.png"/>
<info name="Documentation" value="PAIO/PAIOUserGuide.html#_hyperparameter_optimization"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="YOLO">
      <description>
        <![CDATA[ You Only Look Once (YOLO) is a  single neural network to predict bounding boxes and class probabilities.
You can see more details in: https://pjreddie.com/media/files/papers/YOLOv3.pdf
https://github.com/eriklindernoren/PyTorch-YOLOv3 ]]>
      </description>
      <variables>
        <variable inherited="false" name="LEARNING_RATE" value="0.001" description="Initial learning rate."/>
        <variable inherited="false" name="MOMENTUM" value="0.9" description="Momentum value for optimization."/>
        <variable inherited="false" name="WEIGHT_DECAY" value="0.0005" description="Weight decay for SGD."/>
        <variable inherited="false" name="IMG_SIZE" value="(416, 416)" description=" Specifies (width, height) of the images as a tuple with 2 elements."/>
        <variable inherited="false" name="NUM_CLASSES" value="81" description="Number of classes."/>
        <variable inherited="false" name="CONF_THRESHOLD" value="0.5" description="Confidence score that determines how certain the predicted bounding box actually encloses an object."/>
        <variable inherited="false" name="NMS_THRESHOLD" value="0.45" description="All the predicted bounding boxes that have a detection probability less than the given NMS threshold will be removed."/>
        <variable inherited="false" name="LABEL_PATH" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/coco.names" description="URL of the file containing the class names of the dataset."/>
        <variable inherited="false" name="USE_PRETRAINED_MODEL" value="True" description="Parameter to use pre-trained model for training. If True, the pre-trained model with the corresponding number of layers is loaded and used for training. Otherwise, the network is trained from scratch."/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_detection.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_hyperparameter_optimization"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/ai-auto-ml-optimization/resources/get_automl_params/raw"/>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import json
import numpy as np
from ast import literal_eval as make_tuple

IMG_SIZE = variables.get("IMG_SIZE") 
NUM_CLASSES = int(str(variables.get("NUM_CLASSES")))  
CONF_THRESHOLD  = float(str(variables.get("CONF_THRESHOLD"))) 
NMS_THRESHOLD = float(str(variables.get("NMS_THRESHOLD")))
LABEL_PATH  = variables.get("LABEL_PATH")
LEARNING_RATE = float(str(variables.get("LEARNING_RATE")))
MOMENTUM = float(str(variables.get("MOMENTUM")))
WEIGHT_DECAY = float(str(variables.get("WEIGHT_DECAY")))
USE_PRETRAINED_MODEL = variables.get("USE_PRETRAINED_MODEL") 
input_variables = variables.get("INPUT_VARIABLES")

############################ INPUT FROM AUTOML ############################
"""
SEARCH_SPACE:
{
	"LEARNING_RATE": {
		"choice": [0.0001, 0.00025]
	},
	"CONF_THRESHOLD": {
		"choice": [0.1, 0.5]
	},
	"NMS_THRESHOLD": {
		"choice": [0.35, 0.45]
	},
	"MOMENTUM": {
		"choice": [0.0009, 0.009]
	},
	"WEIGHT_DECAY": {
		"choice": [0.0005, 0.005]
	}
}

INPUT_VARIABLES:
{"LEARNING_RATE" : 0.0025, "CONF_THRESHOLD" : 0.1, "NMS_THRESHOLD" : 0.35, "MOMENTUM" : 0.09, "WEIGHT_DECAY" : 0.005}
"""

if input_variables is not None and input_variables !="":
    input_variables = json.loads(input_variables)
    LEARNING_RATE = input_variables["LEARNING_RATE"]
    CONF_THRESHOLD = input_variables["CONF_THRESHOLD"]
    NMS_THRESHOLD = input_variables["NMS_THRESHOLD"]
    MOMENTUM = input_variables["MOMENTUM"]
    WEIGHT_DECAY = input_variables["WEIGHT_DECAY"]
###########################################################################

print('-' * 30)
print('LEARNING_RATE:     ', LEARNING_RATE)
print('CONF_THRESHOLD:    ', CONF_THRESHOLD)
print('NMS_THRESHOLD:     ', NMS_THRESHOLD)
print('MOMENTUM:      	  ', MOMENTUM)
print('WEIGHT_DECAY:      ', WEIGHT_DECAY)
print('-' * 30)

IMG_SIZE = make_tuple(IMG_SIZE)
IMG_SIZE = tuple(IMG_SIZE)
NET_NAME = 'YOLO'

# Define the TRANSFORM functions
NET_TRANSFORM = """

from __future__ import division
import math
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

def load_classes(path):
    fp = open(path, "r")
    names = fp.read().split("\\n")[:-1]
    return names

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm2d') != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

def compute_ap(recall, precision):
# =============================================================================
#       Compute the average precision, given the recall and precision curves.
#    Code originally from https://github.com/rbgirshick/py-faster-rcnn.
# Arguments
#        recall:    The recall curve (list).
#        precision: The precision curve (list).
# Returns
#        The average precision as computed in py-faster-rcnn.
# =============================================================================

    # correct AP calculation
    # first append sentinel values at the end
    mrec = np.concatenate(([0.], recall, [1.]))
    mpre = np.concatenate(([0.], precision, [0.]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap

def bbox_iou(box1, box2, x1y1x2y2=True):
# =============================================================================
#   Returns the IoU of two bounding boxes
# =============================================================================
    if not x1y1x2y2:
        # Transform from center and width to exact coordinates
        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2
        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2
        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2
        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2
    else:
        # Get the coordinates of bounding boxes
        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]
        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]

    # get the corrdinates of the intersection rectangle
    inter_rect_x1 =  torch.max(b1_x1, b2_x1)
    inter_rect_y1 =  torch.max(b1_y1, b2_y1)
    inter_rect_x2 =  torch.min(b1_x2, b2_x2)
    inter_rect_y2 =  torch.min(b1_y2, b2_y2)
    # Intersection area
    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \
                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)
    # Union Area
    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)
    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)

    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)

    return iou


def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):
# =====================================================================================
#  Removes detections with lower object confidence score than 'conf_thres' and performs
#  Non-Maximum Suppression to further filter detections.
#  Returns detections with shape:
#   (x1, y1, x2, y2, object_conf, class_score, class_pred)
# =====================================================================================

    # From (center x, center y, width, height) to (x1, y1, x2, y2)
    box_corner = prediction.new(prediction.shape)
    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2
    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2
    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2
    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2
    prediction[:, :, :4] = box_corner[:, :, :4]

    output = [None for _ in range(len(prediction))]
    for image_i, image_pred in enumerate(prediction):
        # Filter out confidence scores below threshold
        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()
        image_pred = image_pred[conf_mask]
        # If none are remaining => process next image
        if not image_pred.size(0):
            continue
        # Get score and class with highest confidence
        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)
        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)
        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)
        # Iterate through all predicted classes
        unique_labels = detections[:, -1].cpu().unique()
        if prediction.is_cuda:
            unique_labels = unique_labels.cuda()
        for c in unique_labels:
            # Get the detections with the particular class
            detections_class = detections[detections[:, -1] == c]
            # Sort the detections by maximum objectness confidence
            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)
            detections_class = detections_class[conf_sort_index]
            # Perform non-maximum suppression
            max_detections = []
            while detections_class.size(0):
                # Get detection with highest confidence and save as max detection
                max_detections.append(detections_class[0].unsqueeze(0))
                # Stop if we're at the last detection
                if len(detections_class) == 1:
                    break
                # Get the IOUs for all boxes with lower confidence
                ious = bbox_iou(max_detections[-1], detections_class[1:])
                # Remove detections with IoU >= NMS threshold
                detections_class = detections_class[1:][ious < nms_thres]

            max_detections = torch.cat(max_detections).data
            # Add max detections to outputs
            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))

    return output

def build_targets(pred_boxes, target, anchors, num_anchors, num_classes, dim, ignore_thres, img_dim):
    nB = target.size(0)
    nA = num_anchors
    nC = num_classes
    dim = dim
    mask        = torch.zeros(nB, nA, dim, dim)
    tx         = torch.zeros(nB, nA, dim, dim)
    ty         = torch.zeros(nB, nA, dim, dim)
    tw         = torch.zeros(nB, nA, dim, dim)
    th         = torch.zeros(nB, nA, dim, dim)
    tconf      = torch.zeros(nB, nA, dim, dim)
    tcls       = torch.zeros(nB, nA, dim, dim, num_classes)

    nGT = 0
    nCorrect = 0
    for b in range(nB):
        for t in range(target.shape[1]):
            if target[b, t].sum() == 0:
                continue
            nGT += 1
            # Convert to position relative to box
            gx = target[b, t, 1] * dim
            gy = target[b, t, 2] * dim
            gw = target[b, t, 3] * dim
            gh = target[b, t, 4] * dim
            # Get grid box indices
            gi = int(gx)
            gj = int(gy)
            # Get shape of gt box
            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)
            # Get shape of anchor box
            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))
            # Calculate iou between gt and anchor shape
            anch_ious = bbox_iou(gt_box, anchor_shapes)
            # Find the best matching anchor box
            best_n = np.argmax(anch_ious)
            best_iou = anch_ious[best_n]
            # Get the ground truth box and corresponding best prediction
            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)
            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)

            # Masks
            mask[b, best_n, gj, gi] = 1
            # Coordinates
            tx[b, best_n, gj, gi] = gx - gi
            ty[b, best_n, gj, gi] = gy - gj
            # Width and height
            tw[b, best_n, gj, gi] = math.log(gw/anchors[best_n][0] + 1e-16)
            th[b, best_n, gj, gi] = math.log(gh/anchors[best_n][1] + 1e-16)
            # One-hot encoding of label
            tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1
            # Calculate iou between ground truth and best matching prediction
            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)
            tconf[b, best_n, gj, gi] = 1

            if iou > 0.5:
                nCorrect += 1

    return nGT, nCorrect, mask, tx, ty, tw, th, tconf, tcls
    
def to_categorical(y, NUM_CLASSES):
# =============================================================================
#    1-hot encodes a tensor
# =============================================================================
    return torch.from_numpy(np.eye(NUM_CLASSES, dtype='uint8')[y])

"""
#print(NET_TRANSFORM)

# Define the NET model
NET_MODEL = """

from __future__ import division

import torch
import numpy as np
from PIL import Image
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from collections import defaultdict

def parse_model_config(path):
# =============================================================================
#   Parses the yolo-v3 layer configuration file and returns module definitions
# =============================================================================
    file = open(path, 'r')
    lines = file.read().split('\\n')
    lines = [x for x in lines if x and not x.startswith('#')]
    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces
    module_defs = []
    for line in lines:
        if line.startswith('['): # This marks the start of a new block
            module_defs.append({})
            module_defs[-1]['type'] = line[1:-1].rstrip()
            if module_defs[-1]['type'] == 'convolutional':
                module_defs[-1]['batch_normalize'] = 0
        else:
            key, value = line.split("=")
            value = value.strip()
            module_defs[-1][key.rstrip()] = value.strip()

    return module_defs

def parse_data_config(path):
# =============================================================================
#   Parses the data configuration file
# =============================================================================
    options = dict()
    options['gpus'] = '0,1,2,3'
    options['num_workers'] = '10'
    with open(path, 'r') as fp:
        lines = fp.readlines()
    for line in lines:
        line = line.strip()
        if line == '' or line.startswith('#'):
            continue
        key, value = line.split('=')
        options[key.strip()] = value.strip()
    return options


def create_modules(module_defs):
# =====================================================================================
#   Constructs module list of layer blocks from module configuration in module_defs
# =====================================================================================
    hyperparams = module_defs.pop(0)
    output_filters = [int(hyperparams['channels'])]
    module_list = nn.ModuleList()
    for i, module_def in enumerate(module_defs):
        modules = nn.Sequential()

        if module_def['type'] == 'convolutional':
            bn = int(module_def['batch_normalize'])
            filters = int(module_def['filters'])
            kernel_size = int(module_def['size'])
            pad = (kernel_size - 1) // 2 if int(module_def['pad']) else 0
            modules.add_module('conv_%d' % i, nn.Conv2d(in_channels=output_filters[-1],
                                                        out_channels=filters,
                                                        kernel_size=kernel_size,
                                                        stride=int(module_def['stride']),
                                                        padding=pad,
                                                        bias=not bn))
            if bn:
                modules.add_module('batch_norm_%d' % i, nn.BatchNorm2d(filters))
            if module_def['activation'] == 'leaky':
                modules.add_module('leaky_%d' % i, nn.LeakyReLU(0.1))

        elif module_def['type'] == 'upsample':
            upsample = nn.Upsample( scale_factor=int(module_def['stride']),
                                    mode='nearest')
            modules.add_module('upsample_%d' % i, upsample)

        elif module_def['type'] == 'route':
            layers = [int(x) for x in module_def["layers"].split(',')]
            filters = sum([output_filters[layer_i] for layer_i in layers])
            modules.add_module('route_%d' % i, EmptyLayer())

        elif module_def['type'] == 'shortcut':
            filters = output_filters[int(module_def['from'])]
            modules.add_module("shortcut_%d" % i, EmptyLayer())

        elif module_def["type"] == "yolo":
            anchor_idxs = [int(x) for x in module_def["mask"].split(",")]
            # Extract anchors
            anchors = [int(x) for x in module_def["anchors"].split(",")]
            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]
            anchors = [anchors[i] for i in anchor_idxs]
            num_classes = int(module_def['classes'])
            img_height = int(hyperparams['height'])
            # Define detection layer
            yolo_layer = YOLOLayer(anchors, num_classes, img_height)
            modules.add_module('yolo_%d' % i, yolo_layer)
        # Register module list and number of output filters
        module_list.append(modules)
        output_filters.append(filters)

    return hyperparams, module_list

class EmptyLayer(nn.Module):
# =====================================================================================
#   Placeholder for 'route' and 'shortcut' layers
# =====================================================================================
    def __init__(self):
        super(EmptyLayer, self).__init__()

class YOLOLayer(nn.Module):
# =====================================================================================
#   Detection layer
# =====================================================================================
    def __init__(self, anchors, num_classes, img_dim):
        super(YOLOLayer, self).__init__()
        self.anchors = anchors
        self.num_anchors = len(anchors)
        self.num_classes = num_classes
        self.bbox_attrs = 5 + num_classes
        self.img_dim = img_dim
        self.ignore_thres = 0.5
        self.lambda_coord = 5
        self.lambda_noobj = 0.5

        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCELoss()

    def forward(self, x, targets=None):
        bs = x.size(0)
        g_dim = x.size(2)
        stride =  self.img_dim / g_dim
        # Tensors for cuda support
        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor

        prediction = x.view(bs,  self.num_anchors, self.bbox_attrs, g_dim, g_dim).permute(0, 1, 3, 4, 2).contiguous()

        # Get outputs
        x = torch.sigmoid(prediction[..., 0])          # Center x
        y = torch.sigmoid(prediction[..., 1])          # Center y
        w = prediction[..., 2]                         # Width
        h = prediction[..., 3]                         # Height
        conf = torch.sigmoid(prediction[..., 4])       # Conf
        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.

        # Calculate offsets for each grid
        grid_x = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).repeat(bs*self.num_anchors, 1, 1).view(x.shape).type(FloatTensor)
        grid_y = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).t().repeat(bs*self.num_anchors, 1, 1).view(y.shape).type(FloatTensor)
        scaled_anchors = [(a_w / stride, a_h / stride) for a_w, a_h in self.anchors]
        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(w.shape)
        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(h.shape)

        # Add offset and scale with anchors
        pred_boxes = FloatTensor(prediction[..., :4].shape)
        pred_boxes[..., 0] = x.data + grid_x
        pred_boxes[..., 1] = y.data + grid_y
        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w
        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h

        # Training
        if targets is not None:

            if x.is_cuda:
                self.mse_loss = self.mse_loss.cuda()
                self.bce_loss = self.bce_loss.cuda()

            nGT, nCorrect, mask, tx, ty, tw, th, tconf, tcls = build_targets(pred_boxes.cpu().data,
                                                                            targets.cpu().data,
                                                                            scaled_anchors,
                                                                            self.num_anchors,
                                                                            self.num_classes,
                                                                            g_dim,
                                                                            self.ignore_thres,
                                                                            self.img_dim)

            nProposals = int((conf > 0.25).sum().item())
            recall = float(nCorrect / nGT) if nGT else 1

            mask = Variable(mask.type(FloatTensor))

            tx    = Variable(tx.type(FloatTensor), requires_grad=False)
            ty    = Variable(ty.type(FloatTensor), requires_grad=False)
            tw    = Variable(tw.type(FloatTensor), requires_grad=False)
            th    = Variable(th.type(FloatTensor), requires_grad=False)
            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)
            tcls  = Variable(tcls.type(FloatTensor), requires_grad=False)

            # Mask outputs to ignore non-existing objects (but keep confidence predictions)
            loss_x = self.lambda_coord * self.bce_loss(x * mask, tx * mask) / 2
            loss_y = self.lambda_coord * self.bce_loss(y * mask, ty * mask) / 2
            loss_w = self.lambda_coord * self.mse_loss(w * mask, tw * mask) / 2
            loss_h = self.lambda_coord * self.mse_loss(h * mask, th * mask) / 2
            loss_conf = self.bce_loss(conf * mask, mask) + \
                        self.lambda_noobj * self.bce_loss(conf * (1 - mask), mask * (1 - mask))
            loss_cls = self.bce_loss(pred_cls[mask == 1], tcls[mask == 1])
            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls

            return loss, loss_x.item(), loss_y.item(), loss_w.item(), loss_h.item(), loss_conf.item(), loss_cls.item(), recall

        else:
            # If not in training phase return predictions
            output = torch.cat((pred_boxes.view(bs, -1, 4) * stride, conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1)
            return output.data


class Darknet(nn.Module):
# =====================================================================================
#   YOLOv3 object detection model
# =====================================================================================
    def __init__(self, config_path, img_size=416):
        super(Darknet, self).__init__()
        self.module_defs = parse_model_config(config_path)
        self.hyperparams, self.module_list = create_modules(self.module_defs)
        self.img_size = img_size
        self.seen = 0
        self.header_info = np.array([0, 0, 0, self.seen, 0])
        self.loss_names = ['x', 'y', 'w', 'h', 'conf', 'cls', 'recall']

    def forward(self, x, targets=None):
        is_training = targets is not None
        output = []
        self.losses = defaultdict(float)
        layer_outputs = []
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if module_def['type'] in ['convolutional', 'upsample']:
                x = module(x)
            elif module_def['type'] == 'route':
                layer_i = [int(x) for x in module_def['layers'].split(',')]
                x = torch.cat([layer_outputs[i] for i in layer_i], 1)
            elif module_def['type'] == 'shortcut':
                layer_i = int(module_def['from'])
                x = layer_outputs[-1] + layer_outputs[layer_i]
            elif module_def['type'] == 'yolo':
                # Train phase: get loss
                if is_training:
                    x, *losses = module[0](x, targets)
                    for name, loss in zip(self.loss_names, losses):
                        self.losses[name] += loss
                # Test phase: Get detections
                else:
                    x = module(x)
                output.append(x)
            layer_outputs.append(x)

        self.losses['recall'] /= 3
        return sum(output) if is_training else torch.cat(output, 1)


    def load_weights(self, weights_path):
# =====================================================================================
#  Parses and loads the weights stored in 'weights_path
# =====================================================================================

        #Open the weights file
        fp = open(weights_path, "rb")
        header = np.fromfile(fp, dtype=np.int32, count=5)   # First five are header values

        # Needed to write header when saving weights
        self.header_info = header

        self.seen = header[3]
        weights = np.fromfile(fp, dtype=np.float32)         # The rest are weights
        fp.close()

        ptr = 0
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if module_def['type'] == 'convolutional':
                conv_layer = module[0]
                if module_def['batch_normalize']:
                    # Load BN bias, weights, running mean and running variance
                    bn_layer = module[1]
                    num_b = bn_layer.bias.numel() # Number of biases
                    # Bias
                    bn_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.bias)
                    bn_layer.bias.data.copy_(bn_b)
                    ptr += num_b
                    # Weight
                    bn_w = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.weight)
                    bn_layer.weight.data.copy_(bn_w)
                    ptr += num_b
                    # Running Mean
                    bn_rm = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_mean)
                    bn_layer.running_mean.data.copy_(bn_rm)
                    ptr += num_b
                    # Running Var
                    bn_rv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_var)
                    bn_layer.running_var.data.copy_(bn_rv)
                    ptr += num_b
                else:
                    # Load conv. bias
                    num_b = conv_layer.bias.numel()
                    conv_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(conv_layer.bias)
                    conv_layer.bias.data.copy_(conv_b)
                    ptr += num_b
                # Load conv. weights
                num_w = conv_layer.weight.numel()
                conv_w = torch.from_numpy(weights[ptr:ptr + num_w]).view_as(conv_layer.weight)
                conv_layer.weight.data.copy_(conv_w)
                ptr += num_w

# =====================================================================================
#  @:param path    - path of the new weights file
#  @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)
# =====================================================================================
    def save_weights(self, path, cutoff=-1):

        fp = open(path, 'wb')
        self.header_info[3] = self.seen
        self.header_info.tofile(fp)

        # Iterate through layers
        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):
            if module_def['type'] == 'convolutional':
                conv_layer = module[0]
                # If batch norm, load bn first
                if module_def['batch_normalize']:
                    bn_layer = module[1]
                    bn_layer.bias.data.cpu().numpy().tofile(fp)
                    bn_layer.weight.data.cpu().numpy().tofile(fp)
                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)
                    bn_layer.running_var.data.cpu().numpy().tofile(fp)
                # Load conv bias
                else:
                    conv_layer.bias.data.cpu().numpy().tofile(fp)
                # Load conv weights
                conv_layer.weight.data.cpu().numpy().tofile(fp)

        fp.close()
"""
#print(NET_TRANSFORM)

variables.put("NET_MODEL", NET_MODEL)
variables.put("NET_TRANSFORM", NET_TRANSFORM)
variables.put("IMG_SIZE", IMG_SIZE)
variables.put("NUM_CLASSES", NUM_CLASSES)
variables.put("LEARNING_RATE",  LEARNING_RATE)
variables.put("MOMENTUM",  MOMENTUM)
variables.put("WEIGHT_DECAY", WEIGHT_DECAY)
variables.put("CONF_THRESHOLD", CONF_THRESHOLD)
variables.put("NMS_THRESHOLD", NMS_THRESHOLD)
variables.put("LABEL_PATH", LABEL_PATH)
variables.put("USE_PRETRAINED_MODEL", USE_PRETRAINED_MODEL)
variables.put("NET_NAME", NET_NAME)

print("END YOLO")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            181.2216033935547
        </positionTop>
        <positionLeft>
            176.54830932617188
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Import_Image_Dataset">
      <description>
        <![CDATA[ Load and return an image dataset. ]]>
      </description>
      <variables>
        <variable inherited="false" name="DATA_PATH" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/coco.zip" description="Path or name of the file that contains the image dataset."/>
        <variable inherited="false" name="TRAIN_SPLIT" value="0.60" description="Must be a float within the range (0.0, 1.0), not including the values 0.0 and 1.0."/>
        <variable inherited="false" name="VAL_SPLIT" value="0.15" description="Must be a float within the range (0.0, 1.0), not including the values 0.0 and 1.0."/>
        <variable inherited="false" name="TEST_SPLIT" value="0.25" description="Must be a float within the range (0.0, 1.0), not including the values 0.0 and 1.0."/>
        <variable inherited="false" name="DATASET_TYPE" model="PA:LIST(Classification, Detection, Segmentation)" value="Detection" description="Type of the image dataset to load."/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_import_image_dataset"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <file language="cpython" url="${PA_CATALOG_REST_URL}/buckets/ai-deep-learning/resources/Import_Image_Dataset_Script/raw"/>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <outputFiles>
        <files accessMode="transferToGlobalSpace" includes="$DATASET_PATH/**"/>
      </outputFiles>
      <metadata>
        <positionTop>
            181.2216033935547
        </positionTop>
        <positionLeft>
            304.5454406738281
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Train_Image_Object_Detection_Model">
      <description>
        <![CDATA[ Train a model using an image object detection network. ]]>
      </description>
      <variables>
        <variable inherited="false" name="NUM_EPOCHS" value="1" description="Number of times all the training vectors are used once to update the weights."/>
        <variable inherited="false" name="BATCH_SIZE" value="1" description="Batch size to be used."/>
        <variable inherited="false" name="NUM_WORKERS" value="1" description="Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process."/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_train.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_train_image_segmentation_model"/>
      </genericInformation>
      <depends>
        <task ref="YOLO"/>
        <task ref="Import_Image_Dataset"/>
      </depends>
      <inputFiles>
        <files accessMode="transferFromGlobalSpace" includes="$DATASET_PATH/**"/>
        <files accessMode="transferFromGlobalSpace" includes="$LABEL_PATH/**"/>
      </inputFiles>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/ai-auto-ml-optimization/resources/get_automl_token/raw"/>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")
print("BEGIN " + __file__)

import os
import json
import cv2
import sys
import time
import wget
import copy
import uuid
import types
import torch
import pprint
import zipfile
import numpy as np
import os.path as osp
from numpy import random

# Check GPU support
CONTAINER_GPU_ENABLED = variables.get("CONTAINER_GPU_ENABLED")
if CONTAINER_GPU_ENABLED is not None and CONTAINER_GPU_ENABLED.lower() == 'true':
    import GPUtil as GPU
    from random import randint, uniform
    # Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # Show the utilization of all GPUs in a nice table
    print("Current GPU utilization:")
    GPU.showUtilization()
    # Get the first available GPU
    # order - The order in which the available GPU device ids are returned: first, last, random, load, memory
    # limit - Limits the number of GPU device ids returned. Must be positive integer. (default = 1)
    # maxLoad - Maximum current relative load for a GPU to be considered available. (default = 0.5)
    # maxMemory - Maximum current relative memory usage for a GPU to be considered available. (default = 0.5)
    # attempts - Number of attempts before giving up finding an available GPU. (default = 1)
    # interval - Interval in seconds between each attempt to find an available GPU. (default = 900 --> 15 min)
    # verbose - If True, prints the attempt number before each attempt and the GPU id if an available is found.
    # NOTE: this step will fail if all GPUs are busy
    # deviceIDs = GPUtil.getAvailable(order='first', limit=1, maxLoad=0.5, maxMemory=0.5)
    maxMemory = 0.6  # round(uniform(0.2, 0.5), 2)
    maxLoad = 0.6    # round(uniform(0.2, 0.5), 2)
    attempts = 10    # internal attempts
    interval = 10    # interval = randint(3, 30)  # 3sec to 30sec
    print('Looking for available GPU id with memory < ' +
          str(maxMemory * 100) + '%, and load < ' + str(maxLoad * 100) + '%)')
    print('# of attempts: ' + str(attempts) + ', interval:' + str(interval))
    DEVICE_ID_LIST = GPU.getFirstAvailable(
        order='random', maxMemory=maxMemory, maxLoad=maxLoad, attempts=attempts, interval=interval
    )
    DEVICE_ID = DEVICE_ID_LIST[0]  # grab first element from list
    print('First available GPU id (memory < ' +
          str(maxMemory * 100) + '%, load < ' + str(maxLoad * 100) + '%):')
    print(DEVICE_ID)
    # Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id
    os.environ["CUDA_VISIBLE_DEVICES"] = str(DEVICE_ID)

import torch
import torch.nn as nn
from PIL import Image
import torch.optim as optim

import torch.nn.init as init
import torch.nn.functional as F
import torch.utils.data as data

from torch.optim import SGD, Adam
from torch.autograd import Function
from torch.autograd import Variable
import torch.backends.cudnn as cudnn

from math import sqrt as sqrt
from skimage.transform import resize
from itertools import product as product
from os import remove, listdir, makedirs
from ast import literal_eval as make_tuple
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from os.path import basename, splitext, exists, join
from lxml.etree import Element, SubElement, tostring
from xml.dom.minidom import parseString
if sys.version_info[0] == 2:
    import xml.etree.cElementTree as ET
else:
    import xml.etree.ElementTree as ET

NUM_EPOCHS = int(str(variables.get("NUM_EPOCHS")))
NUM_CLASSES = int(str(variables.get("NUM_CLASSES")))
NET_MODEL     = variables.get("NET_MODEL")
NET_TRANSFORM = variables.get("NET_TRANSFORM")
NET_CRITERION = variables.get("NET_CRITERION")
DATASET_PATH  = variables.get("DATASET_PATH")
LABEL_PATH  = variables.get("LABEL_PATH")
IMG_SIZE = variables.get("IMG_SIZE")
LEARNING_RATE = float(str(variables.get("LEARNING_RATE")))
MOMENTUM = float(str(variables.get("MOMENTUM")))
WEIGHT_DECAY = float(str(variables.get("WEIGHT_DECAY")))
BATCH_SIZE = int(str(variables.get("BATCH_SIZE")))
NUM_WORKERS = int(str(variables.get("NUM_WORKERS")))
NET_NAME = variables.get("NET_NAME")
USE_PRETRAINED_MODEL = variables.get("USE_PRETRAINED_MODEL")

assert DATASET_PATH is not None
assert NET_MODEL is not None
assert NET_TRANSFORM is not None

IMG_SIZE = tuple(IMG_SIZE)

# Save onnx model
MODEL_TYPE_ONNX = False

# Download class file 
print("Downloading...")
filename = wget.download(LABEL_PATH)
print("FILENAME: " + filename)
print("OK")

# Class names
CLASSES = tuple(open(filename).read().splitlines())

DATASET_TRAIN_PATH = join(DATASET_PATH, 'train')
DIR_EXT = join(DATASET_TRAIN_PATH, 'classes')

## Get extension class folder
files = os.listdir(DIR_EXT);
checkds_store = '.DS_Store' in files
if checkds_store == True:
    files.remove('.DS_Store')
file_name = files[0]
base_file, ext = os.path.splitext(file_name)

# Check if gpu is available
use_gpu = torch.cuda.is_available()
print("Using GPU: ", use_gpu)

# Get an unique ID
ID = str(uuid.uuid4())

# Create an empty dir
MODEL_FOLDER = join('models', ID)
os.makedirs(MODEL_FOLDER, exist_ok=True)
print("MODEL_FOLDER: " + MODEL_FOLDER)

##################################  BEGIN SSD NET ######################################
def weights_init(m):
    if isinstance(m, nn.Conv2d):
       # xavier(m.weight.data)
        nn.init.xavier_uniform(m.weight.data)
        m.bias.data.zero_()
def adjust_learning_rate(optimizer, GAMMA, step):
# ======================================================================================
#    Sets the learning rate to the initial LR decayed by 10 at every
#        specified step
#    # Adapted from PyTorch Imagenet example:
#    # https://github.com/pytorch/examples/blob/master/imagenet/main.py
# ======================================================================================
    LEARNING_RATE = LR_FACTOR * (GAMMA ** (step))
    for param_group in optimizer.param_groups:
        param_group['LEARNING_RATE'] = LR
# ======================================================================================   
# Begin Load Dataset COCO and PASCAL VOC formats
# ======================================================================================  
class VOCAnnotationTransform(object):
# =============================================================================
#    Transforms a VOC annotation into a Tensor of bbox coords and label index
#    Initilized with a dictionary lookup of classnames to indexes
#
#    Arguments:
#        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes
#            (default: alphabetic indexing of VOC's 20 classes)
#        keep_difficult (bool, optional): keep difficult instances or not
#            (default: False)
#        height (int): height
#        width (int): width
# =============================================================================
    def __init__(self, class_to_ind=None, keep_difficult=False):
        self.class_to_ind = class_to_ind or dict(
            zip(CLASSES, range(len(CLASSES))))
        self.keep_difficult = keep_difficult
    def __call__(self, target, width, height):
# ======================================================================================
#        Arguments:
#            target (annotation) : the target annotation to be made usable
#                will be an ET.Element
#        Returns:
#            a list containing lists of bounding boxes  [bbox coords, class name]
# ======================================================================================
        res = []
        for obj in target.iter('object'):
            difficult = int(obj.find('difficult').text) == 1
            if not self.keep_difficult and difficult:
                continue
            name = obj.find('name').text.lower().strip()
            bbox = obj.find('bndbox')
            pts = ['xmin', 'ymin', 'xmax', 'ymax']
            bndbox = []
            for i, pt in enumerate(pts):
                cur_pt = int(bbox.find(pt).text) - 1
                # scale height or width
                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height
                bndbox.append(cur_pt)
            label_idx = self.class_to_ind[name]
            bndbox.append(label_idx)
            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]
            # img_id = target.find('filename').text[:-4]
        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]
class LoadDetection(data.Dataset):
# ======================================================================================
#    PASCAL VOC 2007 format  - Detection Dataset Object
#
#    input is image, target is annotation
#
#    Arguments:
#        root (string): filepath to VOCdevkit folder.
#        image_set (string): imageset to use (eg. 'train', 'val', 'test')
#        transform (callable, optional): transformation to perform on the
#            input image
#        target_transform (callable, optional): transformation to perform on the
#            target `annotation`
#            (eg: take in caption string, return tensor of word indices)
#        dataset_name (string, optional): which dataset to load   
# ======================================================================================
    def __init__(self, root, transform=None):
        labels_root = join(root, 'classes')
        target_transform=VOCAnnotationTransform()
        self.transform = transform
        self.target_transform = target_transform        
        self._annopath = osp.join(root, 'classes', '%s.xml')
        self._imgpath = osp.join(root, 'images', '%s.jpg')
        self.ids = list()
        l=os.listdir(labels_root)
        self.ids = [x.split('.')[0] for x in l]
    def __getitem__(self, index):
        im, gt, h, w = self.pull_item(index)
        return im, gt
    def __len__(self):
        return len(self.ids)
    def pull_item(self, index):
        img_id = self.ids[index] 
        target = ET.parse(self._annopath % img_id).getroot()
        img = cv2.imread(self._imgpath % img_id)
        height, width, channels = img.shape
        if self.target_transform is not None:
            target = self.target_transform(target, width, height)
        if self.transform is not None:
            target = np.array(target)
            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])
            # to rgb
            img = img[:, :, (2, 1, 0)]
            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))
        return torch.from_numpy(img).permute(2, 0, 1), target, height, width
# ====================================================================================== 
# End Load Dataset format COCO and PASCAL VOC formats
# ======================================================================================    

# ======================================================================================
## COCO format to PASCAL VOC format
# ======================================================================================
def unconvert(class_id, width, height, x, y, w, h):
    xmax = int((x*width) + (w * width)/2.0)
    xmin = int((x*width) - (w * width)/2.0)
    ymax = int((y*height) + (h * height)/2.0)
    ymin = int((y*height) - (h * height)/2.0)
    class_id = int(class_id)
    return (class_id, xmin, xmax, ymin, ymax)
def xml_transform(root, classes):
    annopath = join(root, 'classes', '%s.txt')
    _imgpath = join(root, 'images', '%s.jpg')
    new_annopath = join(root, 'classes', '%s.xml')
    mypath = join(root, 'classes')
    ids = list()
    l = os.listdir(mypath)
    ids = [x.split('.')[0] for x in l]
    for i in range(len(ids)):
        img_id = ids[i] 
        img= cv2.imread(_imgpath % img_id)
        height, width, channels = img.shape
        node_root = Element('annotation')
        node_folder = SubElement(node_root, 'folder')
        node_folder.text = 'VOC2007'
        img_name = img_id + '.jpg'
        node_filename = SubElement(node_root, 'filename')
        node_filename.text = img_name
        node_source= SubElement(node_root, 'source')
        node_database = SubElement(node_source, 'database')
        node_database.text = 'Coco database'
        node_size = SubElement(node_root, 'size')
        node_width = SubElement(node_size, 'width')
        node_width.text = str(width)
        node_height = SubElement(node_size, 'height')
        node_height.text = str(height)
        node_depth = SubElement(node_size, 'depth')
        node_depth.text = str(channels)
        node_segmented = SubElement(node_root, 'segmented')
        node_segmented.text = '0'
        target = (annopath % img_id)
        if os.path.exists(target):
            label_norm = np.loadtxt(target).reshape(-1, 5)
            for i in range(len(label_norm)):
                labels_conv = label_norm[i]
                new_label = unconvert(labels_conv[0], width, height, labels_conv[1], labels_conv[2], labels_conv[3], labels_conv[4])
                node_object = SubElement(node_root, 'object')
                node_name = SubElement(node_object, 'name')
                node_name.text = CLASSES[new_label[0]]
                node_pose = SubElement(node_object, 'pose')
                node_pose.text = 'Unspecified'
                node_truncated = SubElement(node_object, 'truncated')
                node_truncated.text = '0'  
                node_difficult = SubElement(node_object, 'difficult')
                node_difficult.text = '0'
                node_bndbox = SubElement(node_object, 'bndbox')
                node_xmin = SubElement(node_bndbox, 'xmin')
                node_xmin.text = str(new_label[1])
                node_ymin = SubElement(node_bndbox, 'ymin')
                node_ymin.text = str(new_label[3])
                node_xmax = SubElement(node_bndbox, 'xmax')
                node_xmax.text =  str(new_label[2])
                node_ymax = SubElement(node_bndbox, 'ymax')
                node_ymax.text = str(new_label[4])
                xml = tostring(node_root, pretty_print=True)
        f =  open(new_annopath % img_id, "wb")
        os.remove(target)
        f.write(xml)
        f.close()
# ======================================================================================
# END COCO format to PASCAL VOC format
# ======================================================================================

##################################  BEGIN SSD NET ######################################
if (NET_NAME == 'SSD'):
    LR_STEPS = variables.get("LR_STEPS")
    LR_FACTOR = variables.get("LR_FACTOR")    
    START_ITERATION  = int(str(variables.get("START_ITERATION")))
    MAX_ITERATION = int(str(variables.get("MAX_ITERATION")))
    MIN_SIZES = variables.get("MIN_SIZES")
    MAX_SIZES = variables.get("MAX_SIZES")
    MEANS = (104, 117, 123)
    BUILD_TYPE = 'train'
    LR_STEPS = tuple(LR_STEPS)
    MIN_SIZES  = make_tuple(MIN_SIZES)
    MIN_SIZES  = tuple(MIN_SIZES)
    MIN_SIZES  = list(MIN_SIZES)
    MAX_SIZES  = make_tuple(MAX_SIZES)
    MAX_SIZES  = tuple(MAX_SIZES)
    MAX_SIZES  = list(MAX_SIZES)
    DIR_EXT = join(DATASET_TRAIN_PATH, 'classes')
    files = os.listdir(DIR_EXT)
    checkds_store = '.DS_Store' in files
    if checkds_store == True:
        files.remove('.DS_Store')
    filename = files[0]
    base_file, ext = os.path.splitext(filename)
    if ext == '.txt':  # input coco format
        xml_transform(DATASET_TRAIN_PATH, CLASSES)
    # Load NET model
    exec(NET_MODEL)
    MODEL_NAME = 'SSD'
    ssd_net = build_ssd(BUILD_TYPE, IMG_SIZE[0], NUM_CLASSES)
    Net = ssd_net
    assert Net is not None, f'model {MODEL_NAME} not available'
    model = Net
    # Load criterion NET 
    exec(NET_CRITERION)
    # Load transform NET
    exec(NET_TRANSFORM)
    if use_gpu:
        torch.cuda.FloatTensor
    else:
        print("WARNING: It looks like you have a CUDA device, but aren't " +  "using CUDA.\nRun with --cuda for optimal training speed.")
        torch.FloatTensor
    if use_gpu:
        # model = torch.nn.DataParallel(ssd_net)
        model = ssd_net
        cudnn.benchmark = True
    # download VGG model
    print("Downloading...")
    VGG_MODEL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/vgg16_reducedfc.pth'
    filename = wget.download(VGG_MODEL)
    print("VGG MODEL: " + filename)
    resume = None 
    model_config_path = os.path.realpath(filename)
    if USE_PRETRAINED_MODEL == False:
        print('Resuming training, loading {}...'.format(resume))
        ssd_net.load_weights(None)
    else:
        vgg_weights = torch.load(model_config_path)
        print('Loading base network...')
        ssd_net.vgg.load_state_dict(vgg_weights)
    if use_gpu:
        model = model.cuda()
    if not resume:
        print('Initializing weights...')
        # initialize newly added layers' weights with xavier method
        ssd_net.extras.apply(weights_init)
        ssd_net.loc.apply(weights_init)
        ssd_net.conf.apply(weights_init)
    optimizer_ft = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
    criterion_ft = MultiBoxLoss(NUM_CLASSES, 0.5, True, 0, True, 3, 0.5, False, use_gpu)
    model.train()
    print('Loading the dataset...')
    dataset = LoadDetection(root=DATASET_TRAIN_PATH, transform=SSDAugmentation(IMG_SIZE[0], MEANS))
    # epoch_size = len(dataset) // BATCH_SIZE
    epoch_size = 1
    data_loader = data.DataLoader(dataset, BATCH_SIZE, num_workers=NUM_WORKERS, 
                                  shuffle=True, collate_fn=detection_collate, 
                                  pin_memory=True)
    def train_model(model, optimizer, criterion, start_iter, max_iter, data_loader, use_gpu):
        # loss counters
        loc_loss = 0
        conf_loss = 0
        epoch = 0
        step_index = 0
        since = time.time()
        batch_iterator = None
        for iteration in range(START_ITERATION, MAX_ITERATION):
            if (not batch_iterator) or (iteration % epoch_size == 0):
                batch_iterator = iter(data_loader)
                # reset epoch loss counters            
                loc_loss = 0
                conf_loss = 0
                epoch += 1
            if iteration in LR_STEPS: 
                step_index += 1
                adjust_learning_rate(optimizer, GAMMA, step_index)
            # load train data      
            images, targets = next(batch_iterator)
            if use_gpu:
                images = Variable(images).cuda()
                targets = [Variable(ann).cuda() for ann in targets]                
            else:
                images = Variable(images)
                targets = [Variable(ann) for ann in targets]
            # forward 
            outputs = model(images)
            # backprop
            optimizer.zero_grad()
            loss_l, loss_c = criterion(outputs, targets)
            loss = loss_l + loss_c
            print(loss)
            loss.backward()
            optimizer.step()
            t1 = time.time()
            loc_loss += loss_l.item()
            conf_loss += loss_c.item()
            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.item()), end=' ')
            time_elapsed = time.time() - since
        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
        return model
    # Return the model 
    model = train_model(model, optimizer_ft, criterion_ft, START_ITERATION, MAX_ITERATION, data_loader, use_gpu)
    # Save pytorch trained model
    print('Saving trained model...')
    MODEL_PATH = join(MODEL_FOLDER, "model.pt")
    torch.save(model.state_dict(), MODEL_PATH)
    print("Model information: ")
    print("MODEL_PATH:  " + MODEL_PATH)
    # Save onnx trained model
    if MODEL_TYPE_ONNX:
        MODEL_ONNX_PATH = None
        print('The SSD network does not yet support the ONNX format!')
    else: 
        MODEL_ONNX_PATH = None
    variables.put("MODEL_FOLDER", MODEL_FOLDER)
    variables.put("MODEL_PATH", MODEL_PATH)
    variables.put("MODEL_ONNX_PATH", MODEL_ONNX_PATH)
    variables.put("MEANS", MEANS)
    print("END Train_Image_Object_Detection_Model")
###################################  END SSD NET #######################################

# ======================================================================================
## PASCAL VOC format to COCO format
# ======================================================================================
def convert(size, box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = (box[0] + box[1])/2.0
    y = (box[2] + box[3])/2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)
def convert_annotation(input_file, output_file, labels_root):
    in_file = open(input_file)
    out_file = open(labels_root +'/' + output_file + '.txt', 'w')
    tree=ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)
    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in CLASSES or int(difficult) == 1:
            continue
        cls_id = CLASSES.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))
        bb = convert((w,h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')
# ======================================================================================
# END PASCAL VOC format to COCO format
# ======================================================================================

# ======================================================================================
# BEGIN READ PASCAL VOC 2012
# ======================================================================================
class ListDataset(Dataset):
    def __init__(self, list_path, img_size=416):
        images_root = join(list_path, 'images')
        labels_root = join(list_path, 'classes')
        self.img_files = [os.path.join(r,file) for r,d,f in os.walk(images_root) for file in f]
        self.label_files = [os.path.join(r,file) for r,d,f in os.walk(labels_root) for file in f]
        self.img_shape = (img_size, img_size)
        self.max_objects = 50
    def __getitem__(self, index):
        #---------
        #  Image
        #---------
        img_path = self.img_files[index % len(self.img_files)].rstrip()
        img = np.array(Image.open(img_path))
        # Black and white images
        if len(img.shape) == 2:
            img = np.repeat(img[:, :, np.newaxis], 3, axis=2)
        h, w, _ = img.shape
        dim_diff = np.abs(h - w)
        # Upper (left) and lower (right) padding
        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2
        # Determine padding
        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))
        # Add padding
        input_img = np.pad(img, pad, 'constant', constant_values=128) / 255.
        padded_h, padded_w, _ = input_img.shape
        # Resize and normalize
        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')
        # Channels-first
        input_img = np.transpose(input_img, (2, 0, 1))
        # As pytorch tensor
        input_img = torch.from_numpy(input_img).float()
        #---------
        #  Label
        #---------
        label_path = self.label_files[index % len(self.img_files)].rstrip()
        labels = None
        if os.path.exists(label_path):
            labels = np.loadtxt(label_path).reshape(-1, 5)
            # Extract coordinates for unpadded + unscaled image
            x1 = w * (labels[:, 1] - labels[:, 3]/2)
            y1 = h * (labels[:, 2] - labels[:, 4]/2)
            x2 = w * (labels[:, 1] + labels[:, 3]/2)
            y2 = h * (labels[:, 2] + labels[:, 4]/2)
            # Adjust for added padding
            x1 += pad[1][0]
            y1 += pad[0][0]
            x2 += pad[1][0]
            y2 += pad[0][0]
            # Calculate ratios from coordinates
            labels[:, 1] = ((x1 + x2) / 2) / padded_w
            labels[:, 2] = ((y1 + y2) / 2) / padded_h
            labels[:, 3] *= w / padded_w
            labels[:, 4] *= h / padded_h
        # Fill matrix
        filled_labels = np.zeros((self.max_objects, 5))
        if labels is not None:
            filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]
        filled_labels = torch.from_numpy(filled_labels)
        return img_path, input_img, filled_labels
    def __len__(self):
        return len(self.img_files)
# ======================================================================================
# END READ PASCAL VOC 2012
# ======================================================================================

##################################  BEGIN YOLO NET #####################################
if (NET_NAME == 'YOLO'):
    ##  Download Model config
    print("Downloading...")
    MODEL_CONFIG_PATH = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/yolov3.cfg'
    filename = wget.download(MODEL_CONFIG_PATH)
    print("MODEL_CONFIG_PATH: " + filename)
    model_config_path = os.path.realpath(filename)
    # download initial weights
    print("Downloading...")
    MODEL_WEIGHT_PATH = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/yolov3.weights'
    file_weights = wget.download(MODEL_WEIGHT_PATH)
    print("MODEL_WEIGHT_PATH: " + file_weights)
    model_weight_path = os.path.realpath(file_weights)
    # xml to txt class files
    l = os.listdir(DIR_EXT)
    ids = [x.split('.')[0] for x in l]
    # converts all .xml to .txt files 
    if ext == '.xml':
        cont = 0
        for f in os.listdir(DIR_EXT):
            if not f.startswith('.'):
                input_file = join(DIR_EXT, f)
                convert_annotation(input_file, ids[cont], DIR_EXT)
            cont += 1
        # renove all .xml files 
        filelist = [ f for f in os.listdir(DIR_EXT) if f.endswith(".xml") ]
        for f in filelist:
            os.remove(os.path.join(DIR_EXT, f))
    # Load NET transforms
    exec(NET_TRANSFORM)
    # Load NET model
    exec(NET_MODEL)
    MODEL_NAME = 'YOLO'
    Net = Darknet(model_config_path)
    if not USE_PRETRAINED_MODEL:
        print('Loading initial weights network...')
        Net.apply(weights_init_normal)
        assert Net is not None, f'model {MODEL_NAME} not available'
        model = Net
    else:
        print('Loading base network...')
        Net.load_weights(model_weight_path)
        assert Net is not None, f'model {MODEL_NAME} not available'
        model = Net
    if use_gpu:
        model = model.cuda()
    model.train()
    loader = DataLoader(ListDataset(DATASET_TRAIN_PATH), num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)
    optimizer_ft = SGD(model.parameters(), lr=LEARNING_RATE/BATCH_SIZE, momentum=MOMENTUM, dampening=0, weight_decay=WEIGHT_DECAY*BATCH_SIZE)
    def train_model(model, optimizer, num_epochs):
        since = time.time()
        best_model = model
        best_loss = None
        for epoch in range(1, NUM_EPOCHS+1):
            for batch_i, (_, images, labels) in enumerate(loader):
                if use_gpu:
                    images = images.cuda()
                    labels = labels.cuda()
                inputs = Variable(images)
                targets = Variable(labels, requires_grad=False)
                optimizer.zero_grad()
                loss = model(inputs, targets)
                loss.backward()
                optimizer.step()
                print('[Epoch %d/%d, Batch %d/%d] [Losses -  x: %f, y: %f, w: %f, h: %f, conf: %f, cls: %f, total: %f, recall: %.5f]' %
                                      (epoch, epoch, batch_i, len(loader),
                                      model.losses['x'], model.losses['y'], model.losses['w'],
                                      model.losses['h'], model.losses['conf'], model.losses['cls'],
                                      loss.item(), model.losses['recall']))
                model.seen += images.size(0)
            if best_loss is None or model.losses['conf'] < best_loss:
                best_loss = model.losses['conf']
                best_model = model
        time_elapsed = time.time() - since
        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
        return best_model, best_loss
    # Return the best model              
    model, loss = train_model(model, optimizer_ft, num_epochs=NUM_EPOCHS)
    print('Saving trained model...')
    MODEL_PATH = join(MODEL_FOLDER, "model.weights")
    model.save_weights(MODEL_PATH)
    print("Model information")
    print("MODEL_PATH: " + MODEL_PATH)
    # Save onnx trained model
    if MODEL_TYPE_ONNX:
        print('Saving trained model...')
        MODEL_ONNX_PATH = join(MODEL_FOLDER, "model.onnx")
        CHANNEL = 3
        #input - 3 channels, 416x416 image size,
        dummy_input = Variable(torch.randn(BATCH_SIZE, CHANNEL, IMG_SIZE[0], IMG_SIZE[1]))
        # Invoke export
        torch.onnx.export(model, dummy_input, MODEL_ONNX_PATH)
        print("Model information")
        print("MODEL_PATH: " + MODEL_ONNX_PATH)
    else:        
        MODEL_ONNX_PATH = None
    variables.put("MODEL_FOLDER", MODEL_FOLDER)
    variables.put("MODEL_PATH", MODEL_PATH)
    variables.put("MODEL_ONNX_PATH", MODEL_ONNX_PATH)
# ======================================================================================

############################ OUTPUT FOR AUTOML ############################
# Convert from JSON to dict
token = variables.get("TOKEN")
token = json.loads(token)

# Return the loss value
result_map = {'token': token, 'loss': loss}
print('result_map: ', result_map)
resultMap.put("RESULT_JSON", json.dumps(result_map))

# To appear in Job Analytics
resultMap.put("LOSS", str(loss))
###########################################################################

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <outputFiles>
        <files accessMode="transferToGlobalSpace" includes="$MODEL_FOLDER/**"/>
      </outputFiles>
      <metadata>
        <positionTop>
            309.21873474121094
        </positionTop>
        <positionLeft>
            240.53976440429688
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2603px;
            height:3065px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-176.2216033935547px;left:-171.54830932617188px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_827" style="top: 181.222px; left: 176.551px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="You Only Look Once (YOLO) is a  single neural network to predict bounding boxes and class probabilities.
You can see more details in: https://pjreddie.com/media/files/papers/YOLOv3.pdf
https://github.com/eriklindernoren/PyTorch-YOLOv3"><img src="/automation-dashboard/styles/patterns/img/wf-icons/deep_detection.png" width="20px">&nbsp;<span class="name">YOLO</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_830" style="top: 181.222px; left: 304.551px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Load and return an image dataset."><img src="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png" width="20px">&nbsp;<span class="name">Import_Image_Dataset</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_833" style="top: 309.222px; left: 240.551px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Train a model using an image object detection network."><img src="/automation-dashboard/styles/patterns/img/wf-icons/deep_train.png" width="20px">&nbsp;<span class="name">Train_Image_Object_Detection_Model</span></a></div><svg style="position:absolute;left:215.5px;top:220.5px" width="138.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 117.5 88 C 127.5 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M103.941410625,61.4125935 L89.66651829125225,45.752853755677634 L91.6215345706727,54.762732297988904 L83.01665708924115,58.072729810004944 L103.941410625,61.4125935" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M103.941410625,61.4125935 L89.66651829125225,45.752853755677634 L91.6215345706727,54.762732297988904 L83.01665708924115,58.072729810004944 L103.941410625,61.4125935" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:333px;top:220.5px" width="50.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 39.5 50 29.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-0.19430087500000195,65.8307285 L14.119682683751062,50.20671254123305 L5.321872471305423,52.963256657656245 L1.2522108414072983,44.69053919492763 L-0.19430087500000195,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-0.19430087500000195,65.8307285 L14.119682683751062,50.20671254123305 L5.321872471305423,52.963256657656245 L1.2522108414072983,44.69053919492763 L-0.19430087500000195,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 216px; top: 211px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 363px; top: 211px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 333.5px; top: 339px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 333.5px; top: 299px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
