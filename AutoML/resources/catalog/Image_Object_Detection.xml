<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.12" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Image_Object_Detection" onTaskError="continueJobExecution" priority="normal" projectName="2. Objective ML Examples" xsi:schemaLocation="urn:proactive:jobdescriptor:3.12 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.12/schedulerjob.xsd">
  <variables>
    <variable model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
    <variable model="PA:Boolean" name="DOCKER_GPU_ENABLED" value="True"/>
    <variable model="PA:LIST(,activeeon/dlm3,activeeon/cuda)" name="DOCKER_IMAGE" value=""/>
    <variable name="NODE_SOURCE_NAME" value=""/>
    <variable name="NODE_ACCESS_TOKEN" value=""/>
    <variable name="INPUT_VARIABLES" value="{&quot;LEARNING_RATE&quot; : 0.0025, &quot;CONF_THRESHOLD&quot; : 0.1, &quot;NMS_THRESHOLD&quot; : 0.35, &quot;MOMENTUM&quot; : 0.09, &quot;WEIGHT_DECAY&quot; : 0.005}"/>
    <variable name="SEARCH_SPACE" value="{&quot;LEARNING_RATE&quot;: choice([0.0001, 0.00025]), &quot;CONF_THRESHOLD&quot;: choice([0.1, 0.5]), &quot;NMS_THRESHOLD&quot;: choice([0.35, 0.45]), &quot;MOMENTUM&quot;: choice([0.0009, 0.009]), &quot;WEIGHT_DECAY&quot;: choice([0.0005, 0.005])}"/>
  </variables>
  <description>
    <![CDATA[ Train a YOLO model using the PyTorch library. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="auto-ml-optimization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/pytorch-logo-dark.png"/>
<info name="NODE_ACCESS_TOKEN" value="$NODE_ACCESS_TOKEN"/>
<info name="Documentation" value="MLOS/MLOSUserGuide.html#_alexnet"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="YOLO">
      <description>
        <![CDATA[ You Only Look Once (YOLO) is a  single neural network to predict bounding boxes and class probabilities.
You can see more details in: https://pjreddie.com/media/files/papers/YOLOv3.pdf
https://github.com/eriklindernoren/PyTorch-YOLOv3 ]]>
      </description>
      <variables>
        <variable inherited="false" name="LEARNING_RATE" value="0.001"/>
        <variable inherited="false" name="MOMENTUM" value="0.9"/>
        <variable inherited="false" name="WEIGHT_DECAY" value="0.0005"/>
        <variable inherited="false" name="IMG_SIZE" value="(416, 416)"/>
        <variable inherited="false" name="NUM_CLASSES" value="81"/>
        <variable inherited="false" name="CONF_THRESHOLD" value="0.5"/>
        <variable inherited="false" name="NMS_THRESHOLD" value="0.45"/>
        <variable inherited="false" name="LABEL_PATH" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/coco.names"/>
        <variable inherited="false" name="USE_PRETRAINED_MODEL" value="True"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_detection.png"/>
        <info name="task.documentation" value="MLOS/MLOSUserGuide.html#_segnet"/>
      </genericInformation>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_source_name/raw"/>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_cuda_universal/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/get_automl_params/raw"/>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import json
import numpy as np
from ast import literal_eval as make_tuple

IMG_SIZE = variables.get("IMG_SIZE") 
NUM_CLASSES = int(str(variables.get("NUM_CLASSES")))  
CONF_THRESHOLD  = float(str(variables.get("CONF_THRESHOLD"))) 
NMS_THRESHOLD = float(str(variables.get("NMS_THRESHOLD")))
LABEL_PATH  = variables.get("LABEL_PATH")
LEARNING_RATE = float(str(variables.get("LEARNING_RATE")))
MOMENTUM = float(str(variables.get("MOMENTUM")))
WEIGHT_DECAY = float(str(variables.get("WEIGHT_DECAY")))
USE_PRETRAINED_MODEL = variables.get("USE_PRETRAINED_MODEL") 
input_variables = variables.get("INPUT_VARIABLES")

######################## AUTOML SETTING ##########################  

# SEARCH_SPACE:
# {"LEARNING_RATE": choice([0.0001, 0.00025]), 
#  'CONF_THRESHOLD': choice([0.1, 0.5]), 
#  'NMS_THRESHOLD': choice([0.35,  0.45]), 
#  'MOMENTUM': choice([0.0009, 0.009]), 
#  'WEIGHT_DECAY': choice([0.0005, 0.005])}
#"""

if input_variables is not None and input_variables !="":
    input_variables = json.loads(input_variables)
    LEARNING_RATE = input_variables["LEARNING_RATE"]
    CONF_THRESHOLD = input_variables["CONF_THRESHOLD"]
    NMS_THRESHOLD = input_variables["NMS_THRESHOLD"]
    MOMENTUM = input_variables["MOMENTUM"]
    WEIGHT_DECAY = input_variables["WEIGHT_DECAY"]
##################################################################

print('-' * 30)
print('LEARNING_RATE:     ', LEARNING_RATE)
print('CONF_THRESHOLD:    ', CONF_THRESHOLD)
print('NMS_THRESHOLD:     ', NMS_THRESHOLD)
print('MOMENTUM:      	  ', MOMENTUM)
print('WEIGHT_DECAY:      ', WEIGHT_DECAY)
print('-' * 30)

IMG_SIZE = make_tuple(IMG_SIZE)
IMG_SIZE = tuple(IMG_SIZE)
NET_NAME = 'YOLO'

# Define the TRANSFORM functions
NET_TRANSFORM = """

from __future__ import division
import math
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

def load_classes(path):
    fp = open(path, "r")
    names = fp.read().split("\\n")[:-1]
    return names

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm2d') != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

def compute_ap(recall, precision):
# =============================================================================
#       Compute the average precision, given the recall and precision curves.
#    Code originally from https://github.com/rbgirshick/py-faster-rcnn.
    # Arguments
#        recall:    The recall curve (list).
#        precision: The precision curve (list).
    # Returns
#        The average precision as computed in py-faster-rcnn.
# =============================================================================

    # correct AP calculation
    # first append sentinel values at the end
    mrec = np.concatenate(([0.], recall, [1.]))
    mpre = np.concatenate(([0.], precision, [0.]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap

def bbox_iou(box1, box2, x1y1x2y2=True):
# =============================================================================
#   Returns the IoU of two bounding boxes
# =============================================================================
    if not x1y1x2y2:
        # Transform from center and width to exact coordinates
        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2
        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2
        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2
        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2
    else:
        # Get the coordinates of bounding boxes
        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]
        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]

    # get the corrdinates of the intersection rectangle
    inter_rect_x1 =  torch.max(b1_x1, b2_x1)
    inter_rect_y1 =  torch.max(b1_y1, b2_y1)
    inter_rect_x2 =  torch.min(b1_x2, b2_x2)
    inter_rect_y2 =  torch.min(b1_y2, b2_y2)
    # Intersection area
    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \
                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)
    # Union Area
    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)
    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)

    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)

    return iou


def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):
# =====================================================================================
#  Removes detections with lower object confidence score than 'conf_thres' and performs
#  Non-Maximum Suppression to further filter detections.
#  Returns detections with shape:
#   (x1, y1, x2, y2, object_conf, class_score, class_pred)
# =====================================================================================

    # From (center x, center y, width, height) to (x1, y1, x2, y2)
    box_corner = prediction.new(prediction.shape)
    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2
    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2
    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2
    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2
    prediction[:, :, :4] = box_corner[:, :, :4]

    output = [None for _ in range(len(prediction))]
    for image_i, image_pred in enumerate(prediction):
        # Filter out confidence scores below threshold
        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()
        image_pred = image_pred[conf_mask]
        # If none are remaining => process next image
        if not image_pred.size(0):
            continue
        # Get score and class with highest confidence
        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)
        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)
        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)
        # Iterate through all predicted classes
        unique_labels = detections[:, -1].cpu().unique()
        if prediction.is_cuda:
            unique_labels = unique_labels.cuda()
        for c in unique_labels:
            # Get the detections with the particular class
            detections_class = detections[detections[:, -1] == c]
            # Sort the detections by maximum objectness confidence
            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)
            detections_class = detections_class[conf_sort_index]
            # Perform non-maximum suppression
            max_detections = []
            while detections_class.size(0):
                # Get detection with highest confidence and save as max detection
                max_detections.append(detections_class[0].unsqueeze(0))
                # Stop if we're at the last detection
                if len(detections_class) == 1:
                    break
                # Get the IOUs for all boxes with lower confidence
                ious = bbox_iou(max_detections[-1], detections_class[1:])
                # Remove detections with IoU >= NMS threshold
                detections_class = detections_class[1:][ious < nms_thres]

            max_detections = torch.cat(max_detections).data
            # Add max detections to outputs
            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))

    return output

def build_targets(pred_boxes, target, anchors, num_anchors, num_classes, dim, ignore_thres, img_dim):
    nB = target.size(0)
    nA = num_anchors
    nC = num_classes
    dim = dim
    mask        = torch.zeros(nB, nA, dim, dim)
    tx         = torch.zeros(nB, nA, dim, dim)
    ty         = torch.zeros(nB, nA, dim, dim)
    tw         = torch.zeros(nB, nA, dim, dim)
    th         = torch.zeros(nB, nA, dim, dim)
    tconf      = torch.zeros(nB, nA, dim, dim)
    tcls       = torch.zeros(nB, nA, dim, dim, num_classes)

    nGT = 0
    nCorrect = 0
    for b in range(nB):
        for t in range(target.shape[1]):
            if target[b, t].sum() == 0:
                continue
            nGT += 1
            # Convert to position relative to box
            gx = target[b, t, 1] * dim
            gy = target[b, t, 2] * dim
            gw = target[b, t, 3] * dim
            gh = target[b, t, 4] * dim
            # Get grid box indices
            gi = int(gx)
            gj = int(gy)
            # Get shape of gt box
            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)
            # Get shape of anchor box
            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))
            # Calculate iou between gt and anchor shape
            anch_ious = bbox_iou(gt_box, anchor_shapes)
            # Find the best matching anchor box
            best_n = np.argmax(anch_ious)
            best_iou = anch_ious[best_n]
            # Get the ground truth box and corresponding best prediction
            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)
            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)

            # Masks
            mask[b, best_n, gj, gi] = 1
            # Coordinates
            tx[b, best_n, gj, gi] = gx - gi
            ty[b, best_n, gj, gi] = gy - gj
            # Width and height
            tw[b, best_n, gj, gi] = math.log(gw/anchors[best_n][0] + 1e-16)
            th[b, best_n, gj, gi] = math.log(gh/anchors[best_n][1] + 1e-16)
            # One-hot encoding of label
            tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1
            # Calculate iou between ground truth and best matching prediction
            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)
            tconf[b, best_n, gj, gi] = 1

            if iou > 0.5:
                nCorrect += 1

    return nGT, nCorrect, mask, tx, ty, tw, th, tconf, tcls
    
def to_categorical(y, NUM_CLASSES):
# =============================================================================
#    1-hot encodes a tensor
# =============================================================================
    return torch.from_numpy(np.eye(NUM_CLASSES, dtype='uint8')[y])

"""
#print(NET_TRANSFORM)

# Define the NET model
NET_MODEL = """

from __future__ import division

import torch
import numpy as np
from PIL import Image
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from collections import defaultdict

def parse_model_config(path):
# =============================================================================
#   Parses the yolo-v3 layer configuration file and returns module definitions
# =============================================================================
    file = open(path, 'r')
    lines = file.read().split('\\n')
    lines = [x for x in lines if x and not x.startswith('#')]
    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces
    module_defs = []
    for line in lines:
        if line.startswith('['): # This marks the start of a new block
            module_defs.append({})
            module_defs[-1]['type'] = line[1:-1].rstrip()
            if module_defs[-1]['type'] == 'convolutional':
                module_defs[-1]['batch_normalize'] = 0
        else:
            key, value = line.split("=")
            value = value.strip()
            module_defs[-1][key.rstrip()] = value.strip()

    return module_defs

def parse_data_config(path):
# =============================================================================
#   Parses the data configuration file
# =============================================================================
    options = dict()
    options['gpus'] = '0,1,2,3'
    options['num_workers'] = '10'
    with open(path, 'r') as fp:
        lines = fp.readlines()
    for line in lines:
        line = line.strip()
        if line == '' or line.startswith('#'):
            continue
        key, value = line.split('=')
        options[key.strip()] = value.strip()
    return options


def create_modules(module_defs):
# =====================================================================================
#   Constructs module list of layer blocks from module configuration in module_defs
# =====================================================================================
    hyperparams = module_defs.pop(0)
    output_filters = [int(hyperparams['channels'])]
    module_list = nn.ModuleList()
    for i, module_def in enumerate(module_defs):
        modules = nn.Sequential()

        if module_def['type'] == 'convolutional':
            bn = int(module_def['batch_normalize'])
            filters = int(module_def['filters'])
            kernel_size = int(module_def['size'])
            pad = (kernel_size - 1) // 2 if int(module_def['pad']) else 0
            modules.add_module('conv_%d' % i, nn.Conv2d(in_channels=output_filters[-1],
                                                        out_channels=filters,
                                                        kernel_size=kernel_size,
                                                        stride=int(module_def['stride']),
                                                        padding=pad,
                                                        bias=not bn))
            if bn:
                modules.add_module('batch_norm_%d' % i, nn.BatchNorm2d(filters))
            if module_def['activation'] == 'leaky':
                modules.add_module('leaky_%d' % i, nn.LeakyReLU(0.1))

        elif module_def['type'] == 'upsample':
            upsample = nn.Upsample( scale_factor=int(module_def['stride']),
                                    mode='nearest')
            modules.add_module('upsample_%d' % i, upsample)

        elif module_def['type'] == 'route':
            layers = [int(x) for x in module_def["layers"].split(',')]
            filters = sum([output_filters[layer_i] for layer_i in layers])
            modules.add_module('route_%d' % i, EmptyLayer())

        elif module_def['type'] == 'shortcut':
            filters = output_filters[int(module_def['from'])]
            modules.add_module("shortcut_%d" % i, EmptyLayer())

        elif module_def["type"] == "yolo":
            anchor_idxs = [int(x) for x in module_def["mask"].split(",")]
            # Extract anchors
            anchors = [int(x) for x in module_def["anchors"].split(",")]
            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]
            anchors = [anchors[i] for i in anchor_idxs]
            num_classes = int(module_def['classes'])
            img_height = int(hyperparams['height'])
            # Define detection layer
            yolo_layer = YOLOLayer(anchors, num_classes, img_height)
            modules.add_module('yolo_%d' % i, yolo_layer)
        # Register module list and number of output filters
        module_list.append(modules)
        output_filters.append(filters)

    return hyperparams, module_list

class EmptyLayer(nn.Module):
# =====================================================================================
#   Placeholder for 'route' and 'shortcut' layers
# =====================================================================================
    def __init__(self):
        super(EmptyLayer, self).__init__()

class YOLOLayer(nn.Module):
# =====================================================================================
#   Detection layer
# =====================================================================================
    def __init__(self, anchors, num_classes, img_dim):
        super(YOLOLayer, self).__init__()
        self.anchors = anchors
        self.num_anchors = len(anchors)
        self.num_classes = num_classes
        self.bbox_attrs = 5 + num_classes
        self.img_dim = img_dim
        self.ignore_thres = 0.5
        self.lambda_coord = 5
        self.lambda_noobj = 0.5

        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCELoss()

    def forward(self, x, targets=None):
        bs = x.size(0)
        g_dim = x.size(2)
        stride =  self.img_dim / g_dim
        # Tensors for cuda support
        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor

        prediction = x.view(bs,  self.num_anchors, self.bbox_attrs, g_dim, g_dim).permute(0, 1, 3, 4, 2).contiguous()

        # Get outputs
        x = torch.sigmoid(prediction[..., 0])          # Center x
        y = torch.sigmoid(prediction[..., 1])          # Center y
        w = prediction[..., 2]                         # Width
        h = prediction[..., 3]                         # Height
        conf = torch.sigmoid(prediction[..., 4])       # Conf
        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.

        # Calculate offsets for each grid
        grid_x = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).repeat(bs*self.num_anchors, 1, 1).view(x.shape).type(FloatTensor)
        grid_y = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).t().repeat(bs*self.num_anchors, 1, 1).view(y.shape).type(FloatTensor)
        scaled_anchors = [(a_w / stride, a_h / stride) for a_w, a_h in self.anchors]
        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(w.shape)
        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(h.shape)

        # Add offset and scale with anchors
        pred_boxes = FloatTensor(prediction[..., :4].shape)
        pred_boxes[..., 0] = x.data + grid_x
        pred_boxes[..., 1] = y.data + grid_y
        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w
        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h

        # Training
        if targets is not None:

            if x.is_cuda:
                self.mse_loss = self.mse_loss.cuda()
                self.bce_loss = self.bce_loss.cuda()

            nGT, nCorrect, mask, tx, ty, tw, th, tconf, tcls = build_targets(pred_boxes.cpu().data,
                                                                            targets.cpu().data,
                                                                            scaled_anchors,
                                                                            self.num_anchors,
                                                                            self.num_classes,
                                                                            g_dim,
                                                                            self.ignore_thres,
                                                                            self.img_dim)

            nProposals = int((conf > 0.25).sum().item())
            recall = float(nCorrect / nGT) if nGT else 1

            mask = Variable(mask.type(FloatTensor))

            tx    = Variable(tx.type(FloatTensor), requires_grad=False)
            ty    = Variable(ty.type(FloatTensor), requires_grad=False)
            tw    = Variable(tw.type(FloatTensor), requires_grad=False)
            th    = Variable(th.type(FloatTensor), requires_grad=False)
            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)
            tcls  = Variable(tcls.type(FloatTensor), requires_grad=False)

            # Mask outputs to ignore non-existing objects (but keep confidence predictions)
            loss_x = self.lambda_coord * self.bce_loss(x * mask, tx * mask) / 2
            loss_y = self.lambda_coord * self.bce_loss(y * mask, ty * mask) / 2
            loss_w = self.lambda_coord * self.mse_loss(w * mask, tw * mask) / 2
            loss_h = self.lambda_coord * self.mse_loss(h * mask, th * mask) / 2
            loss_conf = self.bce_loss(conf * mask, mask) + \
                        self.lambda_noobj * self.bce_loss(conf * (1 - mask), mask * (1 - mask))
            loss_cls = self.bce_loss(pred_cls[mask == 1], tcls[mask == 1])
            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls

            return loss, loss_x.item(), loss_y.item(), loss_w.item(), loss_h.item(), loss_conf.item(), loss_cls.item(), recall

        else:
            # If not in training phase return predictions
            output = torch.cat((pred_boxes.view(bs, -1, 4) * stride, conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1)
            return output.data


class Darknet(nn.Module):
# =====================================================================================
#   YOLOv3 object detection model
# =====================================================================================
    def __init__(self, config_path, img_size=416):
        super(Darknet, self).__init__()
        self.module_defs = parse_model_config(config_path)
        self.hyperparams, self.module_list = create_modules(self.module_defs)
        self.img_size = img_size
        self.seen = 0
        self.header_info = np.array([0, 0, 0, self.seen, 0])
        self.loss_names = ['x', 'y', 'w', 'h', 'conf', 'cls', 'recall']

    def forward(self, x, targets=None):
        is_training = targets is not None
        output = []
        self.losses = defaultdict(float)
        layer_outputs = []
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if module_def['type'] in ['convolutional', 'upsample']:
                x = module(x)
            elif module_def['type'] == 'route':
                layer_i = [int(x) for x in module_def['layers'].split(',')]
                x = torch.cat([layer_outputs[i] for i in layer_i], 1)
            elif module_def['type'] == 'shortcut':
                layer_i = int(module_def['from'])
                x = layer_outputs[-1] + layer_outputs[layer_i]
            elif module_def['type'] == 'yolo':
                # Train phase: get loss
                if is_training:
                    x, *losses = module[0](x, targets)
                    for name, loss in zip(self.loss_names, losses):
                        self.losses[name] += loss
                # Test phase: Get detections
                else:
                    x = module(x)
                output.append(x)
            layer_outputs.append(x)

        self.losses['recall'] /= 3
        return sum(output) if is_training else torch.cat(output, 1)


    def load_weights(self, weights_path):
# =====================================================================================
#  Parses and loads the weights stored in 'weights_path
# =====================================================================================

        #Open the weights file
        fp = open(weights_path, "rb")
        header = np.fromfile(fp, dtype=np.int32, count=5)   # First five are header values

        # Needed to write header when saving weights
        self.header_info = header

        self.seen = header[3]
        weights = np.fromfile(fp, dtype=np.float32)         # The rest are weights
        fp.close()

        ptr = 0
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if module_def['type'] == 'convolutional':
                conv_layer = module[0]
                if module_def['batch_normalize']:
                    # Load BN bias, weights, running mean and running variance
                    bn_layer = module[1]
                    num_b = bn_layer.bias.numel() # Number of biases
                    # Bias
                    bn_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.bias)
                    bn_layer.bias.data.copy_(bn_b)
                    ptr += num_b
                    # Weight
                    bn_w = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.weight)
                    bn_layer.weight.data.copy_(bn_w)
                    ptr += num_b
                    # Running Mean
                    bn_rm = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_mean)
                    bn_layer.running_mean.data.copy_(bn_rm)
                    ptr += num_b
                    # Running Var
                    bn_rv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_var)
                    bn_layer.running_var.data.copy_(bn_rv)
                    ptr += num_b
                else:
                    # Load conv. bias
                    num_b = conv_layer.bias.numel()
                    conv_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(conv_layer.bias)
                    conv_layer.bias.data.copy_(conv_b)
                    ptr += num_b
                # Load conv. weights
                num_w = conv_layer.weight.numel()
                conv_w = torch.from_numpy(weights[ptr:ptr + num_w]).view_as(conv_layer.weight)
                conv_layer.weight.data.copy_(conv_w)
                ptr += num_w

# =====================================================================================
#  @:param path    - path of the new weights file
#  @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)
# =====================================================================================
    def save_weights(self, path, cutoff=-1):

        fp = open(path, 'wb')
        self.header_info[3] = self.seen
        self.header_info.tofile(fp)

        # Iterate through layers
        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):
            if module_def['type'] == 'convolutional':
                conv_layer = module[0]
                # If batch norm, load bn first
                if module_def['batch_normalize']:
                    bn_layer = module[1]
                    bn_layer.bias.data.cpu().numpy().tofile(fp)
                    bn_layer.weight.data.cpu().numpy().tofile(fp)
                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)
                    bn_layer.running_var.data.cpu().numpy().tofile(fp)
                # Load conv bias
                else:
                    conv_layer.bias.data.cpu().numpy().tofile(fp)
                # Load conv weights
                conv_layer.weight.data.cpu().numpy().tofile(fp)

        fp.close()
"""
#print(NET_TRANSFORM)


if 'variables' in locals():
  variables.put("NET_MODEL", NET_MODEL)
  variables.put("NET_TRANSFORM", NET_TRANSFORM)    
  variables.put("IMG_SIZE", IMG_SIZE)
  variables.put("NUM_CLASSES", NUM_CLASSES)
  variables.put("LEARNING_RATE",  LEARNING_RATE) 
  variables.put("MOMENTUM",  MOMENTUM) 
  variables.put("WEIGHT_DECAY", WEIGHT_DECAY)
  variables.put("CONF_THRESHOLD", CONF_THRESHOLD)
  variables.put("NMS_THRESHOLD", NMS_THRESHOLD)
  variables.put("LABEL_PATH", LABEL_PATH)
  variables.put("USE_PRETRAINED_MODEL", USE_PRETRAINED_MODEL)
  variables.put("NET_NAME", NET_NAME)

print("END YOLO")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            100.48828125
        </positionTop>
        <positionLeft>
            301.71875
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Import_Image_Dataset">
      <description>
        <![CDATA[ Load and return an image dataset. ]]>
      </description>
      <variables>
        <variable inherited="false" name="DATA_PATH" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/coco.zip"/>
        <variable inherited="false" name="TRAIN_SPLIT" value="0.60"/>
        <variable inherited="false" name="VAL_SPLIT" value="0.15"/>
        <variable inherited="false" name="TEST_SPLIT" value="0.25"/>
        <variable inherited="false" model="PA:LIST(Classification, Detection, Segmentation)" name="DATASET_TYPE" value="Detection"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png"/>
        <info name="task.documentation" value="MLOS/MLOSUserGuide.html#_import_image_dataset"/>
      </genericInformation>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_source_name/raw"/>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_cuda_universal/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <file language="cpython" url="${PA_CATALOG_REST_URL}/buckets/deep-learning-scripts/resources/Import_Image_Dataset/raw"/>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <outputFiles>
        <files accessMode="transferToGlobalSpace" includes="$DATASET_PATH/**"/>
      </outputFiles>
      <metadata>
        <positionTop>
            100.48828125
        </positionTop>
        <positionLeft>
            429.7265625
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Train_Image_Object_Detection_Model">
      <description>
        <![CDATA[ Train a model using an image object detection network. ]]>
      </description>
      <variables>
        <variable inherited="false" name="NUM_EPOCHS" value="1"/>
        <variable inherited="false" name="BATCH_SIZE" value="1"/>
        <variable inherited="false" name="NUM_WORKERS" value="1"/>
        <variable inherited="true" name="TOKEN" value="{&quot;_token_id&quot;: 0}"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_train.png"/>
        <info name="task.documentation" value="MLOS/MLOSUserGuide.html#_train_image_segmentation_model"/>
      </genericInformation>
      <depends>
        <task ref="YOLO"/>
        <task ref="Import_Image_Dataset"/>
      </depends>
      <inputFiles>
        <files accessMode="transferFromGlobalSpace" includes="$DATASET_PATH/**"/>
        <files accessMode="transferFromGlobalSpace" includes="$LABEL_PATH/**"/>
      </inputFiles>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_source_name/raw"/>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_cuda_universal/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/get_automl_token/raw"/>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Train_Image_Object_Detection_Model")

import os
import json
import torch
import cv2
import sys
import time
import wget
import copy
import uuid
import types
import torch
import pprint
import zipfile
import numpy as np
import os.path as osp
from numpy import random

import torch.nn as nn
from PIL import Image
import torch.optim as optim

import torch.nn.init as init
import torch.nn.functional as F
import torch.utils.data as data

from torch.optim import SGD, Adam
from torch.autograd import Function
from torch.autograd import Variable
import torch.backends.cudnn as cudnn

from math import sqrt as sqrt
from skimage.transform import resize
from itertools import product as product
from os import remove, listdir, makedirs
from ast import literal_eval as make_tuple
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from os.path import basename, splitext, exists, join
from lxml.etree import Element, SubElement, tostring

from xml.dom.minidom import parseString
if sys.version_info[0] == 2:
    import xml.etree.cElementTree as ET
else:
    import xml.etree.ElementTree as ET
    
VISDOM_ENABLED = variables.get("ENDPOINT_VISDOM")

if VISDOM_ENABLED is not None:
  from visdom import Visdom


if 'variables' in locals():
  NUM_EPOCHS = int(str(variables.get("NUM_EPOCHS")))    
  NUM_CLASSES = int(str(variables.get("NUM_CLASSES")))
  NET_MODEL     = variables.get("NET_MODEL")
  NET_TRANSFORM = variables.get("NET_TRANSFORM")
  NET_CRITERION = variables.get("NET_CRITERION")
  DATASET_PATH  = variables.get("DATASET_PATH")
  LABEL_PATH  = variables.get("LABEL_PATH")
  IMG_SIZE = variables.get("IMG_SIZE")
  LEARNING_RATE = float(str(variables.get("LEARNING_RATE")))
  MOMENTUM = float(str(variables.get("MOMENTUM")))
  WEIGHT_DECAY = float(str(variables.get("WEIGHT_DECAY")))
  BATCH_SIZE = int(str(variables.get("BATCH_SIZE")))
  NUM_WORKERS = int(str(variables.get("NUM_WORKERS")))
  NET_NAME = variables.get("NET_NAME")
  USE_PRETRAINED_MODEL = variables.get("USE_PRETRAINED_MODEL")

assert DATASET_PATH is not None
assert NET_MODEL is not None
assert NET_TRANSFORM is not None

IMG_SIZE = tuple(IMG_SIZE)

# Save onnx model
MODEL_TYPE_ONNX = False

# Download class file 
print("Downloading...")
filename = wget.download(LABEL_PATH)
print("FILENAME: " + filename)
print("OK")

# Class names
CLASSES  = tuple(open(filename).read().splitlines())

DATASET_TRAIN_PATH = join(DATASET_PATH, 'train')
DIR_EXT = join(DATASET_TRAIN_PATH, 'classes')

## Get extension class folder
files = os.listdir(DIR_EXT);
checkds_store = '.DS_Store' in files
if checkds_store == True:
    files.remove('.DS_Store')
file_name = files[0]
base_file, ext = os.path.splitext(file_name)

# Check if gpu is available
use_gpu = torch.cuda.is_available()

# Get an unique ID
ID = str(uuid.uuid4())

# Create an empty dir
MODEL_FOLDER = join('models', ID)
os.makedirs(MODEL_FOLDER, exist_ok=True)
print("MODEL_FOLDER: " + MODEL_FOLDER)


# ======================================================================================

##################################  BEGIN SSD NET ###################################### 

# ======================================================================================

def weights_init(m):
    if isinstance(m, nn.Conv2d):
       # xavier(m.weight.data)
        nn.init.xavier_uniform(m.weight.data)
        m.bias.data.zero_()

def adjust_learning_rate(optimizer, GAMMA, step):
# ======================================================================================
#    Sets the learning rate to the initial LR decayed by 10 at every
#        specified step
#    # Adapted from PyTorch Imagenet example:
#    # https://github.com/pytorch/examples/blob/master/imagenet/main.py
# ======================================================================================
    LEARNING_RATE = LR_FACTOR * (GAMMA ** (step))
    for param_group in optimizer.param_groups:
        param_group['LEARNING_RATE'] = LR

        
# ======================================================================================   
# Begin Load Dataset COCO and PASCAL VOC formats
# ======================================================================================  
class VOCAnnotationTransform(object):
# =============================================================================
#    Transforms a VOC annotation into a Tensor of bbox coords and label index
#    Initilized with a dictionary lookup of classnames to indexes
#
#    Arguments:
#        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes
#            (default: alphabetic indexing of VOC's 20 classes)
#        keep_difficult (bool, optional): keep difficult instances or not
#            (default: False)
#        height (int): height
#        width (int): width
# =============================================================================

    def __init__(self, class_to_ind=None, keep_difficult=False):
        self.class_to_ind = class_to_ind or dict(
            zip(CLASSES, range(len(CLASSES))))
        self.keep_difficult = keep_difficult

    def __call__(self, target, width, height):
# ======================================================================================
#        Arguments:
#            target (annotation) : the target annotation to be made usable
#                will be an ET.Element
#        Returns:
#            a list containing lists of bounding boxes  [bbox coords, class name]
# ======================================================================================
        res = []
        for obj in target.iter('object'):
            difficult = int(obj.find('difficult').text) == 1
            if not self.keep_difficult and difficult:
                continue
            name = obj.find('name').text.lower().strip()
            bbox = obj.find('bndbox')

            pts = ['xmin', 'ymin', 'xmax', 'ymax']
            bndbox = []
            for i, pt in enumerate(pts):
                cur_pt = int(bbox.find(pt).text) - 1
                # scale height or width
                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height
                bndbox.append(cur_pt)
            label_idx = self.class_to_ind[name]
            bndbox.append(label_idx)
            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]
            # img_id = target.find('filename').text[:-4]

        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]

 
class LoadDetection(data.Dataset):
# ======================================================================================
#    PASCAL VOC 2007 format  - Detection Dataset Object
#
#    input is image, target is annotation
#
#    Arguments:
#        root (string): filepath to VOCdevkit folder.
#        image_set (string): imageset to use (eg. 'train', 'val', 'test')
#        transform (callable, optional): transformation to perform on the
#            input image
#        target_transform (callable, optional): transformation to perform on the
#            target `annotation`
#            (eg: take in caption string, return tensor of word indices)
#        dataset_name (string, optional): which dataset to load   
# ======================================================================================
    
    def __init__(self, root,
                 transform=None):    

        labels_root = join(root, 'classes')

        target_transform=VOCAnnotationTransform()
        self.transform = transform
        self.target_transform = target_transform        
        self._annopath = osp.join(root, 'classes', '%s.xml')
        self._imgpath = osp.join(root, 'images', '%s.jpg')
        self.ids = list()
        l=os.listdir(labels_root)
        self.ids=[x.split('.')[0] for x in l]

    def __getitem__(self, index):
        im, gt, h, w = self.pull_item(index) 
        
        return im, gt
    
    def __len__(self):
        return len(self.ids)
        
    def pull_item(self, index):

        img_id = self.ids[index] 
        target = ET.parse(self._annopath % img_id).getroot() 

        img = cv2.imread(self._imgpath % img_id)
        height, width, channels = img.shape 
      
        if self.target_transform is not None:
            target = self.target_transform(target, width, height)

        if self.transform is not None:
            target = np.array(target)
            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])
            # to rgb
            img = img[:, :, (2, 1, 0)]
            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))
        return torch.from_numpy(img).permute(2, 0, 1), target, height, width

# ====================================================================================== 
# End Load Dataset format COCO and PASCAL VOC formats
# ======================================================================================    
             
# ======================================================================================
## COCO format to PASCAL VOC format
# ======================================================================================
def unconvert(class_id, width, height, x, y, w, h):

    xmax = int((x*width) + (w * width)/2.0)
    xmin = int((x*width) - (w * width)/2.0)
    ymax = int((y*height) + (h * height)/2.0)
    ymin = int((y*height) - (h * height)/2.0)
    class_id = int(class_id)
    return (class_id, xmin, xmax, ymin, ymax)

def xml_transform(root, classes):
    annopath = join(root, 'classes', '%s.txt')
    _imgpath = join(root, 'images', '%s.jpg')
    new_annopath = join(root, 'classes', '%s.xml')

    mypath  = join(root, 'classes')
    ids = list()
    l=os.listdir(mypath)
    ids=[x.split('.')[0] for x in l]

    for i in range(len(ids)):
        img_id = ids[i] 
        img= cv2.imread(_imgpath % img_id)
        height, width, channels = img.shape 

        node_root = Element('annotation')
        node_folder = SubElement(node_root, 'folder')
        node_folder.text = 'VOC2007'
        img_name = img_id + '.jpg'
    
        node_filename = SubElement(node_root, 'filename')
        node_filename.text = img_name
        
        node_source= SubElement(node_root, 'source')
        node_database = SubElement(node_source, 'database')
        node_database.text = 'Coco database'
        
        node_size = SubElement(node_root, 'size')
        node_width = SubElement(node_size, 'width')
        node_width.text = str(width)
    
        node_height = SubElement(node_size, 'height')
        node_height.text = str(height)

        node_depth = SubElement(node_size, 'depth')
        node_depth.text = str(channels)
    
        node_segmented = SubElement(node_root, 'segmented')
        node_segmented.text = '0'

        target = (annopath % img_id)
        if os.path.exists(target):
            label_norm= np.loadtxt(target).reshape(-1, 5)

            for i in range(len(label_norm)):
                labels_conv = label_norm[i]
                new_label = unconvert(labels_conv[0], width, height, labels_conv[1], labels_conv[2], labels_conv[3], labels_conv[4])
                node_object = SubElement(node_root, 'object')
                node_name = SubElement(node_object, 'name')
                node_name.text = CLASSES[new_label[0]]
                node_pose = SubElement(node_object, 'pose')
                node_pose.text = 'Unspecified'
                

                node_truncated = SubElement(node_object, 'truncated')
                node_truncated.text = '0'  
                node_difficult = SubElement(node_object, 'difficult')
                node_difficult.text = '0'
                node_bndbox = SubElement(node_object, 'bndbox')
                node_xmin = SubElement(node_bndbox, 'xmin')
                node_xmin.text = str(new_label[1])
                node_ymin = SubElement(node_bndbox, 'ymin')
                node_ymin.text = str(new_label[3])
                node_xmax = SubElement(node_bndbox, 'xmax')
                node_xmax.text =  str(new_label[2])
                node_ymax = SubElement(node_bndbox, 'ymax')
                node_ymax.text = str(new_label[4])
                xml = tostring(node_root, pretty_print=True)  
     
        f =  open(new_annopath % img_id, "wb")
        os.remove(target)
        f.write(xml)
        f.close()     
        
# ======================================================================================   
# END COCO format to PASCAL VOC format
# ======================================================================================  

if (NET_NAME == 'SSD'):
    
    LR_STEPS = variables.get("LR_STEPS")
    LR_FACTOR = variables.get("LR_FACTOR")    
    START_ITERATION  = int(str(variables.get("START_ITERATION")))
    MAX_ITERATION = int(str(variables.get("MAX_ITERATION")))
    MIN_SIZES = variables.get("MIN_SIZES")
    MAX_SIZES = variables.get("MAX_SIZES")
    MEANS = (104, 117, 123)
    
    BUILD_TYPE = 'train'
    LR_STEPS = tuple(LR_STEPS)
    
    MIN_SIZES  = make_tuple(MIN_SIZES)
    MIN_SIZES  = tuple(MIN_SIZES)
    MIN_SIZES  = list(MIN_SIZES)

    MAX_SIZES  = make_tuple(MAX_SIZES)
    MAX_SIZES  = tuple(MAX_SIZES)
    MAX_SIZES  = list(MAX_SIZES)

    DIR_EXT = join(DATASET_TRAIN_PATH, 'classes')
    files = os.listdir(DIR_EXT)
    
    checkds_store = '.DS_Store' in files
    if checkds_store == True:
        files.remove('.DS_Store')
    filename = files[0]
    base_file, ext = os.path.splitext(filename)
    
    if ext  == '.txt': # input coco format 
        xml_transform(DATASET_TRAIN_PATH, CLASSES)
        
    # Load NET model
    exec(NET_MODEL)
    
    MODEL_NAME = 'SSD'
    ssd_net = build_ssd(BUILD_TYPE, IMG_SIZE[0], NUM_CLASSES)
    Net = ssd_net
    assert Net is not None, f'model {MODEL_NAME} not available'
    model = Net

    # Load criterion NET 
    exec(NET_CRITERION)

    # Load transform NET
    exec(NET_TRANSFORM)

    if use_gpu:
        torch.cuda.FloatTensor
    else:
        print("WARNING: It looks like you have a CUDA device, but aren't " +  "using CUDA.\nRun with --cuda for optimal training speed.")
        torch.FloatTensor
          
    if use_gpu:
        #model = ssd_net

        #model = torch.nn.DataParallel(ssd_net)
        model = ssd_net
        cudnn.benchmark = True         
        

    ##  Download VGG model
    print("Downloading...")
    VGG_MODEL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/vgg16_reducedfc.pth'
    filename = wget.download(VGG_MODEL)
    print("VGG MODEL: " + filename)
    print("OK")

    resume = None 
    model_config_path = os.path.realpath(filename)

    if USE_PRETRAINED_MODEL == False:
        print('Resuming training, loading {}...'.format(resume))
        ssd_net.load_weights(None)
    else:
        vgg_weights = torch.load(model_config_path)
        print('Loading base network...')
        ssd_net.vgg.load_state_dict(vgg_weights)

    if use_gpu:
        model = model.cuda()  
                  
        
    if not resume:
        print('Initializing weights...')
        # initialize newly added layers' weights with xavier method        
        ssd_net.extras.apply(weights_init)
        ssd_net.loc.apply(weights_init)
        ssd_net.conf.apply(weights_init) 
        
        
    optimizer_ft = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY) 

    criterion_ft = MultiBoxLoss(NUM_CLASSES, 0.5, True, 0, True, 3, 0.5, False, use_gpu)  

    model.train()

    print('Loading the dataset...')
    dataset = LoadDetection(root=DATASET_TRAIN_PATH, transform=SSDAugmentation(IMG_SIZE[0], MEANS))
    #epoch_size = len(dataset) // BATCH_SIZE 
    epoch_size = 1

    data_loader = data.DataLoader(dataset, BATCH_SIZE, num_workers=NUM_WORKERS, 
                                  shuffle=True, collate_fn=detection_collate, 
                                  pin_memory=True)
           

###############################IF VISDOM IS ENABLED###############################
    if VISDOM_ENABLED is not None:
        visdom_endpoint = VISDOM_ENABLED.replace("http://", "")

        (VISDOM_HOST, VISDOM_PORT) = visdom_endpoint.split(":")  

        print("Connecting to %s" % VISDOM_PORT)
        viz = Visdom(server="http://"+VISDOM_HOST, port=VISDOM_PORT)
        assert viz.check_connection()


        win_global_loss_train = viz.line(Y = np.array([1]), X = np.array([1]), 
                                         opts = dict(
                                             xlabel = 'Epoch',
                                             ylabel = 'Loss',
                                             title = 'Training loss (per epoch)',
                                             ),
                                         )

        win_train = viz.text("Training:\n")  
      
##################################################################################  

    def train_model(model, optimizer, criterion, start_iter, max_iter, data_loader, use_gpu):
        # loss counters
        loc_loss = 0
        conf_loss = 0
        epoch = 0  
        step_index = 0  
    
        since = time.time() 
        batch_iterator = None
    
        for iteration in range(START_ITERATION, MAX_ITERATION):
            if (not batch_iterator) or (iteration % epoch_size == 0):
                batch_iterator = iter(data_loader)
        
                # reset epoch loss counters            
                loc_loss = 0
                conf_loss = 0
                epoch += 1    
          
            if iteration in LR_STEPS: 
                step_index += 1
                adjust_learning_rate(optimizer, GAMMA, step_index) 
                 
            # load train data      
            images, targets = next(batch_iterator)
        
            if use_gpu:
                images = Variable(images).cuda()
                targets = [Variable(ann).cuda() for ann in targets]                
            else:
                images = Variable(images)
                targets = [Variable(ann) for ann in targets]
            
            # forward 
            outputs = model(images)
        
            # backprop
            optimizer.zero_grad()
            loss_l, loss_c = criterion(outputs, targets)
            loss = loss_l + loss_c
            print(loss)
        
            #break
            loss.backward()
            optimizer.step()
        
            t1 = time.time()
            loc_loss += loss_l.item()
            conf_loss += loss_c.item()
                                   
	  ##################################################IF VISDOM IS ENABLED###########################################
            if VISDOM_ENABLED is not None:
                viz.text('-' * 10, win=win_train, append=True) 
                viz.text('Epoch {}/{}'.format(epoch, epoch), win=win_train, append=True)     
                viz.text('Loss: {:.4f}'.format(loss.item()), win=win_train, append=True)         
          
                # plot loss and accuracy per epoch
                if epoch == 1:
                	viz.line(Y = np.array([loss.item()]), X = np.array([epoch]), win = win_global_loss_train, update='replace')
                elif epoch != 1:
                	viz.line(Y = np.array([loss.item()]), X = np.array([epoch]), win = win_global_loss_train, update='append')
               
	  ####################################################################################################################                    
                
            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.item()), end=' ')
            time_elapsed = time.time() - since
       
        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
        return model
    
    # Return the model 
    model = train_model(model, optimizer_ft, criterion_ft, START_ITERATION, MAX_ITERATION, data_loader, use_gpu)

    # Save pytorch trained model
    print('Saving trained model...')
    MODEL_PATH = join(MODEL_FOLDER, "model.pt")
    torch.save(model.state_dict(), MODEL_PATH)
    print("Model information: ")
    print("MODEL_PATH:  " + MODEL_PATH)
        
    # Save onnx trained model
    if MODEL_TYPE_ONNX:
        MODEL_ONNX_PATH = None
        print('The SSD network does not yet support the ONNX format!')
    else: 
        MODEL_ONNX_PATH = None 
    
    if 'variables' in locals():
        variables.put("MODEL_FOLDER", MODEL_FOLDER)
        variables.put("MODEL_PATH", MODEL_PATH)
        variables.put("MODEL_ONNX_PATH", MODEL_ONNX_PATH)
        variables.put("MEANS", MEANS)
 
    print("END Train_Image_Object_Detection_Model")
# ======================================================================================

###################################  END SSD NET ####################################### 

# ======================================================================================




# ======================================================================================

##################################  BEGIN YOLO NET #####################################

# ======================================================================================

# ======================================================================================
## PASCAL VOC format to COOC format
# ======================================================================================
def convert(size, box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = (box[0] + box[1])/2.0
    y = (box[2] + box[3])/2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)


def convert_annotation(input_file, output_file, labels_root):
    in_file = open(input_file)
    out_file = open(labels_root +'/' + output_file + '.txt', 'w')
    tree=ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)

    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in CLASSES or int(difficult) == 1:
            continue
        cls_id = CLASSES.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))
        bb = convert((w,h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')

# ====================================================================================== 
# END PASCAL VOC format to COCO format
# ======================================================================================  

# ====================================================================================== 
# BEGIN READ PASCAL VOC 2012
# ======================================================================================  

class ListDataset(Dataset):
    def __init__(self, list_path, img_size=416):
 
        images_root = join(list_path, 'images')
        labels_root = join(list_path, 'classes')

        self.img_files = [os.path.join(r,file) for r,d,f in os.walk(images_root) for file in f]
        self.label_files = [os.path.join(r,file) for r,d,f in os.walk(labels_root) for file in f]
        self.img_shape = (img_size, img_size)
        self.max_objects = 50
        

    def __getitem__(self, index):

        #---------
        #  Image
        #---------

        img_path = self.img_files[index % len(self.img_files)].rstrip()
        img = np.array(Image.open(img_path))

        # Black and white images
        if len(img.shape) == 2:
            img = np.repeat(img[:, :, np.newaxis], 3, axis=2)

        h, w, _ = img.shape
        dim_diff = np.abs(h - w)
        # Upper (left) and lower (right) padding
        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2
        # Determine padding
        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))
        # Add padding
        input_img = np.pad(img, pad, 'constant', constant_values=128) / 255.
        padded_h, padded_w, _ = input_img.shape
        # Resize and normalize
        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')
        # Channels-first
        input_img = np.transpose(input_img, (2, 0, 1))
        # As pytorch tensor
        input_img = torch.from_numpy(input_img).float()

        #---------
        #  Label
        #---------

        
        label_path = self.label_files[index % len(self.img_files)].rstrip()

        labels = None

        if os.path.exists(label_path):

            labels = np.loadtxt(label_path).reshape(-1, 5)

            # Extract coordinates for unpadded + unscaled image
            x1 = w * (labels[:, 1] - labels[:, 3]/2)
            y1 = h * (labels[:, 2] - labels[:, 4]/2)
            x2 = w * (labels[:, 1] + labels[:, 3]/2)
            y2 = h * (labels[:, 2] + labels[:, 4]/2)
            # Adjust for added padding
            x1 += pad[1][0]
            y1 += pad[0][0]
            x2 += pad[1][0]
            y2 += pad[0][0]
            # Calculate ratios from coordinates
            labels[:, 1] = ((x1 + x2) / 2) / padded_w
            labels[:, 2] = ((y1 + y2) / 2) / padded_h
            labels[:, 3] *= w / padded_w
            labels[:, 4] *= h / padded_h
        # Fill matrix
        filled_labels = np.zeros((self.max_objects, 5))
        if labels is not None:
            filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]
        filled_labels = torch.from_numpy(filled_labels)

        return img_path, input_img, filled_labels

    def __len__(self):
        return len(self.img_files)

# ======================================================================================  
# END READ PASCAL VOC 2012
# ======================================================================================

if (NET_NAME == 'YOLO'):
    
    ##  Download Model config
    print("Downloading...")
    MODEL_CONFIG_PATH = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/yolov3.cfg'
    filename = wget.download(MODEL_CONFIG_PATH)
    print("MODEL_CONFIG_PATH: " + filename)
    print("OK")
    model_config_path = os.path.realpath(filename)
    
    
    ##  Download initial weights
    print("Downloading...")
    MODEL_WEIGHT_PATH = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/yolov3.weights'
    file_weights = wget.download(MODEL_WEIGHT_PATH)
    print("MODEL_WEIGHT_PATH: " + file_weights)
    print("OK")
    model_weight_path = os.path.realpath(file_weights)
    
    # xml to txt class files
    l=os.listdir(DIR_EXT)
    ids=[x.split('.')[0] for x in l]     

    # converts all .xml to .txt files 
    if ext == '.xml':
        cont = 0
        for f in os.listdir(DIR_EXT):
            if not f.startswith('.'):
                input_file = join(DIR_EXT, f)
                convert_annotation(input_file, ids[cont], DIR_EXT)
            cont += 1

        # renove all .xml files 
        filelist = [ f for f in os.listdir(DIR_EXT) if f.endswith(".xml") ]
        for f in filelist:
            os.remove(os.path.join(DIR_EXT, f))   
    
    # Load NET transforms
    exec(NET_TRANSFORM)
    # Load NET model
    exec(NET_MODEL)

    MODEL_NAME = 'YOLO'
    Net = Darknet(model_config_path)
  
    if USE_PRETRAINED_MODEL == False:
        print('Loading initial weights network...')
        Net.apply(weights_init_normal)
        assert Net is not None, f'model {MODEL_NAME} not available'
        model = Net
    else:
        print('Loading base network...')
        Net.load_weights(model_weight_path)
        assert Net is not None, f'model {MODEL_NAME} not available'
        model = Net     
    
    if use_gpu:
        model = model.cuda()
    model.train()
    
    loader = DataLoader(ListDataset(DATASET_TRAIN_PATH),
                        num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)

    optimizer_ft = SGD(model.parameters(), lr=LEARNING_RATE/BATCH_SIZE, momentum=MOMENTUM, dampening=0, weight_decay=WEIGHT_DECAY*BATCH_SIZE)   
    
###############################IF VISDOM IS ENABLED###############################
    if VISDOM_ENABLED is not None:
        visdom_endpoint = VISDOM_ENABLED.replace("http://", "")

        (VISDOM_HOST, VISDOM_PORT) = visdom_endpoint.split(":")  

        print("Connecting to %s" % VISDOM_PORT)
        viz = Visdom(server="http://"+VISDOM_HOST, port=VISDOM_PORT)
        assert viz.check_connection()


        win_global_loss_train = viz.line(Y = np.array([1]), X = np.array([1]), 
                                         opts = dict(
                                             xlabel = 'Epoch',
                                             ylabel = 'Loss',
                                             title = 'Training loss (per epoch)',
                                             ),
                                         )


        win_global_recall_train = viz.line(Y = np.array([1]), X = np.array([1]),
                                         opts = dict(
                                             xlabel = 'Epoch',
                                             ylabel = 'Recall',
                                             title = 'Training recall (per epoch)',
                                             ),
                                         )                                   


        win_train = viz.text("Training:\n")  
      
	##################################################################################      
    

    def train_model(model, optimizer, num_epochs):
      since = time.time() 
      best_model = model
      best_loss = None

      for epoch in range(1, NUM_EPOCHS+1):     
            
          for batch_i, (_, images, labels) in enumerate(loader):
              if use_gpu:
                  images = images.cuda()
                  labels = labels.cuda()
              inputs = Variable(images)
            
              targets = Variable(labels, requires_grad=False)
              optimizer.zero_grad()
          
              loss = model(inputs, targets)
              loss.backward()
              optimizer.step()
            
	  ##################################################IF VISDOM IS ENABLED###########################################
          if VISDOM_ENABLED is not None:
              viz.text('-' * 10, win=win_train, append=True) 
              viz.text('Epoch {}/{}'.format(epoch, epoch), win=win_train, append=True)     
              viz.text('Losses - x: {:.4f} y: {:.4f} w: {:.4f} h: {:.4f} conf: {:.4f} cls: {:.4f} total: {:.4f} recall:                      	                         	{:.4f}'.format(model.losses['x'], model.losses['y'], model.losses['w'], model.losses['h'], model.losses['conf'],                               			model.losses['cls'], loss.item(), model.losses['recall']), win=win_train, append=True)         
          
              # plot loss and accuracy per epoch
              if epoch == 1:
                  viz.line(Y = np.array([loss.item()]), X = np.array([epoch]), win = win_global_loss_train, update='replace')
                  viz.line(Y = np.array([model.losses['recall']]), X = np.array([epoch]), win = win_global_recall_train, update='replace')            
              elif epoch != 1:
                  viz.line(Y = np.array([loss.item()]), X = np.array([epoch]), win = win_global_loss_train, update='append')
                  viz.line(Y = np.array([model.losses['recall']]), X = np.array([epoch]), win = win_global_recall_train, update='append')      
               
	  ####################################################################################################################    


              print('[Epoch %d/%d, Batch %d/%d] [Losses -  x: %f, y: %f, w: %f, h: %f, conf: %f, cls: %f, total: %f, recall: %.5f]' %
                                    (epoch, epoch, batch_i, len(loader),
                                    model.losses['x'], model.losses['y'], model.losses['w'],
                                    model.losses['h'], model.losses['conf'], model.losses['cls'],
                                    loss.item(), model.losses['recall']))
              model.seen += images.size(0)
            
                       
          if best_loss is None or model.losses['conf'] < best_loss:
              best_loss = model.losses['conf']
              best_model = model
            
      time_elapsed = time.time() - since
      print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
     
      return best_model, best_loss
  
    # Return the best model              
    model, loss = train_model(model, optimizer_ft, num_epochs=NUM_EPOCHS)

    print('Saving trained model...')
    MODEL_PATH = join(MODEL_FOLDER, "model.weights")
    model.save_weights(MODEL_PATH)
    print("Model information: ")
    print("MODEL_PATH:  " + MODEL_PATH)
        
    # Save onnx trained model
    if MODEL_TYPE_ONNX:
        print('Saving trained model...')
        MODEL_ONNX_PATH = join(MODEL_FOLDER, "model.onnx")
        CHANNEL = 3
        #input - 3 channels, 416x416 image size,
        dummy_input = Variable(torch.randn(BATCH_SIZE, CHANNEL, IMG_SIZE[0], IMG_SIZE[1]))
        # Invoke export
        torch.onnx.export(model, dummy_input, MODEL_ONNX_PATH)
        print("Model information: ")
        print("MODEL_PATH:  " + MODEL_ONNX_PATH)
    else:        
        MODEL_ONNX_PATH = None
        

    if 'variables' in locals():
        variables.put("MODEL_FOLDER", MODEL_FOLDER)
        variables.put("MODEL_PATH", MODEL_PATH)    
        variables.put("MODEL_ONNX_PATH", MODEL_ONNX_PATH) 
        
    print("END Train_Image_Object_Detection_Model")

# ======================================================================================

##################################  END YOLO NET #######################################

# ======================================================================================
######################## AUTOML SETTINGS ##########################
#"""
token = variables.get("TOKEN")
# Convert from JSON to dict
token = json.loads(token)

# Return the loss value
result_map = {'token': token, 'loss': loss}
print('result_map: ', result_map)

resultMap.put("RESULT_JSON", json.dumps(result_map))
#"""
###################################################################
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <post>
        <script>
          <code language="bash">
            <![CDATA[

]]>
          </code>
        </script>
      </post>
      <outputFiles>
        <files accessMode="transferToGlobalSpace" includes="$MODEL_FOLDER/**"/>
      </outputFiles>
      <metadata>
        <positionTop>
            228.49609375
        </positionTop>
        <positionLeft>
            365.72265625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2646px;
            height:3501px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-95.48828125px;left:-296.71875px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_901" style="top: 100.5px; left: 301.727px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="You Only Look Once (YOLO) is a  single neural network to predict bounding boxes and class probabilities.
You can see more details in: https://pjreddie.com/media/files/papers/YOLOv3.pdf
https://github.com/eriklindernoren/PyTorch-YOLOv3"><img src="/automation-dashboard/styles/patterns/img/wf-icons/deep_detection.png" width="20px">&nbsp;<span class="name">YOLO</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_904" style="top: 100.5px; left: 429.727px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Load and return an image dataset."><img src="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png" width="20px">&nbsp;<span class="name">Import_Image_Dataset</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_907" style="top: 228.5px; left: 365.727px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Train a model using an image object detection network."><img src="/automation-dashboard/styles/patterns/img/wf-icons/deep_train.png" width="20px">&nbsp;<span class="name">Train_Image_Object_Detection_Model</span></a></div><svg style="position:absolute;left:340.5px;top:139.5px" width="150" height="90" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 129 89 C 139 39 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M113.120832,61.85427200000001 L98.29319382819449,46.71686316473311 L100.57026461923897,55.65078439857954 L92.08970622677401,59.267430545494136 L113.120832,61.85427200000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M113.120832,61.85427200000001 L98.29319382819449,46.71686316473311 L100.57026461923897,55.65078439857954 L92.08970622677401,59.267430545494136 L113.120832,61.85427200000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:469.5px;top:139.5px" width="45.5" height="90" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 89 C -10 39 34.5 50 24.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-0.6443596250000018,66.74071675 L12.67515666992815,50.260714150815026 L4.064825915890321,53.55649999125891 L-0.5090600888129444,45.55152860992471 L-0.6443596250000018,66.74071675" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-0.6443596250000018,66.74071675 L12.67515666992815,50.260714150815026 L4.064825915890321,53.55649999125891 L-0.5090600888129444,45.55152860992471 L-0.6443596250000018,66.74071675" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 341px; top: 130px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 494.5px; top: 130px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 470px; top: 259px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 470px; top: 219px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
