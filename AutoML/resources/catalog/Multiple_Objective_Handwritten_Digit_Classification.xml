<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Multiple_Objective_Handwritten_Digit_Classification" onTaskError="continueJobExecution" priority="normal" tags="DistributedAutoML,HyperParameterOptimization,Classification,AutoML,TunningAlgorithms" projectName="4.  Neural Architecture Search" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(,docker://activeeon/pytorch:cpu_latest,docker://activeeon/pytorch:gpu_latest)" name="CONTAINER_IMAGE" value="docker://activeeon/pytorch:gpu_latest"/>
    <variable advanced="true" description="If True, containers will run based on images containing libraries that are compatible with GPU." group="Container Parameters" hidden="false" model="PA:Boolean" name="CONTAINER_GPU_ENABLED" value="True"/>
    <variable advanced="true" description="If true, the container will run in rootless mode." group="Container Parameters" model="PA:Boolean" name="CONTAINER_ROOTLESS_ENABLED" value="False"/>
    <variable advanced="true" description="If True, do not mount users home directory if home is not the current working directory (only for singularity containers)." group="Container Parameters" model="PA:Boolean" name="CONTAINER_NO_HOME_ENABLED" value="False"/>
    <variable advanced="false" description="Number of times data is passed forward and backward through the training algorithm." hidden="false" model="PA:Integer" name="NUM_EPOCHS" value="10"/>
    <variable advanced="false" description="A set of specific variables (usecase-related) that are used in the model training process." hidden="false" model="PA:JSON" name="INPUT_VARIABLES" value="{&quot;CONV&quot;:&quot;nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=2)&quot;,&quot;DROPOUT&quot;:&quot;nn.AlphaDropout()&quot;,&quot;ACTIVATION&quot;:&quot;nn.ReLU(inplace=True)&quot;,&quot;POOLING&quot;:&quot;nn.MaxPool2d(kernel_size=2, stride=2)&quot;,&quot;NORMALIZATION&quot;:&quot;nn.InstanceNorm2d(100)&quot;,&quot;CONV2&quot;:&quot;nn.Conv2d(4, 4, kernel_size=5, stride=1, padding=2)&quot;,&quot;DROPOUT2&quot;:&quot;nn.Dropout()&quot;,&quot;ACTIVATION2&quot;:&quot;nn.ReLU6(inplace=True)&quot;,&quot;POOLING2&quot;:&quot;nn.MaxPool2d(kernel_size=2, stride=2)&quot;,&quot;NORMALIZATION2&quot;:&quot;nn.BatchNorm2d(4)&quot;,&quot;CRITERION&quot;:&quot;nn.CrossEntropyLoss()&quot;,&quot;OPTIMIZER&quot;:&quot;torch.optim.Adam(model.parameters(), lr=0.01)&quot;,&quot;MODEL&quot;:&quot;arch2&quot;}"/>
    <variable advanced="false" description="Specifies the representation of the search space which has to be defined using dictionaries or by entering the path of a json file stored in the catalog." hidden="false" model="PA:CATALOG_OBJECT" name="SEARCH_SPACE" value="ai-auto-ml-optimization/Handwritten_Digit_Classification_Search_Space"/>
  </variables>
  <description>
    <![CDATA[ Train a simple deep CNN on the MNIST dataset using the Pytorch library. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-auto-ml-optimization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/pytorch-logo-dark.png"/>
<info name="Documentation" value="PAIO/PAIOUserGuide.html#_neural_architecture_search"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="Handwritten_Digit_Classification">
      <description>
        <![CDATA[ Train a simple deep CNN on the MNIST dataset using the Pytorch library. ]]>
      </description>
      <variables>
        <variable inherited="true" advanced="false" description="Specifies the representation of the search space which has to be defined using dictionaries or by entering the path of a json file stored in the catalog." hidden="false" model="PA:CATALOG_OBJECT" name="SEARCH_SPACE" value="ai-auto-ml-optimization/Handwritten_Digit_Classification_Search_Space"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/pytorch-logo-dark.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_neural_architecture_search"/>
      </genericInformation>
      <depends>
        <task ref="get_automl_variables"/>
      </depends>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="bash">
            <![CDATA[
if [ ! -d /tmp/MNIST ]; then
  cd /tmp
  # fix from https://twitter.com/marc_lelarge/status/1370340616215859203
  # wget www.di.ens.fr/~lelarge/MNIST.tar.gz
  # wget -O MNIST.tar.gz https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.new.tar.gz
  wget -O MNIST.tar.gz https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.old.tar.gz
  tar -zxvf MNIST.tar.gz
fi
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")
print("BEGIN " + __file__)

import json
import numpy as np

import pip
def import_or_install(package):
    try:
        __import__(package)
    except ImportError:
        pip.main(['install', package])
import_or_install("thop")
from thop import profile

######################### CHECK GPU AVAILABILITY ##########################
try:
    import GPUtil as GPU
    from random import randint, uniform
    # Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # Show the utilization of all GPUs in a nice table
    print("Current GPU utilization:")
    GPU.showUtilization()
    # Get the first available GPU
    # order - The order in which the available GPU device ids are returned: first, last, random, load, memory
    # limit - Limits the number of GPU device ids returned. Must be positive integer. (default = 1)
    # maxLoad - Maximum current relative load for a GPU to be considered available. (default = 0.5)
    # maxMemory - Maximum current relative memory usage for a GPU to be considered available. (default = 0.5)
    # attempts - Number of attempts before giving up finding an available GPU. (default = 1)
    # interval - Interval in seconds between each attempt to find an available GPU. (default = 900 --> 15 min)
    # verbose - If True, prints the attempt number before each attempt and the GPU id if an available is found.
    # NOTE: this step will fail if all GPUs are busy
    # deviceIDs = GPUtil.getAvailable(order='first', limit=1, maxLoad=0.5, maxMemory=0.5)
    maxMemory = 0.4  # round(uniform(0.2, 0.5), 2)
    maxLoad = 0.4    # round(uniform(0.2, 0.5), 2)
    attempts = 10    # internal attempts
    interval = 10    # interval = randint(3, 30)  # 3sec to 30sec
    print('Looking for available GPU id with memory < ' +
          str(maxMemory * 100) + '%, and load < ' + str(maxLoad * 100) + '%)')
    print('# of attempts: ' + str(attempts) + ', interval:' + str(interval))
    DEVICE_ID_LIST = GPU.getFirstAvailable(
        order='random', maxMemory=maxMemory, maxLoad=maxLoad, attempts=attempts, interval=interval
    )
    DEVICE_ID = DEVICE_ID_LIST[0]  # grab first element from list
    print('First available GPU id (memory < ' +
          str(maxMemory * 100) + '%, load < ' + str(maxLoad * 100) + '%):')
    print(DEVICE_ID)
    # Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id
    os.environ["CUDA_VISIBLE_DEVICES"] = str(DEVICE_ID)
except:
    pass
###########################################################################

import torch
from torch import nn, optim
from torch.autograd import Variable
from torchvision import datasets, transforms

# Hyperparameters
num_classes = 10
batch_size = 32
num_epochs = int(variables.get("NUM_EPOCHS"))

############################ INPUT FROM AUTOML ############################
"""
SEARCH_SPACE:
ai-auto-ml-optimization/Handwritten_Digit_Classification_Search_Space

INPUT_VARIABLES:
{
  "CONV": "nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=2)",
  "DROPOUT": "nn.AlphaDropout()",
  "ACTIVATION": "nn.ReLU(inplace=True)",
  "POOLING": "nn.MaxPool2d(kernel_size=2, stride=2)",
  "NORMALIZATION": "nn.InstanceNorm2d(100)",
  "CONV2": "nn.Conv2d(4, 4, kernel_size=5, stride=1, padding=2)",
  "DROPOUT2": "nn.Dropout()",
  "ACTIVATION2": "nn.ReLU6(inplace=True)",
  "POOLING2": "nn.MaxPool2d(kernel_size=2, stride=2)",
  "NORMALIZATION2": "nn.BatchNorm2d(4)",
  "CRITERION": "nn.CrossEntropyLoss()",
  "OPTIMIZER": "torch.optim.Adam(model.parameters(), lr=0.01)",
  "MODEL": "arch2"
}
"""
input_variables = variables.get("INPUT_VARIABLES")
if input_variables is not None and input_variables != '':
    input_variables = json.loads(input_variables)
    CONV = input_variables["CONV"]
    ACTIVATION = input_variables["ACTIVATION"]
    POOLING = input_variables["POOLING"]
    NORMALIZATION = input_variables["NORMALIZATION"]
    CONV2 = input_variables["CONV2"]
    ACTIVATION2 = input_variables["ACTIVATION2"]
    POOLING2 = input_variables["POOLING2"]
    NORMALIZATION2 = input_variables["NORMALIZATION2"]
    CRITERION = input_variables["CRITERION"]
    OPTIMIZER = input_variables["OPTIMIZER"]
    MODEL = input_variables["MODEL"]
    if MODEL == 'arch2':
        DROPOUT = input_variables["DROPOUT"]
        DROPOUT2 = input_variables["DROPOUT2"]
###########################################################################

# Get current job ID
PA_JOB_ID = variables.get("PA_JOB_ID")

# Check parent job ID
PARENT_JOB_ID = genericInformation.get('PARENT_JOB_ID')

############################## BEGIN VISDOM ###############################
VISDOM_ENABLED = variables.get("VISDOM_ENABLED")
VISDOM_ENDPOINT = variables.get("VISDOM_ENDPOINT")

if VISDOM_ENABLED is not None and VISDOM_ENABLED.lower() == "true":
    VISDOM_ENABLED = True
else:
    VISDOM_ENABLED = False

if VISDOM_ENABLED and VISDOM_ENDPOINT is not None:
    try:
        from visdom import Visdom
        VISDOM_ENDPOINT = VISDOM_ENDPOINT.replace("http://", "")
        print("VISDOM_ENDPOINT: ", VISDOM_ENDPOINT)
        (VISDOM_HOST, VISDOM_PORT) = VISDOM_ENDPOINT.split(":")
        print("VISDOM_HOST: ", VISDOM_HOST)
        print("VISDOM_PORT: ", VISDOM_PORT)
        print("Connecting to %s:%s" % (VISDOM_HOST, VISDOM_PORT))
        vis = Visdom(server="http://" + VISDOM_HOST, port=int(VISDOM_PORT))
        assert vis.check_connection()
    except Exception as err:
        print("An exception occurred while plotting in Visdom")
        print(err)

env = 'main'
if PARENT_JOB_ID is not None:
    env = 'job_id_' + PARENT_JOB_ID
###########################################################################

print('-' * 30)
print('MODEL:', MODEL)
print('CONV:', CONV)
print('ACTIVATION:', ACTIVATION)
print('POOLING:', POOLING)
print('NORMALIZATION:', NORMALIZATION)
print('CONV2:', CONV)
print('ACTIVATION2:', ACTIVATION)
print('POOLING2:', POOLING)
print('NORMALIZATION2:', NORMALIZATION)
print('CRITERION:', CRITERION)
print('OPTIMIZER:', OPTIMIZER)
if MODEL == 'arch2':
    print('DROPOUT :', DROPOUT)
    print('DROPOUT2:', DROPOUT2)
print('-' * 30)

# transformations to be applied on images
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ])

# define the training and testing set
trainset = datasets.MNIST('/tmp/', download=False, train=True, transform=transform)
testset = datasets.MNIST('/tmp/', download=False, train=False, transform=transform)

# define trainloader and testloader
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)

# shape of training data
dataiter = iter(trainloader)
images, labels = dataiter.next()

# defining the model architecture
if MODEL == 'arch1':
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.cnn_layers = nn.Sequential(
                # Defining a 2D convolution layer
                eval(CONV),
                eval(NORMALIZATION),
                eval(ACTIVATION),
                eval(POOLING),
                # Defining another 2D convolution layer
                eval(CONV2),
                eval(NORMALIZATION2),
                eval(ACTIVATION2),
                eval(POOLING2),
            )
            self.linear_layers = nn.Sequential(
                nn.Linear(4 * 7 * 7, num_classes)
            )
        # Defining the forward pass
        def forward(self, x):
            x = self.cnn_layers(x)
            x = x.view(x.size(0), -1)
            x = self.linear_layers(x)
            return x
else:
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.cnn_layers = nn.Sequential(
                # Defining a 2D convolution layer
                eval(CONV),
                eval(DROPOUT),
                eval(NORMALIZATION),
                eval(ACTIVATION),
                eval(POOLING),
                # Defining another 2D convolution layer
                eval(CONV2),
                eval(DROPOUT2),
                eval(NORMALIZATION2),
                eval(ACTIVATION2),
                eval(POOLING2),
            )
            self.linear_layers = nn.Sequential(
                nn.Linear(4 * 7 * 7, num_classes)
            )
        # Defining the forward pass
        def forward(self, x):
            x = self.cnn_layers(x)
            x = x.view(x.size(0), -1)
            x = self.linear_layers(x)
            return x


# define the model
model = Net()

# define the optimizer
optimizer = eval(OPTIMIZER)

# define the loss function
criterion = eval(CRITERION)

# check if GPU is available
use_gpu = torch.cuda.is_available()
print("Using GPU: ", use_gpu)
if use_gpu:
    model = model.cuda()
    criterion = criterion.cuda()
print(model)

# train model
for i in range(num_epochs):
    running_loss = 0
    for data in trainloader:
        # get the inputs
        inputs, labels = data
        # wrap them in Variable
        if use_gpu:
            inputs = Variable(inputs.cuda())
            labels = Variable(labels.cuda())
        else:
            inputs, labels = Variable(inputs), Variable(labels)
        # training pass
        optimizer.zero_grad()
        # forward
        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)
        # this is where the model learns by backpropagating
        loss.backward()
        # and optimizes its weights here
        optimizer.step()
        # statistics
        running_loss += loss.item()
        # running_corrects += torch.sum(preds == labels.data)
        epoch_loss = running_loss / len(trainloader)
        # epoch_acc = running_corrects.item() / len(trainloader)
    else:
        print("Epoch {} - Training loss: {:.4f}".format(i + 1, epoch_loss))
        ############################## BEGIN VISDOM ###############################
        if VISDOM_ENABLED:
            try:
                # Line plot
                win_gloss = 'win_gloss_' + str(PA_JOB_ID)
                update = 'append' if vis.win_exists(win_gloss, env=env) else None
                vis.line(Y=np.array([i]), X=np.array([epoch_loss]), env=env, win=win_gloss, update=update)
            except Exception as err:
                print("An exception occurred while plotting in Visdom")
                print(err)
        ###########################################################################

# get predictions on test set and measure the performance
correct_count, all_count = 0, 0
for images, labels in testloader:
    for i in range(len(labels)):
        if torch.cuda.is_available():
            images = images.cuda()
            labels = labels.cuda()
        img = images[i].view(1, 1, 28, 28)
        with torch.no_grad():
            logps = model(img)
        ps = torch.exp(logps)
        probab = list(ps.cpu()[0])
        pred_label = probab.index(max(probab))
        true_label = labels.cpu()[i]
        if true_label == pred_label:
            correct_count += 1
        all_count += 1

print("Number of tested images: ", all_count)
print("Model accuracy: ", (correct_count / all_count))

# Score trained model
acc1 = (correct_count / all_count)
acc = 1 - (correct_count / all_count)
accuracy = np.round(acc, 4)
input = torch.randn(1, 1, 28, 28)
if use_gpu:
    model = model.cpu()
macs, params = profile(model, inputs=(input, ))
macs = np.round(macs / 1e6, 4)
params = np.round(params, 4)
loss = [accuracy, macs, params]
  
print('Number of parameters: ', params)
print('MACs: ', macs)

############################ OUTPUT FOR AUTOML ############################
# Convert from JSON to dict
token = variables.get("TOKEN")
token = json.loads(token)

# Return the loss value
result_map = {'token': token, 'loss': loss}
print('result_map: ', result_map)
resultMap.put("RESULT_JSON", json.dumps(result_map))

# To appear in Job Analytics
resultMap.put("LOSS", str(loss))
resultMap.put("ACCURACY", str(accuracy))
resultMap.put("MACs", str(macs))
resultMap.put("PARAMETER", str(params))
###########################################################################

print("END " + __file__)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="end"/>
      <metadata>
        <positionTop>
            236.76217651367188
        </positionTop>
        <positionLeft>
            180.49481201171875
        </positionLeft>
      </metadata>
    </task>
    <task fork="false" name="get_automl_variables">
      <description>
        <![CDATA[ The simplest task, ran by a Groovy engine. ]]>
      </description>
      <scriptExecutable>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/ai-auto-ml-optimization/resources/get_automl_variables/raw"/>
        </script>
      </scriptExecutable>
      <controlFlow block="start"/>
      <metadata>
        <positionTop>
            104.765625
        </positionTop>
        <positionLeft>
            138.498291015625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2476px;
            height:3432px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-99.765625px;left:-133.498291015625px"><div class="task block-end ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_7" style="top: 236.766px; left: 180.5px; z-index: 24;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Train a simple deep CNN on the MNIST dataset using the Pytorch library."><img src="/automation-dashboard/styles/patterns/img/wf-icons/pytorch-logo-dark.png" width="20px">&nbsp;<span class="name">Handwritten_Digit_Classification</span></a></div><div class="task block-start _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_10" style="top: 104.766px; left: 138.5px; z-index: 24;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="The simplest task, ran by a Groovy engine."><img src="/studio/images/Groovy.png" width="20px">&nbsp;<span class="name">get_automl_variables</span></a></div><svg style="position:absolute;left:193px;top:144.265625px" width="87" height="93" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 66 92 C 76 42 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style="--darkreader-inline-stroke:#a8a095;" data-darkreader-inline-stroke=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M61.377312,67.09912399999999 L52.78432054095478,47.73006367729006 L51.79110816891262,56.89595311498363 L42.58114965593842,57.31626750837745 L61.377312,67.09912399999999" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)" data-darkreader-inline-fill="" data-darkreader-inline-stroke="" style="--darkreader-inline-fill:#a8a095; --darkreader-inline-stroke:#a8a095;"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M61.377312,67.09912399999999 L52.78432054095478,47.73006367729006 L51.79110816891262,56.89595311498363 L42.58114965593842,57.31626750837745 L61.377312,67.09912399999999" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)" data-darkreader-inline-fill="" data-darkreader-inline-stroke="" style="--darkreader-inline-fill:#a8a095; --darkreader-inline-stroke:#a8a095;"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 259.5px; top: 266.766px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill:#a8a095; --darkreader-inline-stroke:none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 259.5px; top: 226.766px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill:#a8a095; --darkreader-inline-stroke:none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 193.5px; top: 134.766px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill:#a8a095; --darkreader-inline-stroke:none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
