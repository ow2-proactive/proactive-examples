<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.13" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="TensorFlow_Keras_Multi_Node_Multi_GPU_On_Slurm" onTaskError="continueJobExecution" priority="normal" projectName="5. Distributed Training" xsi:schemaLocation="urn:proactive:jobdescriptor:3.13 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.13/schedulerjob.xsd">
  <variables>
    <variable name="NUMBER_OF_COMPUTE_NODES" model="PA:INTEGER" value="2" description="Number of compute nodes." advanced="false" hidden="false" group="Resource Management"/>
    <variable name="NUMBER_OF_GPU_PER_NODE" model="PA:INTEGER" value="2" description="Number of GPU per node." advanced="false" hidden="false" group="Resource Management"/>
    <variable name="CONTAINER_PLATFORM" model="PA:LIST(no-container,docker,podman,singularity)" value="no-container" advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false"/>
    <variable name="PYTHON_COMMAND" model="PA:LIST(,python3,/nfs/virtualenvs/tf-nightly-gpu/bin/python3)" value="/nfs/virtualenvs/tf-nightly-gpu/bin/python3" advanced="true" description="Python version." hidden="false"/>
    <variable name="CONTAINER_IMAGE" model="PA:LIST(,/nfs/singularity/images/activeeon_cuda2.sif,activeeon/cuda2)" value="/nfs/singularity/images/activeeon_cuda2.sif" advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false"/>
    <variable name="SEARCH_SPACE" value="{&quot;OPTIMIZER&quot;: {&quot;choice&quot;: [&quot;Adam&quot;, &quot;SGD&quot;, &quot;RMSprop&quot;]}, &quot;NUM_EPOCHS&quot;: {&quot;choice&quot;: [10, 20, 30, 40]}}" description="Specifies the representation of the search space which has to be defined using dictionaries or by entering the path of a json file stored in the catalog." advanced="false" hidden="false"/>
    <variable name="INPUT_VARIABLES" model="PA:JSON" value="{&quot;OPTIMIZER&quot;: &quot;Adam&quot;, &quot;NUM_EPOCHS&quot;: 10}" description="A set of specific variables (usecase-related) that are used in the model training process." advanced="false" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ Simple TensorFlow + Keras template for distributed training (multi-node multi-gpu) on Slurm with AutoML support. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-auto-ml-optimization"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/tensorflow.png"/>
<info name="PYTHON_COMMAND" value="$PYTHON_COMMAND"/>
<info name="Documentation" value="PML/PMLUserGuide.html#_distributed_training"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="false" name="TensorFlow_Multi_Node_Task" preciousResult="true">
      <description>
        <![CDATA[ This is a TensorFlow + Keras workflow template for distributed training (multi-node multi-gpu) with AutoML support. ]]>
      </description>
      <variables>
        <variable inherited="false" model="PA:URI" name="DATASET_PATH" value="/nfs/datasets/mnist.pickle" description="Path or name of the file that contains the dataset."/>
        <variable inherited="false" model="PA:LIST(,/nfs/shared/tensorboard-logs)" name="TENSORBOARD_LOG_DIR" value="" description="Path of the directory where to save the Tensorboard log files to be used by the TensorBoard service for visualization."/>
        <variable inherited="false" name="SCRIPT_PATH" value="/nfs/activeeon/job_id_${PA_JOB_ID}_script.py" description="Temporally file path where the task pre-scrit will be stored for execution."/>
        <variable inherited="false" name="RESULT_FILE" value="job_id_${PA_JOB_ID}_result.json" description="Temporally file path used to store results to be parsed by the post-script."/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/python.png"/>
        <info name="task.documentation" value="PML/PMLUserGuide.html#_distributed_training"/>
        <info name="PRE_SCRIPT_AS_FILE" value="$SCRIPT_PATH"/>
      </genericInformation>
      <depends>
        <task ref="get_automl_variables"/>
      </depends>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[
from __future__ import print_function
from __future__ import absolute_import
from __future__ import division

import re
import os
import sys
import json
import time
import pickle
import shutil
import socket

import numpy as np

__file__ = os.environ['variables_PA_TASK_NAME']
print("BEGIN " + __file__)
print("Running on: ", socket.gethostname())

# Get the job ID from Proactive
PA_JOB_ID = int(os.environ['variables_PA_JOB_ID'])

############################ INPUT FROM AUTOML ############################
# Get the workflow input variables generated by AutoML.
# The AutoML workflow uses the SEARCH_SPACE workflow variable
# to generate a set of parameters to be used to train your model.
#
# Example of search space for hyper parameter optimization:
#   SEARCH_SPACE: {"OPTIMIZER": {"choice": ["Adam", "SGD", "RMSprop"]}, "NUM_EPOCHS": {"choice": [10, 20, 30, 40]}}
# Put it in your workflow variables.
#
# For more info, please see:
# https://try.activeeon.com/doc/PML/PMLUserGuide.html#_AutoML
#
input_variables = os.environ['variables_INPUT_VARIABLES']
if input_variables is not None and input_variables != '':
    input_variables = json.loads(input_variables)
    OPTIMIZER = input_variables["OPTIMIZER"]
    NUM_EPOCHS = int(input_variables["NUM_EPOCHS"])
    # ...
print('Selected optimizer: ', OPTIMIZER)
print('Selected epochs:    ', NUM_EPOCHS)
###########################################################################

############################## GPU SETTINGS ###############################
from subprocess import Popen, PIPE
from distutils import spawn
# import os
import math
import random
# import time
# import sys
import platform


class GPU:
    def __init__(self, ID, uuid, load, memoryTotal, memoryUsed, memoryFree, driver, gpu_name, serial, display_mode, display_active, temp_gpu):
        self.id = ID
        self.uuid = uuid
        self.load = load
        self.memoryUtil = float(memoryUsed)/float(memoryTotal)
        self.memoryTotal = memoryTotal
        self.memoryUsed = memoryUsed
        self.memoryFree = memoryFree
        self.driver = driver
        self.name = gpu_name
        self.serial = serial
        self.display_mode = display_mode
        self.display_active = display_active
        self.temperature = temp_gpu


def safeFloatCast(strNumber):
    try:
        number = float(strNumber)
    except ValueError:
        number = float('nan')
    return number


def getGPUs():
    if platform.system() == "Windows":
        # If the platform is Windows and nvidia-smi
        # could not be found from the environment path,
        # try to find it from system drive with default installation path
        nvidia_smi = spawn.find_executable('nvidia-smi')
        if nvidia_smi is None:
            nvidia_smi = "%s\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe" % os.environ['systemdrive']
    else:
        nvidia_smi = "nvidia-smi"
    # Get ID, processing and memory utilization for all GPUs
    try:
        p = Popen([nvidia_smi, "--query-gpu=index,uuid,utilization.gpu,memory.total,memory.used,memory.free,driver_version,name,gpu_serial,display_active,display_mode,temperature.gpu", "--format=csv,noheader,nounits"], stdout=PIPE)
        stdout, _ = p.communicate()
    except:
        return []
    output = stdout.decode('UTF-8')
    # Parse output
    lines = output.split(os.linesep)
    numDevices = len(lines)-1
    GPUs = []
    for g in range(numDevices):
        line = lines[g]
        vals = line.split(', ')
        for i in range(12):
            if i == 0:
                deviceIds = int(vals[i])
            elif i == 1:
                uuid = vals[i]
            elif i == 2:
                gpuUtil = safeFloatCast(vals[i])/100
            elif i == 3:
                memTotal = safeFloatCast(vals[i])
            elif i == 4:
                memUsed = safeFloatCast(vals[i])
            elif i == 5:
                memFree = safeFloatCast(vals[i])
            elif i == 6:
                driver = vals[i]
            elif i == 7:
                gpu_name = vals[i]
            elif i == 8:
                serial = vals[i]
            elif i == 9:
                display_active = vals[i]
            elif i == 10:
                display_mode = vals[i]
            elif i == 11:
                temp_gpu = safeFloatCast(vals[i])
        GPUs.append(GPU(deviceIds, uuid, gpuUtil, memTotal, memUsed, memFree, driver, gpu_name, serial, display_mode, display_active, temp_gpu))
    return GPUs  # (deviceIds, gpuUtil, memUtil)


def getAvailability(GPUs, maxLoad=0.5, maxMemory=0.5, memoryFree=0, includeNan=False, excludeID=[], excludeUUID=[]):
    # Determine, which GPUs are available
    GPUavailability = [
        1 if (gpu.memoryFree >= memoryFree)
        and (gpu.load < maxLoad or (includeNan and math.isnan(gpu.load)))
        and (gpu.memoryUtil < maxMemory or (includeNan and math.isnan(gpu.memoryUtil)))
        and ((gpu.id not in excludeID) and (gpu.uuid not in excludeUUID)) else 0 for gpu in GPUs
    ]
    return GPUavailability


def getAvailable(order='first', limit=1, maxLoad=0.5, maxMemory=0.5, memoryFree=0, includeNan=False, excludeID=[], excludeUUID=[]):
    # order = first | last | random | load | memory
    #    first --> select the GPU with the lowest ID (DEFAULT)
    #    last --> select the GPU with the highest ID
    #    random --> select a random available GPU
    #    load --> select the GPU with the lowest load
    #    memory --> select the GPU with the most memory available
    # limit = 1 (DEFAULT), 2, ..., Inf
    #     Limit sets the upper limit for the number of GPUs to return. E.g. if limit = 2, but only one is available, only one is returned.
    # Get device IDs, load and memory usage
    GPUs = getGPUs()
    # Determine, which GPUs are available
    GPUavailability = getAvailability(GPUs, maxLoad=maxLoad, maxMemory=maxMemory, memoryFree=memoryFree, includeNan=includeNan, excludeID=excludeID, excludeUUID=excludeUUID)
    availAbleGPUindex = [idx for idx in range(0, len(GPUavailability)) if (GPUavailability[idx] == 1)]
    # Discard unavailable GPUs
    GPUs = [GPUs[g] for g in availAbleGPUindex]
    # Sort available GPUs according to the order argument
    if order == 'first':
        GPUs.sort(key=lambda x: float('inf') if math.isnan(x.id) else x.id, reverse=False)
    elif order == 'last':
        GPUs.sort(key=lambda x: float('-inf') if math.isnan(x.id) else x.id, reverse=True)
    elif order == 'random':
        GPUs = [GPUs[g] for g in random.sample(range(0, len(GPUs)), len(GPUs))]
    elif order == 'load':
        GPUs.sort(key=lambda x: float('inf') if math.isnan(x.load) else x.load, reverse=False)
    elif order == 'memory':
        GPUs.sort(key=lambda x: float('inf') if math.isnan(x.memoryUtil) else x.memoryUtil, reverse=False)
    # Extract the number of desired GPUs, but limited to the total number of available GPUs
    GPUs = GPUs[0:min(limit, len(GPUs))]
    # Extract the device IDs from the GPUs and return them
    deviceIds = [gpu.id for gpu in GPUs]
    return deviceIds


try:
    # Get how many GPUs per node are needed
    NUMBER_OF_GPU_PER_NODE = int(os.environ['variables_NUMBER_OF_GPU_PER_NODE'])
    print("NUMBER_OF_GPU_PER_NODE:", NUMBER_OF_GPU_PER_NODE)
    # Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # Get a list ids of available GPUs
    # The availability is determined based on current memory usage and load.
    # order - The order in which the available GPU device ids are returned: first, last, random, load, memory
    # limit - Limits the number of GPU device ids returned. Must be positive integer. (default = 1)
    # maxLoad - Maximum current relative load for a GPU to be considered available. (default = 0.5)
    # maxMemory - Maximum current relative memory usage for a GPU to be considered available. (default = 0.5)
    print('Looking for available GPU')
    DEVICE_ID_LIST = getAvailable(order='first', limit=NUMBER_OF_GPU_PER_NODE)
    # DEVICE_ID = DEVICE_ID_LIST[0]  # grab first element from list
    DEVICE_ID = ','.join(map(str, DEVICE_ID_LIST))
    print('List of available GPU:', DEVICE_ID)
    # Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id
    os.environ["CUDA_VISIBLE_DEVICES"] = str(DEVICE_ID)
except Exception as ex:
    print("Error while defining the CUDA visible devices!")
    print(ex)
    pass
###########################################################################

################################## SLURM ##################################
#
# from tensorflow_on_slurm import tf_config_from_slurm
#
def tf_config_from_slurm(ps_number, port_number=2222):
    """
    Creates configuration for a distributed tensorflow session
    from environment variables  provided by the Slurm cluster
    management system.

    @param: ps_number number of parameter servers to run
    @param: port_number port number to be used for communication
    @return: a tuple containing cluster with fields cluster_spec,
             task_name and task_id
    """

    nodelist = os.environ["SLURM_JOB_NODELIST"]
    nodename = os.environ["SLURMD_NODENAME"]
    nodelist = _expand_nodelist(nodelist)
    num_nodes = int(os.getenv("SLURM_JOB_NUM_NODES"))

    if len(nodelist) != num_nodes:
        raise ValueError("Number of slurm nodes {} not equal to {}".format(len(nodelist), num_nodes))

    if nodename not in nodelist:
        raise ValueError("Nodename({}) not in nodelist({}). This should not happen! ".format(nodename,nodelist))

    ps_nodes = [node for i, node in enumerate(nodelist) if i < ps_number]
    worker_nodes = [node for i, node in enumerate(nodelist) if i >= ps_number]

    if nodename in ps_nodes:
        my_job_name = "ps"
        my_task_index = ps_nodes.index(nodename)
    else:
        my_job_name = "worker"
        my_task_index = worker_nodes.index(nodename)
        # if len(worker_nodes) > 0:
        #     my_task_index = worker_nodes.index(nodename)
        # else:
        #     my_task_index = 1

    worker_sockets = [":".join([node, str(port_number)]) for node in worker_nodes]
    # if len(worker_nodes) > 0:
    #     worker_sockets = [":".join([node, str(port_number)]) for node in worker_nodes]
    # else:
    #     worker_sockets = [":".join([node, str(port_number+1)]) for node in ps_nodes]

    ps_sockets = [":".join([node, str(port_number)]) for node in ps_nodes]
    cluster = {"worker": worker_sockets, "ps" : ps_sockets}

    return cluster, my_job_name, my_task_index

def _pad_zeros(iterable, length):
    return (str(t).rjust(length, '0') for t in iterable) # default
    # return (str(t) for t in iterable) # for IDRIS

def _expand_ids(ids):
    ids = ids.split(',')
    result = []
    for id in ids:
        if '-' in id:
            begin, end = [int(token) for token in id.split('-')]
            token = [int(token) for token in id.split('-')]
            result.extend(_pad_zeros(range(begin, end+1), len(token)))
        else:
            result.append(id)
    return result

def _expand_nodelist(nodelist):
    nodes = nodelist.split(',')
    result = []
    for node in nodes:
        try:
            prefix, ids = re.findall("(.*)\[(.*)\]", node)[0]
            ids = _expand_ids(ids)
            result_aux = [prefix + str(id) for id in ids]
            result = result + result_aux
        except IndexError:
            result.append(node)
    return result

def _worker_task_id(nodelist, nodename):
    return nodelist.index(nodename)

cluster, my_job_name, my_task_index = tf_config_from_slurm(ps_number=1)
print("cluster: ", cluster, ", my_job_name: ", my_job_name, ", my_task_index: ", my_task_index)
os.environ['TF_CONFIG'] = json.dumps(cluster)
num_workers = len(cluster['worker'])
# per_worker_batch_size = 128
per_worker_batch_size = 16

import tensorflow as tf
print("tf.__version__: ", tf.__version__)
# https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory
# config = tf.ConfigProto()
# config.gpu_options.allow_growth=True
# sess = tf.Session(config=config)
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

############################### TENSORBOARD ###############################
from tensorflow.keras.callbacks import TensorBoard
tensorboard = None
tensorboard_logs_base_dir = os.environ['variables_TENSORBOARD_LOG_DIR']
if len(tensorboard_logs_base_dir) > 0:
    PA_JOB_ID_LOG = "job_ID_" + str(PA_JOB_ID)
    tensorboard_log_dir = os.path.join(tensorboard_logs_base_dir, PA_JOB_ID_LOG)
    try:
        os.mkdir(tensorboard_log_dir)
    except:
        pass
    tensorboard = TensorBoard(log_dir=tensorboard_log_dir)
else:
    print("Tensorboard disabled")
###########################################################################

# https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy
# strategy = tf.distribute.MultiWorkerMirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

def mnist_dataset(batch_size, train_size):
    # (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    DATASET_PATH = os.environ['variables_DATASET_PATH']
    with open(DATASET_PATH, 'rb') as fp:
        x_train, y_train, x_test, y_test = pickle.load(fp)
    # The `x` arrays are in uint8 and have values in the range [0, 255].
    # You need to convert them to float32 with values in the range [0, 1]
    x_train, x_test = x_train / 255.0, x_test / 255.0
    y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)
    # Create the tf.data.Dataset from the existing data
    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    # Split the data into a train and a test set.
    SAMPLES_TRAINING = int(len(x_train)*train_size) # e.g. 80% of data for training, 20% for validation
    train_dataset = dataset.take(SAMPLES_TRAINING)
    val_dataset = dataset.skip(SAMPLES_TRAINING)
    # Both datasets have to be repeated and batched appropriately
    # train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
    train_dataset = train_dataset.repeat().batch(batch_size)
    val_dataset = val_dataset.repeat().batch(batch_size)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).repeat().batch(batch_size)
    train_len = len(x_train)
    return train_len, train_dataset, val_dataset, test_dataset


def build_and_compile_cnn_model(optimizer):
    model = tf.keras.Sequential([
            tf.keras.Input(shape=(28, 28)),
            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(32, 3, activation='relu'),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(10)
    ])
    model.compile(
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            optimizer=optimizer,
            metrics=['accuracy'])
    return model


# Here the batch size scales up by number of workers since
# `tf.data.Dataset.batch` expects the global batch size. Previously, you used 64,
# and now this becomes 128.
global_batch_size = per_worker_batch_size * num_workers
train_size = 0.8
train_len, train_dataset, val_dataset, test_dataset = mnist_dataset(global_batch_size, train_size)

# learning_rate = 0.001
learning_rate = 0.01
optimizer = None
if OPTIMIZER.lower() == 'adam':
    optimizer = tf.keras.optimizers.Adam(learning_rate)
if OPTIMIZER.lower() == 'sgd':
    optimizer = tf.keras.optimizers.SGD(learning_rate)
if OPTIMIZER.lower() == 'rmsprop':
    optimizer = tf.keras.optimizers.RMSprop(learning_rate)
if optimizer is None:
    sys.exit("Optimizer not defined!")

with strategy.scope():
    # Model building/compiling need to be within `strategy.scope()`.
    multi_worker_model = build_and_compile_cnn_model(optimizer)

# Keras' `model.fit()` trains the model with specified number of epochs and
# number of steps per epoch. Note that the numbers here are for demonstration
# purposes only and may not sufficiently produce a model with good quality.
callbacks = None
if tensorboard is not None:
    callbacks=[tensorboard]
# steps_per_epoch = (train_len*train_size)//global_batch_size
# validation_steps = (train_len*(1-train_size))//global_batch_size
steps_per_epoch = 30
validation_steps = 10
print("steps_per_epoch:  ", steps_per_epoch)
print("validation_steps: ", validation_steps)
history = multi_worker_model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=NUM_EPOCHS,
    steps_per_epoch=steps_per_epoch,
    validation_steps=validation_steps,
    callbacks=callbacks
)
# print(history.history)

################################# RESULTS #################################
if my_job_name == 'ps':
    train_acc = history.history['accuracy'][-1]
    train_loss = history.history['loss'][-1]
    val_acc = history.history['val_accuracy'][-1]
    val_loss = history.history['val_loss'][-1]
    print("train_acc:  ", train_acc)
    print("train_loss: ", train_loss)
    print("val_acc:    ", val_acc)
    print("val_loss:   ", val_loss)
############################ OUTPUT FOR AUTOML ############################
    data={}
    data['train_loss'] = train_loss
    data['train_acc'] = train_acc
    data['val_loss'] = val_loss
    data['val_acc'] = val_acc
    # result_file = 'job_id_'+str(PA_JOB_ID)+'_result.json'
    result_file = os.environ['variables_RESULT_FILE']
    with open(result_file, 'w') as outfile:
        json.dump(data, outfile)
###########################################################################

print("END " + __file__)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
echo "SLURM_JOB_ID:$SLURM_JOB_ID SLURM_JOB_NAME:$SLURM_JOB_NAME SLURM_JOB_NODELIST:$SLURM_JOB_NODELIST SLURMD_NODENAME:$SLURMD_NODENAME SLURM_JOB_NUM_NODES:$SLURM_JOB_NUM_NODES"
echo "---------------------------------------"
CONTAINER_PLATFORM=$variables_CONTAINER_PLATFORM
CONTAINER_IMAGE=$variables_CONTAINER_IMAGE
PYTHON_COMMAND=$variables_PYTHON_COMMAND
DATASET_PATH=$variables_DATASET_PATH
SCRIPT_PATH=$variables_SCRIPT_PATH
TENSORBOARD_LOG_DIR=$variables_TENSORBOARD_LOG_DIR
echo "Runtime configuration:"
echo "CONTAINER_PLATFORM:  $CONTAINER_PLATFORM"
echo "CONTAINER_IMAGE:     $CONTAINER_IMAGE"
echo "PYTHON_COMMAND:      $PYTHON_COMMAND"
echo "DATASET_PATH:        $DATASET_PATH"
echo "SCRIPT_PATH:         $SCRIPT_PATH"
echo "TENSORBOARD_LOG_DIR: $TENSORBOARD_LOG_DIR"
echo "---------------------------------------"
#
# no-container mode (baremetal)
#
if [[ "$CONTAINER_PLATFORM" == "no-container" ]]; then
	# source /nfs/activeeon/virtualenvs/tf-nightly-gpu/bin/activate
	export PATH=/usr/local/cuda/bin:$PATH
	export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    echo "PATH: $PATH"
    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
    echo "srun $PYTHON_COMMAND $SCRIPT_PATH"
    srun $PYTHON_COMMAND $SCRIPT_PATH
fi
#
# singularity container
#
if [[ "$CONTAINER_PLATFORM" == "singularity" ]]; then
	echo "srun singularity exec --nv $CONTAINER_IMAGE python3 $SCRIPT_PATH"
    srun singularity exec --nv $CONTAINER_IMAGE python3 $SCRIPT_PATH
fi
#
# docker container
#
if [[ "$CONTAINER_PLATFORM" == "docker" ]]; then
	# TO SEE: https://gitlab.inria.fr/dkalaina/titanic-docs/-/tree/master/scripts
	# echo "srun docker run --rm --shm-size=256M --runtime=nvidia --privileged --network=host -v $DATASET_PATH:$DATASET_PATH -v $SCRIPT_PATH:$SCRIPT_PATH -v $TENSORBOARD_LOG_DIR:$TENSORBOARD_LOG_DIR $CONTAINER_IMAGE bash -c 'python3 $SCRIPT_PATH'"
    echo "docker not supported!"
fi
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="end"/>
      <post>
        <script>
          <code language="cpython">
            <![CDATA[
import os
import json

__file__ = variables.get("PA_TASK_NAME")
print("BEGIN " + __file__)

# Get the job ID from Proactive
PA_JOB_ID = int(variables.get("PA_JOB_ID"))

############################ OUTPUT FOR AUTOML ############################
# Read and send the loss to AutoML
loss = 0
acc = 0
# result_file = 'job_id_'+str(PA_JOB_ID)+'_result.json'
result_file = variables.get("RESULT_FILE")
if os.path.isfile(result_file):
    with open(result_file) as json_file:
        data = json.load(json_file)
    loss = data['val_loss']
    acc = data['val_acc']
else:
    print(result_file + " does not exists!")
print('Loss:', loss)
print('Accuracy:', acc)

# Return the token + loss value
token = variables.get("TOKEN")
token = json.loads(token)
result_map = {'token': token, 'loss':  loss}
resultMap.put("RESULT_JSON", json.dumps(result_map))

# To appear in Job Analytics
resultMap.put("LOSS", str(loss))
resultMap.put("ACCURACY", str(acc))
###########################################################################

print("END " + __file__)
]]>
          </code>
        </script>
      </post>
      <cleaning>
        <script>
          <code language="bash">
            <![CDATA[
echo "Cleaning files"
echo "rm -f $variables_SCRIPT_PATH"
echo "rm -f $variables_RESULT_FILE"
rm -f $variables_SCRIPT_PATH
rm -f $variables_RESULT_FILE
]]>
          </code>
        </script>
      </cleaning>
      <metadata>
        <positionTop>
            419.53125
        </positionTop>
        <positionLeft>
            95.53125
        </positionLeft>
      </metadata>
    </task>
    <task fork="false" name="get_automl_variables">
      <description>
        <![CDATA[ Get the input variables from the Distributed_Auto_ML workflow during hyperparameter optimization. ]]>
      </description>
      <scriptExecutable>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/ai-auto-ml-optimization/resources/get_automl_variables/raw"/>
        </script>
      </scriptExecutable>
      <controlFlow block="start"/>
      <metadata>
        <positionTop>
            291.53125
        </positionTop>
        <positionLeft>
            95.53125
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2074px;
            height:2820px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-286.53125px;left:-90.53125px"><div class="task block-end ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_4" style="top: 419.531px; left: 95.5312px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This is a TensorFlow + Keras workflow template for distributed training (multi-node multi-gpu) with AutoML support."><img src="/automation-dashboard/styles/patterns/img/wf-icons/python.png" width="20px">&nbsp;<span class="name">TensorFlow_Multi_Node_Task</span></a></div><div class="task block-start _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_7" style="top: 291.531px; left: 95.5312px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Get the input variables from the Distributed_Auto_ML workflow during hyperparameter optimization."><img src="/studio/images/Groovy.png" width="20px">&nbsp;<span class="name">get_automl_variables</span></a></div><svg style="position:absolute;left:150px;top:331.5px" width="40.5" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 19.5 88 C 29.5 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M20.665968,66.303232 L21.850572046463647,45.146750410352304 L16.787023539475157,52.8513254484303 L8.398665494893953,49.02569487087714 L20.665968,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M20.665968,66.303232 L21.850572046463647,45.146750410352304 L16.787023539475157,52.8513254484303 L8.398665494893953,49.02569487087714 L20.665968,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 170px; top: 450px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 170px; top: 410px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 150.5px; top: 322px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
