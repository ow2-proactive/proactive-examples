<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="elasticsearch_connector" projectName="3. NoSQL"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="ELASTICSEARCH_HOSTNAME" value="localhost" />
    <variable name="ELASTICSEARCH_INDEX" value="" />
    <variable name="ELASTICSEARCH_PORT" value="9200" model="PA:Integer"/>
    <variable name="ELASTICSEARCH_QUERY" value="" />
  </variables>
  <description>
    <![CDATA[ Load data from Elasticsearch. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="data-connectors"/>
    <info name="Documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_nosql"/>
    <info name="group" value="public-objects"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/elasticsearch.svg"/>
  </genericInformation>
  <taskFlow>
    <task name="import_from_elasticsearch">
      <description>
        <![CDATA[ Load data from Elasticsearch.
This task uses the task variable NOSQL_DRIVER as a driver to connect to the database. The specified default driver "elasticsearch-py" is already provided for this task. To use another driver, make sure you have it properly installed before.
The task requires the following third-party credentials : ELASTICSEARCH_USERNAME and ELASTICSEARCH_PASSWORD. Please refer to the User documentation to learn how to add third-party credentials.
It also requires a QUERY and an INDEX to fetch data. By default, it will fetch all documents from the specified index.
The imported data is exported in a JSON format. ]]>
      </description>
      <variables>
        <variable name="ELASTICSEARCH_QUERY" value="" inherited="true" />
        <variable name="ELASTICSEARCH_INDEX" value="" inherited="true" />
        <variable name="NOSQL_DRIVER" value="elasticsearch-py" inherited="false" />
        <variable name="ELASTICSEARCH_SIZE" value="10" inherited="false" model="PA:Integer"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/elasticsearch.svg"/>
        <info name="Documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_nosql"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
# In the Java Home location field, use the value: "/usr" to force using the JRE provided in the docker image below (Recommended).
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pandas as pd
import json
from elasticsearch import Elasticsearch

print("BEGIN Import_Data FROM elasticsearch using " + variables.get("NOSQL_DRIVER") + " connector")

SEARCH_SIZE = 10
PORT = 9200

if variables.get("ELASTICSEARCH_HOSTNAME"):
    HOSTNAME = variables.get("ELASTICSEARCH_HOSTNAME")
else:
    print("ELASTICSEARCH_HOSTNAME not defined by the user.")
    sys.exit(1)
if variables.get("ELASTICSEARCH_PORT"):
    PORT = int(variables.get("ELASTICSEARCH_PORT"))
else:    
    print("ELASTICSEARCH_PORT not defined by the user. Using the default value: " + PORT)
if variables.get("ELASTICSEARCH_INDEX"):
    INDEX = variables.get("ELASTICSEARCH_INDEX")
else:
    print("ELASTICSEARCH_INDEX not defined by the user. Searching in all indices.")
    INDEX = ""
if variables.get("ELASTICSEARCH_SIZE"):
    SEARCH_SIZE = int(variables.get("ELASTICSEARCH_SIZE"))
if credentials.get("ELASTICSEARCH_USERNAME") is not None and credentials.get("ELASTICSEARCH_PASSWORD") is not None:
    ELASTICSEARCH_USERNAME = credentials.get("ELASTICSEARCH_USERNAME")
    ELASTICSEARCH_PASSWORD=credentials.get("ELASTICSEARCH_PASSWORD")
else:
    print("You first need to add third-party credentials (ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD) for the database in the scheduler-portal.")
    sys.exit(1)
if variables.get("ELASTICSEARCH_QUERY"):
    # This is a workaround to force the variable string value into json
    exec("QUERY=json.loads(variables.get('ELASTICSEARCH_QUERY'))")
else:
    #print("ELASTICSEARCH_QUERY not defined by the user.")
    #sys.exit(1)
    print("ELASTICSEARCH_QUERY not defined by the user. Fetching all documents.")
    QUERY = { "query": { "match_all": {} } }



def connect(host, port, username, password):
    """ A util for making a connection to elasticsearch """

    # Connect to cluster over SSL using auth for best security:
    es_header = [{
        'host': host,
        'port': port,
        'use_ssl': True,
        'http_auth': (username,password)
    }]

    # Instantiate the new Elasticsearch connection:
    es = Elasticsearch(es_header)

    return es


def read_elasticsearch(index, query, search_size, host, port, username, password):

    # Connect to elasticsearch
    es = connect(host=host, port=port, username=username, password=password)
    
    
    # Make a query to the specific DB and Collection
    res = es.search(index=index, size=search_size, body=query)
    print("%d documents found" % res['hits']['total'])

    return res

print("EXECUTING QUERY...")
print('ELASTICSEARCH_HOSTNAME='+HOSTNAME)
print('ELASTICSEARCH_PORT=', PORT)
print('ELASTICSEARCH_INDEX='+INDEX)
print('ELASTICSEARCH_QUERY='+json.dumps(QUERY))
print('ELASTICSEARCH_SIZE=', SEARCH_SIZE)


query_results= read_elasticsearch(INDEX, QUERY, SEARCH_SIZE, HOSTNAME,  PORT,  ELASTICSEARCH_USERNAME,  ELASTICSEARCH_PASSWORD)
    
#**************Preview Data*********************
result = json.dumps(query_results).encode('utf-8')
resultMetadata.put("file.extension", ".json")
resultMetadata.put("file.name", "result.json")
resultMetadata.put("content.type", "application/json")
#***********************************************


print("END Import_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>
