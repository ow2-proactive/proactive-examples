<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.9"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.9 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.9/schedulerjob.xsd"
    name="postgres_connector" projectName="SQL Connectors"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="HOSTNAME" value="localhost" />
    <variable name="PORT" value="5432" />
    <variable name="DATABASE" value="" />
  </variables>
  <description>
    <![CDATA[ Load data from a PostgreSQL database. ]]>
  </description>
  <genericInformation>
    <info name="pca.action.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
  </genericInformation>
  <taskFlow>
    <task name="import_from_database">
      <description>
        <![CDATA[ Load data from an SQL database.
This is a GENERIC task that requires as an input the RMDB_NAME and RMDB_DRIVER that can be provided by one of the (sql_server, mysql, postgres, oracle) connector tasks.
It also requires an SQL query or a table name to fetch data from.
The imported data is in a Pandas DataFrame compatible JSON format. ]]>
      </description>
      <variables>
        <variable name="IS_LABELED_DATA" value="True" inherited="false" />
        <variable name="SQL_QUERY" value="" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/sql.png"/>
      </genericInformation>
      <depends>
        <task ref="postgres_connector"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Import_Data FROM " + variables.get("RDBM_NAME") + " database using " + variables.get("RDBM_DRIVER") + " connector")

import pandas as pd
import numpy as np
from sqlalchemy import create_engine

IS_LABELED_DATA = 'True'
try:
  IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
except NameError:
  pass

# Please refer to SQLAlchemy doc for more info about database urls.
# http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls
database_url = variables.get("RDBM_NAME")+'+'+variables.get("RDBM_DRIVER")+'://'+credentials.get("DATABASE_USERNAME")+':'+credentials.get("DATABASE_PASSWORD")+'@'+variables.get("HOSTNAME")+':'+variables.get("PORT")+'/'+variables.get("DATABASE")
engine = create_engine(database_url)

with engine.connect() as conn, conn.begin():
    #pd.read_sql() can take either a SQL query as a parameter or a table name
    dataframe = pd.read_sql(variables.get("SQL_QUERY"), conn)

columns_name = dataframe.columns
columns_number = len(columns_name)

if IS_LABELED_DATA == 'True':
  data  = dataframe.values[:,0:columns_number-1]
  label = dataframe.values[:,columns_number-1]
  
  data_df = pd.DataFrame(data=data,columns=columns_name[0:columns_number-1])
  label_df = pd.DataFrame(data=label,columns=[columns_name[columns_number-1]])
  
  DATAFRAME_JSON = dataframe.to_json(orient='split')
  COLUMNS_NAME_JSON = pd.Series(columns_name).to_json()
  DATA_TRAIN_DF_JSON = data_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_df.to_json(orient='split')
  LABEL_TRAIN_DF_JSON = label_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_df.to_json(orient='split')
  
  try:
    variables.put("DATAFRAME_JSON", DATAFRAME_JSON)
    variables.put("COLUMNS_NAME_JSON", COLUMNS_NAME_JSON)
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
    variables.put("IS_LABELED_DATA", IS_LABELED_DATA)
    
    #**************Preview Data*********************
    result = dataframe.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")
    #***********************************************
  except NameError:
    pass
  
  print("END Import_Data")
    
elif IS_LABELED_DATA == 'False':
  data = dataframe.values
  data_df = pd.DataFrame(data=data,columns=columns_name)
  
  DATAFRAME_JSON = dataframe.to_json(orient='split')
  COLUMNS_NAME_JSON = pd.Series(columns_name).to_json()
  DATA_TRAIN_DF_JSON = data_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_df.to_json(orient='split')
  
  try:
    variables.put("DATAFRAME_JSON", DATAFRAME_JSON)
    variables.put("COLUMNS_NAME_JSON", COLUMNS_NAME_JSON)
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("IS_LABELED_DATA", IS_LABELED_DATA)
    
    #**************Preview Data*********************
    result = dataframe.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")
    #***********************************************
  except NameError:
    pass
  
  print("END Import_Data")
else:
  print('The import data is failure')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </post>
    </task>
    <task name="postgres_connector">
      <description>
        <![CDATA[ This task specifies a driver to be used to connect to a PostgreSQL server. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
      </genericInformation>
      <pre>
        <script>
          <code language="bash">
            <![CDATA[

]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
variables.put("RDBM_NAME", "postgresql")
variables.put("RDBM_DRIVER", "psycopg2")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>