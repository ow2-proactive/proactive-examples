<?xml version="1.0" encoding="UTF-8"?>
<job
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="HDFS" tags="Big Data,Hadoop,HDFS,Service Automation,Analytics" projectName="Service Automation - Deployment" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="1"  >
  <variables>
    <variable model="PA:NOT_EMPTY_STRING" name="INSTANCE_NAME" value="hdfs-$PA_JOB_ID" description="The name of the service to be deployed." advanced="false" hidden="false"/>
    <variable model="PA:NOT_EMPTY_STRING" name="swarm_service_instance_id" value="xx" description="This variable must be set to the targeted Docker_Swarm service instance id." advanced="false" hidden="false"/>
    <variable model="PA:NOT_EMPTY_STRING" name="nb_hdfs_datanodes" value="3" description="Number of HDFS data nodes." advanced="false" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ Deploy a HDFS platform of nb_hdfs_datanodes datanodes.
A Swarm service (Docker_Swarm) needs to be started first, and the swarm_service_instance_id variable must be set to the service instance id of this targeted Docker Swarm. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="service-automation"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
    <info name="pca.states" value="(VOID,RUNNING)"/>
    <info name="Documentation" value="https://hadoop.apache.org/docs/r2.8.2/hadoop-project-dist/hadoop-common/ClusterSetup.html"/>
    <info name="pca.service.id" value="HDFS"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>

    <task name="retrieve_service_variables"
          fork="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Retrieve_variables_from_service_instance_id/raw" language="groovy">
            <arguments>
              <argument value="$swarm_service_instance_id"/>
              <argument value="swarm_token_name"/>
              <argument value="INSTANCE_NAME"/>
              <argument value="swarm_manager_and_workers_pa_node_names"/>
              <argument value="swarm_manager_and_workers_pa_node_names"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
    </task>

    <task name="create_overlay_network_and_start_hdfs_namenode" >
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$swarm_token_name"/>
      </genericInformation>
      <depends>
        <task ref="retrieve_service_variables"/>
      </depends>
      <selection>
        <script type="dynamic">
          <code language="groovy">
            <![CDATA[
// The overlay network must be created on the swarm manager host
def swarm_manager_pa_node_name = variables.get("swarm_manager_and_workers_pa_node_names").split(",")[0]

selected = (nodename == swarm_manager_pa_node_name)
]]>
          </code>
        </script>
      </selection>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
// Retrieve variables
def service_instance_id = variables.get("PCA_INSTANCE_ID") as Long
def instance_name = variables.get("INSTANCE_NAME")
def job_id = variables.get("PA_JOB_ID")
def pa_node_name = variables.get("PA_NODE_NAME")
def nb_hdfs_datanodes = variables.get("nb_hdfs_datanodes") as Integer

// Create the overlay network
def hdfs_network_name = instance_name + "-hdfs-network"
def cmd = ["docker", "network", "create", "-d", "overlay", "--attachable", hdfs_network_name]
println cmd
cmd.execute().waitForProcessOutput(System.out, System.err)

// Find a free port for spark_master_gui_port
def hdfs_gui_port = null
try {
  def server = new ServerSocket(0)
  hdfs_gui_port = server.getLocalPort()
  server.close()
} catch (IOException e) {
  throw new RuntimeException( "Failed to find free local port to bind the agent to", e);
}

// Start the hdfs namenode docker container
def hdfs_namenode_container_name = instance_name + "-hdfs-namenode"
cmd = ["docker", "run", "--rm", "-dit", "--publish", hdfs_gui_port + ":9870", "--name", hdfs_namenode_container_name, "--net", hdfs_network_name, "activeeon/hdfs-spark:latest"]
println cmd
def hdfs_namenode_container_id = new StringBuilder()
cmd.execute().waitForProcessOutput(hdfs_namenode_container_id, System.err)

// Namenode configuration + Start
def config_namenode_core_site_command = "sed s/IP:PORT/" + hdfs_namenode_container_name + ":9000/ \$HADOOP_HOME/etc/hadoop/core-site.xml.template > \$HADOOP_HOME/etc/hadoop/core-site.xml"
def format_namenode_command = "rm -r /tmp; hdfs namenode -format -force"
def start_hdfs_namenode_command = "\$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode"
def command_in_container = config_namenode_core_site_command + ";" + format_namenode_command + ";" + start_hdfs_namenode_command
cmd = ["docker", "exec", hdfs_namenode_container_name, "/bin/sh", "-c", command_in_container]
println cmd
cmd.execute().waitForProcessOutput(System.out, System.err)

// Ensure namenode is started
cmd = ["docker", "exec", hdfs_namenode_container_name, "jps"]
println cmd
def docker_exec_jps_output = new StringBuilder()
while (!docker_exec_jps_output.toString().contains("NameNode")){
	cmd.execute().waitForProcessOutput(docker_exec_jps_output, System.err)
	sleep(1000)
}

// Propagate variables
variables.put("hdfs_gui_port", hdfs_gui_port)
variables.put("hdfs_network_name", hdfs_network_name)
variables.put("hdfs_namenode_container_name", hdfs_namenode_container_name)
variables.put("nb_hdfs_deployments", (nb_hdfs_datanodes + 1))
variables.put("hdfs_namenode_and_datanodes_pa_node_names", pa_node_name)
resultMap.put("hdfs_service_instance_id", service_instance_id)

// Propagate the container id to the post script
new File(localspace, "hdfs_namenode_container_id").text = hdfs_namenode_container_id.toString()
new File(localspace, "hdfs_namenode_container_name").text = hdfs_namenode_container_name
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow >
        <replicate>
          <script>
            <code language="groovy">
              <![CDATA[
runs = variables.get("nb_hdfs_datanodes")
]]>
            </code>
          </script>
        </replicate>
      </controlFlow>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
import groovy.json.JsonOutput

// Retrieve the script arguments
def hdfs_namenode_container_id = new File(localspace, "hdfs_namenode_container_id").text
def hdfs_namenode_container_name = new File(localspace, "hdfs_namenode_container_name").text

// Retrieve variables
def job_id = variables.get("PA_JOB_ID")
def instance_name = variables.get("INSTANCE_NAME")
def pa_node_name = variables.get("PA_NODE_NAME")
def pa_node_host = variables.get("PA_NODE_HOST")
def pa_node_source_name = variables.get("PA_NODE_SOURCE")
def pa_node_url = variables.get("PA_NODE_URL")
def hdfs_gui_port = variables.get("hdfs_gui_port")
def pca_public_rest_url = variables.get('PA_CLOUD_AUTOMATION_REST_PUBLIC_URL')
def instance_id = variables.get("PCA_INSTANCE_ID")

// Build the endpoint url
def endpoint_url = "http://" + pa_node_host + ":" + hdfs_gui_port

// Build the proxified url
def endpoint_id = "hdfs-gui-" + job_id
def endpoint_proxyfied_url = pca_public_rest_url + "/services/" + instance_id + "/endpoints/" + endpoint_id + "/"

// Create the deployment map and json
def deployment_map = ["endpoint":["id":endpoint_id,"url":endpoint_url,"proxyfied_url":endpoint_proxyfied_url],"node":["name":pa_node_name,"host":pa_node_host,"node_source_name":pa_node_source_name,"url":pa_node_url],"container":["id":hdfs_namenode_container_id,"name":hdfs_namenode_container_name]]
def deployment_json = JsonOutput.toJson(deployment_map)

// Propagate the deployment map
variables.put("hdfs_deployment_json_0", deployment_json)
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
          334.6333312988281
        </positionTop>
        <positionLeft>
          596
        </positionLeft>
      </metadata>
    </task>
    <task name="start_hdfs_datanode" >
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$swarm_token_name"/>
      </genericInformation>
      <depends>
        <task ref="create_overlay_network_and_start_hdfs_namenode"/>
      </depends>
      <selection>
        <script type="dynamic">
          <code language="groovy">
            <![CDATA[
def task_replication_id = variables.get("PA_TASK_REPLICATION") as Integer
def swarm_manager_and_workers_pa_node_names = variables.get("swarm_manager_and_workers_pa_node_names")

def targeted_pa_node_name = swarm_manager_and_workers_pa_node_names.split(",")[task_replication_id]

selected = (nodename == targeted_pa_node_name)
]]>
          </code>
        </script>
      </selection>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
// Retrieve variables
def pa_job_id = variables.get("PA_JOB_ID")
def instance_name = variables.get("INSTANCE_NAME")
def task_replication_id = variables.get("PA_TASK_REPLICATION") as Integer
def hdfs_network_name = variables.get("hdfs_network_name")
def hdfs_namenode_container_name = variables.get("hdfs_namenode_container_name")
def pa_node_name = variables.get("PA_NODE_NAME")

// Start the hdfs datanode container
def hdfs_datanode_container_name = instance_name + "-hdfs-datanode-" + task_replication_id
def cmd = ["docker", "run", "--rm", "-dit", "--name", hdfs_datanode_container_name, "--net", hdfs_network_name, "activeeon/hdfs-spark:latest"]
println cmd
def hdfs_datanode_container_id = new StringBuilder()
cmd.execute().waitForProcessOutput(hdfs_datanode_container_id, System.err)

// HDFS datanode configuration + Start
def config_datanode_core_site_command = "sed s/IP:PORT/" + hdfs_namenode_container_name + ":9000/ \$HADOOP_HOME/etc/hadoop/core-site.xml.template > \$HADOOP_HOME/etc/hadoop/core-site.xml"
//def config_datanode_hdfs_site_command = "sed s/PORT/" + datanode_port + "/ \$HADOOP_HOME/etc/hadoop/hdfs-site.xml.template > \$HADOOP_HOME/etc/hadoop/hdfs-site.xml"
def clean_start_datanode_command = "rm -r /tmp; \$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode"
def command_in_container = config_datanode_core_site_command + /* ";" + config_datanode_hdfs_site_command +*/ ";" + clean_start_datanode_command
cmd = ["docker", "exec", hdfs_datanode_container_name, "/bin/sh", "-c", command_in_container]
println cmd
cmd.execute().waitForProcessOutput(System.out, System.err)

// Propagate variables
variables.put("hdfs_datanode_" + task_replication_id + "_pa_node_name", pa_node_name)

// Propagate the container id to the post script
new File(localspace, "hdfs_datanode_container_id").text = hdfs_datanode_container_id.toString()
new File(localspace, "hdfs_datanode_container_name").text = hdfs_datanode_container_name
]]>
          </code>
        </script>
      </scriptExecutable>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
import groovy.json.JsonOutput

// Retrieve the script arguments
def hdfs_datanode_container_id = new File(localspace, "hdfs_datanode_container_id").text
def hdfs_datanode_container_name = new File(localspace, "hdfs_datanode_container_name").text

// Retrieve variables
def job_id = variables.get("PA_JOB_ID")
def task_replication_id = variables.get("PA_TASK_REPLICATION")
def instance_name = variables.get("INSTANCE_NAME")
def pa_node_name = variables.get("PA_NODE_NAME")
def pa_node_host = variables.get("PA_NODE_HOST")
def pa_node_source_name = variables.get("PA_NODE_SOURCE")
def pa_node_url = variables.get("PA_NODE_URL")

// Create the deployment map and json
def endpoint_id = "hdfs-" + job_id + "-" + task_replication_id
def deployment_map = ["endpoint":["id":endpoint_id],"node":["name":pa_node_name,"host":pa_node_host,"node_source_name":pa_node_source_name,"url":pa_node_url],"container":["id":hdfs_datanode_container_id,"name":hdfs_datanode_container_name]]
def deployment_json = JsonOutput.toJson(deployment_map)

// Propagate the deployment map
variables.put("hdfs_deployment_json_" + ((task_replication_id as Integer) + 1), deployment_json)

// Add token to the current node (RM API)
rmapi.connect()
println "Adding token " + instance_name + " to node " + pa_node_url
rmapi.addNodeToken(pa_node_url, instance_name)
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
          462.6333312988281
        </positionTop>
        <positionLeft>
          596
        </positionLeft>
      </metadata>
    </task>

    <task name="merge_json_deployments_and_propagate"




          fork="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$INSTANCE_NAME"/>
      </genericInformation>
      <depends>
        <task ref="start_hdfs_datanode"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
// Retrieve variables
def nb_hdfs_datanodes = variables.get("nb_hdfs_datanodes") as Integer
def hdfs_namenode_and_datanodes_pa_node_names = variables.get("hdfs_namenode_and_datanodes_pa_node_names")

for (i = 0; i < nb_hdfs_datanodes; i++) {
    hdfs_namenode_and_datanodes_pa_node_names += "," + variables.get("hdfs_datanode_" + i + "_pa_node_name")
}

variables.put("hdfs_namenode_and_datanodes_pa_node_names", hdfs_namenode_and_datanodes_pa_node_names)
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
          542.5
        </positionTop>
        <positionLeft>
          668
        </positionLeft>
      </metadata>
    </task>

    <task name="propagate_variables_and_update_deployments_with_service"




          fork="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$INSTANCE_NAME"/>
      </genericInformation>
      <depends>
        <task ref="merge_json_deployments_and_propagate"/>
      </depends>
      <pre>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Propagate_variables_to_current_service/raw" language="groovy">
            <arguments>
              <argument value="hdfs_network_name"/>
              <argument value="VARIABLE_VALUE"/>
              <argument value="hdfs_namenode_host_port"/>
              <argument value="$hdfs_namenode_container_name:9000"/>
              <argument value="hdfs_namenode_and_datanodes_pa_node_names"/>
              <argument value="VARIABLE_VALUE"/>
            </arguments>
          </file>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Add_deployments_and_update_service/raw" language="groovy">
            <arguments>
              <argument value="hdfs_deployment_json_"/>
              <argument value="$nb_hdfs_deployments"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <cleaning>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Clean_Start_Service/raw" language="groovy"></file>
        </script>
      </cleaning>
    </task>

    <task name="loop_over_hdfs_namenode_status"




          fork="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$INSTANCE_NAME"/>
      </genericInformation>
      <depends>
        <task ref="propagate_variables_and_update_deployments_with_service"/>
      </depends>
      <selection>
        <script type="dynamic">
          <code language="groovy">
            <![CDATA[
def hdfs_namenode_and_datanodes_pa_node_names = variables.get("hdfs_namenode_and_datanodes_pa_node_names")

def hdfs_namenode_pa_node_name = hdfs_namenode_and_datanodes_pa_node_names.split(",")[0]

selected = (nodename == hdfs_namenode_pa_node_name)
]]>
          </code>
        </script>
      </selection>
      <pre>
        <script>
          <code language="groovy">
            <![CDATA[
// Retrieve variables
def hdfs_namenode_container_name = variables.get("hdfs_namenode_container_name")
def instance_name = variables.get("INSTANCE_NAME")

// Ensure namenode is running
def cmd = ["docker", "exec", hdfs_namenode_container_name, "jps"]
println cmd
def docker_exec_jps_output = new StringBuilder()
cmd.execute().waitForProcessOutput(docker_exec_jps_output, System.err)

def is_hdfs_namenode_ok = docker_exec_jps_output.toString().contains("NameNode")
println "DEBUG is_hdfs_namenode_ok " + is_hdfs_namenode_ok
def is_docker_based_service = true
def token_to_remove = instance_name

// Propagate to the current task script
new File(localspace, "arguments.txt").text = String.valueOf(is_hdfs_namenode_ok) + "," + String.valueOf(is_docker_based_service) + "," + token_to_remove
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Loop_over_service_instance_status/raw" language="groovy"></file>
        </script>
      </scriptExecutable>
      <controlFlow >
        <loop target="loop_over_hdfs_namenode_status">
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Fetch_Logs/raw" language="groovy"></file>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
          798.5
        </positionTop>
        <positionLeft>
          668
        </positionLeft>
      </metadata>
    </task>

  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2735px;
            height:2924px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-201.63333129882812px;left:-591px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" style="top: 206.633px; left: 596px;" id="jsPlumb_1_10"><a class="task-name"><img src="/studio/images/Groovy.png" width="20px">&nbsp;<span class="name">get_resources_from_service_instance</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" style="top: 334.633px; left: 596px;" id="jsPlumb_1_13"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">start_spark_master</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" style="top: 462.633px; left: 596px;" id="jsPlumb_1_16"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">start_spark_slave</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" style="top: 590.633px; left: 596px;" id="jsPlumb_1_19"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">update_service_instance</span></a></div><svg style="position:absolute;left:651.5px;top:246.5px" width="67" height="89" pointer-events="none" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 56 50 46 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M1.6926389999999976,64.9032055 L18.66180608105043,52.21276151346453 L9.504895730542636,53.285604853735244 L7.044205434785677,44.400504782921885 L1.6926389999999976,64.9032055" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M1.6926389999999976,64.9032055 L18.66180608105043,52.21276151346453 L9.504895730542636,53.285604853735244 L7.044205434785677,44.400504782921885 L1.6926389999999976,64.9032055" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:683.9px;top:364.5px" width="28.200000000000045" height="99" pointer-events="none" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 88 17.200000000000045 -10 7.2000000000000455 0 " transform="translate(10.5,10.5)" pointer-events="visibleStroke" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#e5db3d" style=""></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M-2.1009752999999947,77.41936575 L7.046180412470953,58.30577000320734 L-0.568522702436107,63.50349031809778 L-6.869695019431264,56.77331740564345 L-2.1009752999999947,77.41936575" class="" stroke="rgba(229,219,61,0.5)" fill="rgba(229,219,61,0.5)" transform="translate(10.5,10.5)"></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M-2.1009752999999947,77.41936575 L7.046180412470953,58.30577000320734 L-0.568522702436107,63.50349031809778 L-6.869695019431264,56.77331740564345 L-2.1009752999999947,77.41936575" class="" stroke="rgba(229,219,61,0.5)" fill="rgba(229,219,61,0.5)" transform="translate(10.5,10.5)"></path></svg><div style="position: absolute; transform: translate(-50%, -50%); left: 697.5px; top: 414.75px;" class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_30">replicate</div><svg style="position:absolute;left:647px;top:374.5px" width="25.5" height="89" pointer-events="none" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 14.5 50 4.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M-2.4501093750000003,66.78168750000002 L6.253690937044999,47.46216731630898 L-1.2390824053543916,52.834163932040326 L-7.69383263091469,46.25114034666338 L-2.4501093750000003,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M-2.4501093750000003,66.78168750000002 L6.253690937044999,47.46216731630898 L-1.2390824053543916,52.834163932040326 L-7.69383263091469,46.25114034666338 L-2.4501093750000003,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:647px;top:502.5px" width="38.5" height="89" pointer-events="none" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 17.5 88 C 27.5 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M18.83704,66.303232 L20.538982678279996,45.182072204196906 L15.288421492048318,52.76043662072092 L6.996187299000918,48.73069071214858 L18.83704,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" d="M18.83704,66.303232 L20.538982678279996,45.182072204196906 L15.288421492048318,52.76043662072092 L6.996187299000918,48.73069071214858 L18.83704,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div style="position: absolute; height: 20px; width: 20px; left: 698px; top: 237px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 652px; top: 365px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 652px; top: 325px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 691.6px; top: 365px;" class="_jsPlumb_endpoint source-endpoint replicate-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#e5db3d" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 647.5px; top: 493px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 684.4px; top: 453px;" class="_jsPlumb_endpoint target-endpoint replicate-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#e5db3d" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 647.5px; top: 453px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 665px; top: 621px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 665px; top: 581px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
      xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
    </visualization>
  </metadata>
</job>