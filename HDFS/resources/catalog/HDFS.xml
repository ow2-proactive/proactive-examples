<?xml version="1.0" encoding="UTF-8"?>
<job
		xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
		xmlns="urn:proactive:jobdescriptor:3.10"
		xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
		name="HDFS" projectName="Cloud Automation - Deployment"
		priority="normal"
		onTaskError="continueJobExecution"
		maxNumberOfExecution="2"
>
	<variables>
		<variable name="HDFS_UI_port" value="6500" />
		<variable name="datanode_starting_port" value="50010" />
		<variable name="fs_name" value="25.25.25.2" />
		<variable name="instance_name" value="my-instance" />
		<variable name="network_name" value="my-net" />
	</variables>
	<description>
		<![CDATA[ Deployment of HDFS. ]]>
	</description>
	<genericInformation>
		<info name="Documentation" value="http://activeeon.com/resources/activeeon-deploy-swarm-hdfs-spark.pdf"/>
		<info name="group" value="public-objects"/>
		<info name="pca.service.id" value="HDFS-Spark"/>
		<info name="pca.states" value="(SWARM_DEPLOYED,HDFS_DEPLOYED)"/>
		<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
	</genericInformation>
	<taskFlow>
		<task name="get_resources_from_service_instance">
			<inputFiles>
				<files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
			</inputFiles>
			<forkEnvironment >
				<additionalClasspath>
					<pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
				</additionalClasspath>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="groovy">
						<![CDATA[
import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData

// Retrieve variables
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def service_instance_id = variables.get("PCA_INSTANCE_ID") as Long

// Define other variables
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Get the targeted resources the PCA service instance
def service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
def node_source_name = service_instance_data.getVariables().get("node_source_name")
def nb_nodes = service_instance_data.getVariables().get("nb_nodes")
variables.put("node_source_name", node_source_name)
variables.put("nb_nodes", nb_nodes)
]]>
					</code>
				</script>
			</scriptExecutable>
		</task>
		<task name="start_namenode">
			<genericInformation>
				<info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
			</genericInformation>
			<depends>
				<task ref="get_resources_from_service_instance"/>
			</depends>
			<selection>
				<script>
					<code language="groovy">
						<![CDATA[
if (variables.get("node_source_name").equals(System.getProperty("proactive.node.nodesource"))) {
    selected = true
} else {
   selected = false
}
]]>
					</code>
				</script>
			</selection>
			<scriptExecutable>
				<script>
					<code language="groovy">
						<![CDATA[
import org.objectweb.proactive.core.util.ProActiveInet

println "run_namenode_in_container ..."

// Retrieve variables
def HDFS_UI_port = variables.get("HDFS_UI_port")
def instance_name = variables.get("instance_name")
def network_name = variables.get("network_name")
def fs_name = variables.get("fs_name")

def namenode_container_name = instance_name + "-namenode"

// Get the current IP address and store it
def HDFS_UI_IP_address = ProActiveInet.getInstance().getInetAddress().getHostAddress()
variables.put("HDFS_UI_IP_address", HDFS_UI_IP_address)

// Start the namenode docker container
def cmd = ["docker", "run", "-dit", "--publish", HDFS_UI_port + ":9870", "--name", namenode_container_name, "--net", network_name, "activeeon/hdfs-spark:1.0"]
cmd.execute().waitForProcessOutput(System.out, System.err)

// Namenode configuration
def docker_config_command = "sed s/IP:PORT/" + fs_name + ":9000/ \$HADOOP_HOME/etc/hadoop/core-site.xml.template > \$HADOOP_HOME/etc/hadoop/core-site.xml; rm -r /tmp; hdfs namenode -format -force"
cmd = ["docker", "exec", namenode_container_name, "/bin/sh", "-c", docker_config_command]
cmd.execute().waitForProcessOutput(System.out, System.err)

// Start namenode
def docker_start_command = "hdfs --daemon start namenode"
cmd = ["docker", "exec", namenode_container_name, "/bin/sh", "-c", docker_start_command]
cmd.execute().waitForProcessOutput(System.out, System.err)

println "... run_namenode_in_container"
]]>
					</code>
				</script>
			</scriptExecutable>
			<controlFlow >
				<replicate>
					<script>
						<code language="groovy">
							<![CDATA[
runs=variables.get("nb_nodes")
]]>
						</code>
					</script>
				</replicate>
			</controlFlow>
			<post>
				<script>
					<code language="groovy">
						<![CDATA[
import groovy.json.JsonSlurper

def namenode_container_name = variables.get("instance_name") + "-namenode"
def network_name = variables.get("network_name")

// Retrieve the namenode IP address in the overlay network
def jsonToParse = new StringBuilder()
["docker","inspect", namenode_container_name].execute().waitForProcessOutput(jsonToParse , System.err)
def jsonSlurper = new JsonSlurper()
def object = jsonSlurper.parseText(jsonToParse+"")
def namenode_IP_address = object.get(0).get("NetworkSettings").get("Networks").get(network_name).get("IPAddress")

// And store it
variables.put("namenode_IP_address", namenode_IP_address)
]]>
					</code>
				</script>
			</post>
		</task>
		<task name="start_datanode">
			<genericInformation>
				<info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
			</genericInformation>
			<depends>
				<task ref="start_namenode"/>
			</depends>
			<selection>
				<script>
					<code language="groovy">
						<![CDATA[
if (variables.get("node_source_name").equals(System.getProperty("proactive.node.nodesource"))) {
    selected = true
} else {
   selected = false
}
]]>
					</code>
				</script>
			</selection>
			<scriptExecutable>
				<script>
					<code language="groovy">
						<![CDATA[
println "start_datanode ..."

// Retrieve variables
def fs_name = variables.get("fs_name")
def task_id = variables.get("PA_TASK_REPLICATION") as Integer
def datanode_starting_port = variables.get("datanode_starting_port") as Integer
def network_name = variables.get("network_name")

def datanode_container_name = variables.get("instance_name") + "-datanode-" + task_id
def datanode_port = datanode_starting_port + task_id

// Start the datanode docker container
def cmd = ["docker", "run", "-dit", "--name", datanode_container_name, "--net", network_name, "activeeon/hdfs-spark:1.0"]
cmd.execute().waitForProcessOutput(System.out, System.err)

// Datanode configuration + Start
def docker_config_command = "sed s/IP:PORT/" + fs_name + ":9000/ \$HADOOP_HOME/etc/hadoop/core-site.xml.template > \$HADOOP_HOME/etc/hadoop/core-site.xml; sed s/PORT/" + datanode_port + "/ \$HADOOP_HOME/etc/hadoop/hdfs-site.xml.template > \$HADOOP_HOME/etc/hadoop/hdfs-site.xml; rm -r /tmp; hdfs --daemon start datanode"
cmd = ["docker", "exec", datanode_container_name, "/bin/sh", "-c", docker_config_command]
cmd.execute().waitForProcessOutput(System.out, System.err)

println "... start_datanode"
]]>
					</code>
				</script>
			</scriptExecutable>
		</task>
		<task name="update_service_instance">
			<genericInformation>
				<info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png"/>
			</genericInformation>
			<depends>
				<task ref="start_datanode"/>
			</depends>
			<inputFiles>
				<files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
			</inputFiles>
			<forkEnvironment >
				<additionalClasspath>
					<pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
				</additionalClasspath>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="groovy">
						<![CDATA[
import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData

// Retrieve variables
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def service_instance_id = variables.get("PCA_INSTANCE_ID") as Long
def namenode_IP_address = variables.get("namenode_IP_address")
def HDFS_UI_IP_address = variables.get("HDFS_UI_IP_address")
def HDFS_UI_port = variables.get("HDFS_UI_port")

// Define other variables
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Update the related service instance
def service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
service_instance_data.setInstanceStatus("HDFS_DEPLOYED")
service_instance_data.getInstanceEndpoints().put("HDFS_UI", "http://" + HDFS_UI_IP_address + ":" + HDFS_UI_port)
service_instance_data.getVariables().put("namenode_IP_address", namenode_IP_address)
service_instance_rest_api.updateServiceInstanceUsingPUT(service_instance_id, service_instance_data)
]]>
					</code>
				</script>
			</scriptExecutable>
		</task>
	</taskFlow>
</job>