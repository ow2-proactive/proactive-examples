<?xml version="1.0" encoding="UTF-8"?>
<job xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="urn:proactive:jobdescriptor:3.8"
	xsi:schemaLocation="urn:proactive:jobdescriptor:3.8 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.8/schedulerjob.xsd"
	name="HDFS" projectName="Cloud Automation - Deployment" priority="normal"
	onTaskError="continueJobExecution" maxNumberOfExecution="2">
	<variables>
		<variable name="instance_name" value="hdfsContainer" />
		<variable name="dashboard_port" value="6000" />
		<variable name="datanode_starting_port" value="50010" />
		<variable name="fs_name" value="25.25.25.2" />
		<variable name="network" value="my-net" />
		<variable name="hdfs_portal_host_name" value="https://try.activeeon.com" />
	</variables>
	<description>
    	<![CDATA[ Deployment of HDFS. ]]>
    </description>
	<genericInformation>
	    <info name="pca.service.id" value="HDFS-Spark" />
		<info name="pca.states" value="(SWARM_DEPLOYED,HDFS_DEPLOYED)"/>
	    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" />
	    <info name="Documentation" value="http://activeeon.com/resources/activeeon-deploy-swarm-hdfs-spark.pdf" />
	    <info name="group" value="public-objects" />
	</genericInformation>
	<taskFlow>
		<task name="get_IP_addresses_from_pca">
			<inputFiles>
				<files  includes="cloud-automation-service-client-8.1.0.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="logging-interceptor-2.7.5.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="okhttp-2.7.5.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="okio-1.6.0.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="gson-2.6.2.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="spring-web-4.2.5.RELEASE.jar" accessMode="transferFromGlobalSpace"/>
			</inputFiles>
			<forkEnvironment >
				<additionalClasspath>
					<pathElement path="cloud-automation-service-client-8.1.0.jar"/>
					<pathElement path="logging-interceptor-2.7.5.jar"/>
					<pathElement path="okhttp-2.7.5.jar"/>
					<pathElement path="okio-1.6.0.jar"/>
					<pathElement path="gson-2.6.2.jar"/>
					<pathElement path="spring-web-4.2.5.RELEASE.jar"/>
				</additionalClasspath>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="groovy">
						<![CDATA[
import io.swagger.client.ApiClient
import io.swagger.client.api.ServiceInstanceRestApi
import io.swagger.client.model.ServiceInstanceData

// Retrieve variables
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def service_instance_id = variables.get("PCA_INSTANCE_ID") as Long

// Define other variables
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Get IP addresses from the PCA service instance
// namenode ---
def service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
def service_instance_end_points = service_instance_data.getInstanceEndpoints()
def node_0_IP_address = service_instance_end_points.get("node_0_IP_address")
variables.put("namenode_IP_address", node_0_IP_address)

// datanodes ---
def datanode_IP_addresses = service_instance_end_points.findAll { it.key =~ /node_\d+_IP_address/ }.values()
variables.put("datanode_IP_addresses", datanode_IP_addresses.join("\n"))
]]>
					</code>
				</script>
			</scriptExecutable>
		</task>
		<task name="run_namenode_in_container">
			<genericInformation>
				<info name="task.icon"
					value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" />
			</genericInformation>
			<depends>
				<task ref="get_IP_addresses_from_pca"/>
			</depends>
			<selection>
				<script>
					<code language="javascript">
            <![CDATA[
if (org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkIp(variables.get('namenode_IP_address'))) {
    selected = true;
} else {
   selected = false;
}
]]>
					</code>
				</script>
			</selection>
			<scriptExecutable>
				<script>
					<code language="bash">
            <![CDATA[
echo "run_namenode_in_container ..."

namenode_container_name=$variables_instance_name"Namenode"

docker run -dit --publish=$variables_dashboard_port:9870 --name=$namenode_container_name --hostname=$namenode_container_name --net=$variables_network activeeon/hdfs-spark:1.0
docker exec $namenode_container_name /bin/sh -c 'sed s/IP:PORT/'$variables_fs_name':9000/ $HADOOP_HOME/etc/hadoop/core-site.xml.template > $HADOOP_HOME/etc/hadoop/core-site.xml; rm -r /tmp; hdfs namenode -format -force'
docker exec $namenode_container_name /bin/sh -c 'hdfs --daemon start namenode'

echo "... run_namenode_in_container"
]]>
					</code>
				</script>
			</scriptExecutable>
			<controlFlow block="none"></controlFlow>
		</task>
		<task name="process_datanode_IP_addresses">
			<genericInformation>
				<info name="task.icon"
					value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" />
			</genericInformation>
			<depends>
				<task ref="run_namenode_in_container" />
			</depends>
			<scriptExecutable>
				<script>
					<code language="groovy">
            <![CDATA[
println "process_datanode_IP_addresses ..."

// Retrieve the datanode IP addresses
def datanode_IP_addresses = variables.get("datanode_IP_addresses")

// 1 IP address per replicated task
String[] datanode_IP_addresses_array = datanode_IP_addresses.split("\n")

// Store the datanode number
def nb_datanodes = datanode_IP_addresses_array.length
variables["nb_datanodes"] = nb_datanodes

// Store IP addresses in variables for the replicated task selection scripts
for (int i = 0; i < nb_datanodes; i++)
{
   variables["IP_address_"+i] = datanode_IP_addresses_array[i]
   println "variable " + variables["IP_address_"+i]
}

println "... process_datanode_IP_addresses"
]]>
					</code>
				</script>
			</scriptExecutable>
			<controlFlow>
				<replicate>
					<script>
						<code language="groovy">
              <![CDATA[
runs=variables.get("nb_datanodes")
]]>
						</code>
					</script>
				</replicate>
			</controlFlow>
		</task>
		<task name="run_datanode_in_container">
			<genericInformation>
				<info name="task.icon"
					value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" />
			</genericInformation>
			<depends>
				<task ref="process_datanode_IP_addresses" />
			</depends>
			<selection>
				<script>
					<code language="javascript">
            <![CDATA[
var task_id = variables.get('PA_TASK_REPLICATION');
var datanode_IP_address = variables.get('IP_address_' + task_id);

if (org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkIp(datanode_IP_address)) {
    print("TASK " + task_id + " on " + datanode_IP_address);
    selected = true;
} else {
   selected = false;
}
]]>
					</code>
				</script>
			</selection>
			<scriptExecutable>
				<script>
					<code language="bash">
            <![CDATA[
echo "run_datanode_in_container ..."

# Retrieve variables
task_id=$variables_PA_TASK_REPLICATION
datanode_container_name=$variables_instance_name"Datanode"$task_id
datanode_port=$(expr $variables_datanode_starting_port + $task_id)

echo "file_system_port "$file_system_port
echo "datanode_port "$datanode_port
echo "DATANODE STARTING IN "$datanode_container_name" ..."

docker run -dit --name=$datanode_container_name --hostname=$datanode_container_name --net=$variables_network activeeon/hdfs-spark:1.0
docker exec $datanode_container_name /bin/sh -c 'sed s/IP:PORT/'$variables_fs_name':9000/ $HADOOP_HOME/etc/hadoop/core-site.xml.template > $HADOOP_HOME/etc/hadoop/core-site.xml; sed s/PORT/'$datanode_port'/ $HADOOP_HOME/etc/hadoop/hdfs-site.xml.template > $HADOOP_HOME/etc/hadoop/hdfs-site.xml; rm -r /tmp; hdfs --daemon start datanode'

echo "... run_datanode_in_container"
]]>
					</code>
				</script>
			</scriptExecutable>
		</task>
		<task name="Update_service_state">
			<genericInformation>
				<info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/hdfs.png" />
			</genericInformation>
			<depends>
				<task ref="run_datanode_in_container" />
			</depends>
			<inputFiles>
				<files  includes="cloud-automation-service-client-8.1.0.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="logging-interceptor-2.7.5.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="okhttp-2.7.5.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="okio-1.6.0.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="gson-2.6.2.jar" accessMode="transferFromGlobalSpace"/>
				<files  includes="spring-web-4.2.5.RELEASE.jar" accessMode="transferFromGlobalSpace"/>
			</inputFiles>
			<forkEnvironment >
				<additionalClasspath>
					<pathElement path="cloud-automation-service-client-8.1.0.jar"/>
					<pathElement path="logging-interceptor-2.7.5.jar"/>
					<pathElement path="okhttp-2.7.5.jar"/>
					<pathElement path="okio-1.6.0.jar"/>
					<pathElement path="gson-2.6.2.jar"/>
					<pathElement path="spring-web-4.2.5.RELEASE.jar"/>
				</additionalClasspath>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="groovy">
						<![CDATA[
import io.swagger.client.ApiClient
import io.swagger.client.api.ServiceInstanceRestApi
import io.swagger.client.model.ServiceInstanceData

// Retrieve variables
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def service_instance_id = variables.get("PCA_INSTANCE_ID") as Long
def hdfs_portal_host_name = variables.get("hdfs_portal_host_name")
def dashboard_port = variables.get("dashboard_port")

// Define other variables
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Update the related service instance
def service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
service_instance_data.setInstanceStatus("HDFS_DEPLOYED")
service_instance_data.getInstanceEndpoints().put("hdfs_portal", hdfs_portal_host_name + ":" + dashboard_port)
service_instance_rest_api.updateServiceInstanceUsingPUT(service_instance_id, service_instance_data)
]]>
					</code>
				</script>
			</scriptExecutable>
		</task>
	</taskFlow>
</job>
