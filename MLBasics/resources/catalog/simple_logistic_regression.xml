<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.8"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.8 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.8/schedulerjob.xsd"
    name="simple_logistic_regression" projectName="Basic Machine Learning"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <description>
    <![CDATA[ Load a Iris dataset for classification, then train a model using the Logistic Regression algorithm, next it generates the predictions. You can see the predictions results in the export data task, and download the trained model. ]]>
  </description>
    <genericInformation>
    <info name="bucketName" value="machine-learning-workflows-tmp"/>
    <info name="pca.action.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/machine_learning.png"/>
    <info name="Documentation" value="http://activeeon.com/resources/automated-machine-learning-activeeon.pdf"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Split_Data">
      <description>
        <![CDATA[ Divide the data into two sets. ]]>
      </description>
      <variables>
        <variable name="TRAIN_SIZE" value="0.7" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/split_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Load_Iris_Dataset"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Split_Data")

from sklearn import model_selection
import pandas as pd

TRAIN_SIZE = 0.7
try:
  IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
  TRAIN_SIZE = float(variables.get("TRAIN_SIZE"))
except NameError:
  pass
test_size = 1 - TRAIN_SIZE

if IS_LABELED_DATA == 'True':
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
    
  data = dataframe.values[:,0:columns_number-1]
  label = dataframe.values[:,columns_number-1]
  indice = dataframe.index.values
  
  data_train, data_test, label_train, label_test, idx_train, idx_test = model_selection.train_test_split(data, label, indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name[0:columns_number-1],index=idx_train)
  label_train_df = pd.DataFrame(data=label_train,columns=[columns_name[columns_number-1]])
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name[0:columns_number-1],index=idx_test)
  label_test_df = pd.DataFrame(data=label_test,columns=[columns_name[columns_number-1]])
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  LABEL_TRAIN_DF_JSON = label_train_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
  except NameError:
    pass
  
  print("END Split_Data")
  
elif IS_LABELED_DATA == 'False' or IS_LABELED_DATA == None:
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
  indice = dataframe.index.values
  data = dataframe.values
  
  data_train, data_test, idx_train, idx_test = model_selection.train_test_split(data,indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name,index=idx_train)
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name,index=idx_test)
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
  except NameError:
    pass
  
  print("END Split_Data")
else:
  print('The data could not be split, please check ypur ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Logistic_Regression">
      <description>
        <![CDATA[ Logistic Regression is a regression model where the Dependent Variable (DV) is categorical. ]]>
      </description>
      <variables>
        <variable name="PENALTY" value="l2" inherited="false" />
        <variable name="SOLVER" value="liblinear" inherited="false" />
        <variable name="MAX_ITERATIONS" value="100" inherited="false" />
        <variable name="N_JOBS" value="1" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/ml_classification.png"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
PENALTY = variables.get("PENALTY")
SOLVER = variables.get("SOLVER")
MAX_ITERATIONS = variables.get("MAX_ITERATIONS")
N_JOBS = variables.get("N_JOBS")

variables.put("PENALTY_PARA", PENALTY)
variables.put("SOLVER_PARA", SOLVER)
variables.put("MAX_ITERATIONS_PARA", MAX_ITERATIONS)
variables.put("N_JOBS_PARA", N_JOBS)
variables.put("ALGORITHM_NAME", "LogisticRegression")
variables.put("SUPERVISED_ALGORITHM", "True")
variables.put("CLASSIFICATION_MEASURE", "True")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Train_Model">
      <description>
        <![CDATA[ Train a model using a classification or regression algorithm. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/train.png"/>
      </genericInformation>
      <depends>
        <task ref="Split_Data"/>
        <task ref="Logistic_Regression"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Train_Model")

import pandas as pd
import random
import pickle

IS_SUPERVIDED_ALGORITHM = variables.get("SUPERVISED_ALGORITHM")
LABEL_TRAIN_DF_JSON = variables.get("LABEL_TRAIN_DF_JSON")
DATA_TRAIN_DF_JSON  = variables.get("DATA_TRAIN_DF_JSON")

if IS_SUPERVIDED_ALGORITHM == 'True' and LABEL_TRAIN_DF_JSON != None and DATA_TRAIN_DF_JSON != None:
  ALGORITHM_NAME = variables.get("ALGORITHM_NAME")
  data_train_df = pd.read_json(DATA_TRAIN_DF_JSON, orient='split')
  label_train_df = pd.read_json(LABEL_TRAIN_DF_JSON, orient='split')
  model = None
  
  # CLASSIFICATION LEARNING
  if ALGORITHM_NAME == 'SupportVectorMachines':
    from sklearn.svm import SVC
    c_para = variables.get("C_PARA")
    kernel_para = variables.get("KERNEL_PARA")
    model = SVC(C=c_para, kernel=kernel_para)
   
  if ALGORITHM_NAME == 'GaussianNaiveBayes':
    from sklearn.naive_bayes import GaussianNB
    model = GaussianNB()
  
  if ALGORITHM_NAME == 'LogisticRegression':
    from sklearn.linear_model import LogisticRegression
    penalty_para = variables.get("PENALTY_PARA")
    solver_para = variables.get("SOLVER_PARA") 
    max_iter_para = variables.get("MAX_ITERATIONS_PARA")
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = LogisticRegression(penalty=penalty_para, solver=solver_para, max_iter = int(max_iter_para), n_jobs = int(n_jobs_para))
  
  # REGRESSION LEARNING
  if ALGORITHM_NAME == 'LinearRegression':
    from sklearn.linear_model import LinearRegression
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = LinearRegression(n_jobs = int(n_jobs_para))

  if ALGORITHM_NAME == 'SupportVectorRegression':
    from sklearn.svm import SVR
    c_para = variables.get("C_PARA")
    kernel_para = variables.get("KERNEL_PARA")
    epsilon_para = variables.get("EPSILON_PARA")
    model = SVR(C=c_para, kernel=kernel_para, epsilon=epsilon_para) 
  
  if ALGORITHM_NAME == 'BayesianRidgeRegression':
    from sklearn import linear_model
    n_iter_para = variables.get("N_ITERATIONS_PARA")
    alpha1_para = variables.get("ALPHA1_PARA")
    alpha2_para = variables.get("ALPHA2_PARA")
    lambda1_para = variables.get("LAMBDA1_PARA")
    lambda2_para = variables.get("LAMBDA2_PARA")
    model = linear_model.BayesianRidge(alpha_1=alpha1_para, alpha_2=alpha2_para, lambda_1=lambda1_para, lambda_2=lambda2_para, n_iter= int(n_iter_para))
  
  model.fit(data_train_df.values, label_train_df.values.ravel())
  model_bin = pickle.dumps(model)
  variables.put("MODEL", model_bin)
  
  print("END Train_Model")

else:
  print('Please check your ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Predict_Model">
      <description>
        <![CDATA[ Generate predictions using a trained model. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/predict.png"/>
      </genericInformation>
      <depends>
        <task ref="Train_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Predict_Model")

import os
import pickle
import pandas as pd

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score

MODEL_BIN = variables.get("MODEL")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")

if MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")

  data_test_df = pd.read_json(DATA_TEST_DF_JSON, orient='split')
  label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  score = loaded_model.score(data_test_df.values, label_test_df.values.ravel())
  predict_data = list(loaded_model.predict(data_test_df.values))
  predict_data_df = pd.DataFrame(predict_data)

  # CLASSIFICATION MEASURES
  try: 
    is_classification_algorithm = variables.get("CLASSIFICATION_MEASURE")
    if is_classification_algorithm == 'True':
      print("**********************CLASSIFICATION MEASURES**********************")
      accuracy_score_result = accuracy_score(label_test_df.values.ravel(), predict_data)
      precision_score_result = precision_score(label_test_df.values.ravel(), predict_data, average='micro')
      confusion_matrix_result = confusion_matrix(label_test_df.values.ravel(), predict_data)
      print("ACCURACY SCORE: %.2f" % accuracy_score_result)
      print("PRECISION SCORE: %.2f" % precision_score_result)
      print("CONFUSION MATRIX:\n%s" % confusion_matrix_result)
      print("*********************************************************************************")
      
  except NameError:
    classification_algorithm = None
    print("SCORE: %.2f" % score)

  # REGRESSION MEASURES
  try:
    is_regression_algorithm = variables.get("REGRESSION_MEASURE")
    if is_regression_algorithm == 'True':
      mean_squared_error_result = mean_squared_error(label_test_df.values.ravel(), predict_data)
      mean_absolute_error_result = mean_absolute_error(label_test_df.values.ravel(), predict_data)
      mean_absolute_error_result = mean_absolute_error(label_test_df.values.ravel(), predict_data)
      r2_score_result = r2_score(label_test_df.values.ravel(), predict_data) 
      print("**********************REGRESSION MEASURES**********************")
      print("MEAN SQUARED ERROR: %.2f" % mean_squared_error_result)
      print("MEAN ABSOLUTE ERROR: %.2f" % mean_absolute_error_result)
      print("COEFFICIENT DE DETERMINATION: %.2f" % r2_score_result)
      print("*****************************************************************************")
  except NameError:        
    is_regression_algorithm = None
    print("SCORE: %.2f" % score)
        
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  print("END Predict_Model")
  
else:
  print('Please check your ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Download_Model">
      <description>
        <![CDATA[ Download a trained model. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/download_model.png"/>
      </genericInformation>
      <depends>
        <task ref="Train_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Download_Model")

import os
   
MODEL_BIN = variables.get("MODEL")

if MODEL_BIN != None:
	result = MODEL_BIN
	resultMetadata.put("file.extension", ".model")
	resultMetadata.put("file.name", "myModel.model")
	resultMetadata.put("content.type", "application/octet-stream")
	print("END Download_Model")
else:
	print("The model is empty, please check your ML pipeline")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Export_Results">
      <description>
        <![CDATA[ Export the results. ]]>
      </description>
      <variables>
        <variable name="OUTPUT_FILE" value="HTML" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/export_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Export_Results")

import pandas as pd
import numpy as np

OUTPUT_FILE = variables.get("OUTPUT_FILE")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
PREDICT_DATA = variables.get("PREDICT_DATA_JSON")

if DATA_TEST_DF_JSON != None and PREDICT_DATA != None: 
  data_test_df  = pd.read_json(DATA_TEST_DF_JSON, orient='split')   
  predict_data  = pd.read_json(PREDICT_DATA, orient='split')    
  frame_prediction = pd.DataFrame(predict_data)    
  prediction_result = data_test_df.assign(predictions=frame_prediction.values)
  prediction_result = prediction_result.sort_index(ascending=True) 
  
  if OUTPUT_FILE == 'CSV':
    result = prediction_result.to_csv()
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", "result.csv")
    resultMetadata.put("content.type", "text/csv")
  elif OUTPUT_FILE == 'HTML':
    result = prediction_result.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "result.html")
    resultMetadata.put("content.type", "text/html")          
  print("END Export_Results")  
else:
  print('It is not possible to export the data')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Load_Iris_Dataset">
      <description>
        <![CDATA[ Load and return the iris dataset classification. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/icons/load_dataset.png"/>
      </genericInformation>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Load_Iris_Dataset")

import pandas as pd
import numpy as np

file_url = "https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/iris.csv"
dataframe = pd.read_csv(file_url,sep=',') 
columns_name = dataframe.columns
columns_number = len(columns_name)

data  = dataframe.values[:,0:columns_number-1]
label = dataframe.values[:,columns_number-1]

data_df = pd.DataFrame(data=data,columns=columns_name[0:columns_number-1])
label_df = pd.DataFrame(data=label,columns=[columns_name[columns_number-1]])

DATAFRAME_JSON = dataframe.to_json(orient='split')
COLUMNS_NAME_JSON = pd.Series(columns_name).to_json()
DATA_TRAIN_DF_JSON = data_df.to_json(orient='split')
LABEL_TRAIN_DF_JSON = label_df.to_json(orient='split')
DATA_TEST_DF_JSON = data_df.to_json(orient='split')
LABEL_TEST_DF_JSON = label_df.to_json(orient='split')
IS_LABELED_DATA = 'True'

try:
  #**************Preview Data*********************
  result = dataframe.to_html()
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", "output.html")
  resultMetadata.put("content.type", "text/html")
  #***********************************************
  
  variables.put("DATAFRAME_JSON", DATAFRAME_JSON)
  variables.put("COLUMNS_NAME_JSON", COLUMNS_NAME_JSON)
  variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
  variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
  variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
  variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
  variables.put("IS_LABELED_DATA", IS_LABELED_DATA)
except NameError:
  pass

print("END Load_Iris_Dataset")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>