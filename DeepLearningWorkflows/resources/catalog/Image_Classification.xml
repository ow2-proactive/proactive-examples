<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="Image_Classification" projectName="4. Custom AI workflows"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="DOCKER_ENABLED" value="True" />
    <variable name="GPU_NODES_ONLY" value="False" />
  </variables>
  <description>
    <![CDATA[ Use a pre-trained deep neural network to classify an image dataset. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="deep-learning-workflows"/>
    <info name="pca.action.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_image_classification.png"/>
    <info name="Documentation" value="http://activeeon.com/resources/automated-machine-learning-activeeon.pdf"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Import_Model">
      <description>
        <![CDATA[ Import a trained model by a deep learning algorithm. ]]>
      </description>
      <variables>
        <variable name="GPU_NODES_ONLY" value="False" inherited="true" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" />
        <variable name="MODEL_URL" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/models/model_resnet18.zip" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_deep_model.png"/>
      </genericInformation>
      <selection>
        <script
         type="static" >
          <code language="python">
            <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Import_Model")

import wget
import uuid
import shutil
import zipfile

from os.path import join, exists
from os import remove, listdir, makedirs

# Load a trained model on ResNet-18 with two classes [ants, bees]
MODEL_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/model_resnet18.zip'

if 'variables' in locals():
  MODEL_URL = variables.get("MODEL_URL")

print("MODEL_URL:   " + MODEL_URL)
assert MODEL_URL is not None

# Get an unique ID
ID = str(uuid.uuid4())

# Create an empty dir
MODEL_FOLDER = join('models', ID)
if exists(MODEL_FOLDER):
  shutil.rmtree(MODEL_FOLDER)
makedirs(MODEL_FOLDER)
print("MODEL_FOLDER: " + MODEL_FOLDER)

print("Downloading...")
filename = wget.download(MODEL_URL, MODEL_FOLDER)
print("FILENAME: " + filename)
print("OK")

print("Extracting...")
dataset_zip = zipfile.ZipFile(filename)
dataset_zip.extractall(MODEL_FOLDER)
dataset_zip.close()
remove(filename)
print("OK")

MODEL_PATH = None
LABELS_PATH = None
TEXT_PATH = None 
for file in listdir(MODEL_FOLDER):
  if file.endswith(".pt"):
    MODEL_PATH = join(MODEL_FOLDER, file)
  if file.endswith("label.pkl"):
    LABELS_PATH = join(MODEL_FOLDER, file)
  if file.endswith("text.pkl"):
    TEXT_PATH = join(MODEL_FOLDER, file)
  if file.endswith(".txt"):
    LABELS_PATH = join(MODEL_FOLDER, file)

assert MODEL_PATH is not None
assert LABELS_PATH is not None

print("Model information: ")
print("MODEL_PATH:  " + MODEL_PATH)
print("LABELS_PATH: " + LABELS_PATH)
print("TEXT_PATH:   " + str(TEXT_PATH))

if 'variables' in locals():
  variables.put("MODEL_PATH", MODEL_PATH)
  variables.put("LABELS_PATH", LABELS_PATH)
  variables.put("TEXT_PATH", TEXT_PATH)
  variables.put("MODEL_FOLDER", MODEL_FOLDER)

print("END Import_Model")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <outputFiles>
        <files  includes="$MODEL_FOLDER/**" accessMode="transferToGlobalSpace"/>
      </outputFiles>
    </task>
    <task name="Import_Image_Dataset">
      <description>
        <![CDATA[ Load and return an image dataset. ]]>
      </description>
      <variables>
        <variable name="GPU_NODES_ONLY" value="False" inherited="true" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" />
        <variable name="DATASET_URL" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/ants_vs_bees.zip" inherited="true" />
        <variable name="SPLIT_TRAIN" value="0.5" inherited="false" />
        <variable name="SPLIT_VAL" value="0.25" inherited="false" />
        <variable name="SPLIT_TEST" value="0.25" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png"/>
      </genericInformation>
      <selection>
        <script
         type="static" >
          <code language="python">
            <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Import_Image_Dataset")

import wget
import zipfile
import shutil
import json
import uuid

from os import remove, listdir, makedirs
from os.path import basename, splitext, exists, join
from sklearn.model_selection import train_test_split

DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/ants_vs_bees.zip'
SPLIT_SETS  = ['train','val','test']
SPLIT_TRAIN = 0.50
SPLIT_VAL   = 0.25
SPLIT_TEST  = 0.25

if 'variables' in locals():
  if variables.get("DATASET_URL") is not None:
    DATASET_URL = variables.get("DATASET_URL")
  if variables.get("SPLIT_TRAIN") is not None:
    SPLIT_TRAIN = float(str(variables.get("SPLIT_TRAIN")))
  if variables.get("SPLIT_VAL") is not None:
    SPLIT_VAL = float(str(variables.get("SPLIT_VAL")))
  if variables.get("SPLIT_TEST") is not None:
    SPLIT_TEST = float(str(variables.get("SPLIT_TEST")))

# Get an unique ID
ID = str(uuid.uuid4())

# Define localspace
LOCALSPACE = join('data', ID)
if exists(LOCALSPACE):
  shutil.rmtree(LOCALSPACE)
makedirs(LOCALSPACE)

print("LOCALSPACE:  " + LOCALSPACE)
print("DATASET_URL: " + DATASET_URL)
assert DATASET_URL is not None

print("Split information: ")
print("SPLIT_TRAIN: " + str(SPLIT_TRAIN))
print("SPLIT_VAL:   " + str(SPLIT_VAL))
print("SPLIT_TEST:  " + str(SPLIT_TEST))

assert SPLIT_TRAIN >= 0.0
assert SPLIT_VAL >= 0.0
assert SPLIT_TEST >= 0.0
assert (SPLIT_TRAIN + SPLIT_VAL + SPLIT_TEST) == 1.0
if SPLIT_TRAIN == 0.0 and SPLIT_VAL > 0.0:
  raise AssertionError("SPLIT_VAL cannot be defined when SPLIT_TRAIN equals zero") 

DATASET_NAME = splitext(DATASET_URL[DATASET_URL.rfind("/")+1:])[0]
DATASET_PATH = join(LOCALSPACE, DATASET_NAME)

if exists(DATASET_PATH):
  shutil.rmtree(DATASET_PATH)
makedirs(DATASET_PATH)

print("Dataset information: ")
print("DATASET_NAME: " + DATASET_NAME)
print("DATASET_PATH: " + DATASET_PATH)

print("Downloading...")
filename = wget.download(DATASET_URL, DATASET_PATH)
print("FILENAME: " + filename)
print("OK")

print("Extracting...")
dataset_zip = zipfile.ZipFile(filename)
dataset_zip.extractall(DATASET_PATH)
dataset_zip.close()
remove(filename)
print("OK")

k = 0
images_list = []
images_label = []
labels_name = []
for root in listdir(DATASET_PATH):
  if (not root.startswith('.')):
    labels_name.append(root)
    label_dir = join(DATASET_PATH, root)
    print(label_dir)
    files = listdir(label_dir)
    files[:] = [join(label_dir,file) for file in files]
    files_size = len(files)
    files_label = [k] * files_size
    images_list = images_list + files
    images_label = images_label + files_label
    k = k + 1

print("Splitting the dataset into train and test")
images_train, images_test, labels_train, labels_test = train_test_split(images_list, images_label, test_size=SPLIT_TEST, random_state=1)

images_val = []
labels_val = []
if SPLIT_TRAIN != 0.0 and SPLIT_VAL != 0.0:
  print("Splitting the train into train and val")
  images_train, images_val, labels_train, labels_val = train_test_split(images_train, labels_train, test_size=SPLIT_VAL, random_state=1)

images_split = {SPLIT_SETS[0] : images_train, SPLIT_SETS[1] : images_val, SPLIT_SETS[2] : images_test}
labels_split = {SPLIT_SETS[0] : labels_train, SPLIT_SETS[1] : labels_val, SPLIT_SETS[2] : labels_test}

def move_images(_path, _images_list, _labels_list, _labels_name):
  if exists(_path):
    shutil.rmtree(_path)
  makedirs(_path)
  for _label_name in _labels_name:
    makedirs(join(_path, _label_name))
  for _image_path, _image_label in zip(_images_list, _labels_list):
    _image_path_dest = join(_path, _labels_name[_image_label], basename(_image_path))
    print("Moving " + _image_path + " to " + _image_path_dest)
    shutil.move(_image_path, _image_path_dest)

DATASET_LABELS = json.dumps(labels_name)
print("DATASET_LABELS: " + DATASET_LABELS)

for SPLIT_NAME in SPLIT_SETS:
  SPLIT_PATH = join(DATASET_PATH, SPLIT_NAME)
  print("SPLIT_PATH: " + SPLIT_PATH)
  move_images(SPLIT_PATH, images_split[SPLIT_NAME], labels_split[SPLIT_NAME], labels_name)

if 'variables' in locals():
  variables.put("DATASET_NAME", DATASET_NAME)
  variables.put("DATASET_PATH", DATASET_PATH)
  variables.put("DATASET_LABELS", DATASET_LABELS)

print("END Import_Image_Dataset")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <outputFiles>
        <files  includes="$DATASET_PATH/**" accessMode="transferToGlobalSpace"/>
      </outputFiles>
    </task>
    <task name="Export_Results">
      <description>
        <![CDATA[ Preview the predicted results ]]>
      </description>
      <variables>
        <variable name="OUTPUT_FILE" value="HTML" inherited="false" />
        <variable name="GPU_NODES_ONLY" value="False" inherited="true" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_results.png"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Image_Model"/>
      </depends>
      <inputFiles>
        <files  includes="$DATASET_PATH/**" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <selection>
        <script
         type="static" >
          <code language="python">
            <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Export_Results")

import base64
import os.path
import pandas as pd
import numpy as np

from pandas.io.json import json_normalize
from PIL import Image
from io import BytesIO

if 'variables' in locals():
  PREDICT_DATA = variables.get("PREDICT_DATA_JSON")
  OUTPUT_FILE = variables.get("OUTPUT_FILE")

assert PREDICT_DATA is not None
df = pd.read_json(PREDICT_DATA, orient='split')      
 
def get_thumbnail(path):
  i = Image.open(path)
  i.thumbnail((150, 150), Image.LANCZOS)
  return i

def image_base64(im):
  if isinstance(im, str):
    im = get_thumbnail(im)
  with BytesIO() as buffer:
    im.save(buffer, 'jpeg')
    return base64.b64encode(buffer.getvalue()).decode()

def image_formatter(im):
  return f'<img src="data:image/jpeg;base64,{image_base64(im)}" height="150" width="150">'

result = ''
with pd.option_context('display.max_colwidth', -1):
  #result = df.to_html(escape=False)
  result = df.to_html(escape=False, formatters=dict(Image=image_formatter))

css_style="""
table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%; 
}
td {
  border: 1px solid #999999;         
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
  #border-bottom: 1px solid #FF8C00;  
  border-bottom: 1px solid #0B6FA4;   
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  #background: #E7702A;       
  #border-left: 2px solid #999999
  border-bottom: 1px solid #FF8C00;            
}
"""
result = """

            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            <!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <style>{0}</style>
                </head>
                <body>{1}</body></html>
""".format(css_style, result)
print(result)

if OUTPUT_FILE == 'HTML':
  result = result.encode('utf-8')
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", "result.html")
  resultMetadata.put("content.type", "text/html")
elif OUTPUT_FILE == 'CSV':
  result = df.to_csv()    
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", "result.csv")
  resultMetadata.put("content.type", "text/csv")    
else:
  print('It is not possible to export the data')

print("END Export_Results")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
        </task>
        <task name="ResNet-18">
          <description>
            <![CDATA[ Deep Residual Networks (ResNet-18) is a deep convolutional neural network, trained on 1.28 million ImageNet training images, coming from 1000 classes. ]]>
          </description>
          <variables>
            <variable name="GPU_NODES_ONLY" value="False" inherited="true" />
            <variable name="DOCKER_ENABLED" value="True" inherited="true" />
            <variable name="USE_PRETRAINED_MODEL" value="True" inherited="false" />
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_learning.png"/>
          </genericInformation>
          <selection>
            <script
         type="static" >
              <code language="python">
                <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
              </code>
            </script>
          </selection>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN ResNet-18")

import json

USE_PRETRAINED_MODEL = 'true'
if 'variables' in locals():
  if variables.get("USE_PRETRAINED_MODEL") is not None:
    USE_PRETRAINED_MODEL = str(variables.get("USE_PRETRAINED_MODEL")).lower()

pretrained = False
if USE_PRETRAINED_MODEL == 'true':
  pretrained = True

# Define the CNN model
CNN_MODEL = """
cnn = models.resnet18(pretrained=""" + str(pretrained) + """)
num_features = cnn.fc.in_features
cnn.fc = nn.Linear(num_features, num_classes)
"""
print(CNN_MODEL)

# Data augmentation and normalization for training
# Just normalization for validation and test
CNN_TRANSFORM = """
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomSizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Scale(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Scale(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}
"""
print(CNN_TRANSFORM)

if 'variables' in locals():
  variables.put("CNN_MODEL", CNN_MODEL)
  variables.put("CNN_TRANSFORM", CNN_TRANSFORM)

print("END ResNet-18")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
        </task>
        <task name="Predict_Image_Model">
          <description>
            <![CDATA[ Predict a model using a deep learning algorithm. ]]>
          </description>
          <variables>
            <variable name="GPU_NODES_ONLY" value="False" inherited="true" />
            <variable name="DOCKER_ENABLED" value="True" inherited="true" />
            <variable name="BATCH_SIZE" value="4" inherited="false" />
            <variable name="NUM_WORKERS" value="2" inherited="false" />
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
          </genericInformation>
          <depends>
            <task ref="Import_Model"/>
            <task ref="ResNet-18"/>
            <task ref="Import_Image_Dataset"/>
          </depends>
          <inputFiles>
            <files  includes="$DATASET_PATH/**" accessMode="transferFromGlobalSpace"/>
            <files  includes="$MODEL_FOLDER/**" accessMode="transferFromGlobalSpace"/>
          </inputFiles>
          <selection>
            <script
         type="static" >
              <code language="python">
                <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
              </code>
            </script>
          </selection>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN Predict_Image_Model")

import torch
import json
import pandas as pd
import os

from os.path import join
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms

BATCH_SIZE  = 4
NUM_WORKERS = 2
SHUFFLE     = True

if 'variables' in locals():
  MODEL_PATH     = variables.get("MODEL_PATH")
  DATASET_PATH   = variables.get("DATASET_PATH")
  LABELS_PATH    = variables.get("LABELS_PATH")
  CNN_TRANSFORM  = variables.get("CNN_TRANSFORM")

assert MODEL_PATH is not None
assert DATASET_PATH is not None
assert LABELS_PATH is not None
assert CNN_TRANSFORM is not None

class_names = None
with open(LABELS_PATH, 'r') as f:
  class_names = json.load(f)
assert class_names is not None

# Load trained model
model = torch.load(MODEL_PATH)

# http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available
# Returns a bool indicating if CUDA is currently available.
use_gpu = torch.cuda.is_available()
if use_gpu:
  model = model.cuda()

# Load CNN transform
# data_transforms
exec(CNN_TRANSFORM)

# Load dataset
image_dataset = {x: 
  datasets.ImageFolder(join(DATASET_PATH, x), data_transforms[x]) 
  for x in ['test']}

data_loader = {x: 
  DataLoader(image_dataset[x], batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS) 
  for x in ['test']}

def predict_model(_model, _data_loader, _use_gpu, _class_names, _max_images=None):
  images_so_far = 0
  predictions = []
  image_target = []  
  for i, data in enumerate(_data_loader['test']):
    inputs, labels = data
    
    if _use_gpu:
      inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())
    else:
      inputs, labels = Variable(inputs), Variable(labels)

    outputs = _model(inputs)
    _, preds = torch.max(outputs.data, 1)
    print(preds)

    for j in range(inputs.size()[0]):
      images_so_far += 1
      result = _class_names[preds[j]]
      predictions.append(result)
      image_target.append(inputs)
      if images_so_far == _max_images:
        return
    
    #break
  return predictions, image_target 

preds, image_target = predict_model(model, data_loader, use_gpu, class_names)

label_test = []
image_input = []
image_name = []
for index, elem in enumerate(image_target):
  #print(index)
  input_select = data_loader['test'].dataset.imgs[index]
  dir_image_temp = input_select[0]
  id_label = input_select[1]  
  #com_data_dir = os.path.abspath(dir_image_temp)
  com_data_name = os.path.basename(dir_image_temp)    
  lab_d = data_loader['test'].dataset.classes
  lab_d = list(lab_d)
  label = lab_d[id_label]
  label_test.append(label)
  #image_input.append(com_data_dir)
  image_input.append(dir_image_temp)
  image_name.append(com_data_name)

df_preds = pd.DataFrame(preds)
df_label = pd.DataFrame(label_test)
df_name = pd.DataFrame(image_name)
df_preds.columns = ['Prediction']
df_label.columns = ['Target']
df_name.columns = ['Image Name']
df_test_image = pd.DataFrame(image_input)
df_test_image.columns = ['Image']
df = pd.concat([df_test_image, df_name, df_label, df_preds], axis=1)

if 'variables' in locals():
  variables.put("PREDICT_DATA_JSON", df.to_json(orient='split'))

print("END Predict_Image_Model")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
        </task>
      </taskFlow>
    </job>