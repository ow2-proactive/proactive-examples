<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.11" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Custom_Sentiment_Analysis_In_Bing_News" onTaskError="continueJobExecution" priority="normal" projectName="3. Mixed Workflows" xsi:schemaLocation="urn:proactive:jobdescriptor:3.11 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.11/schedulerjob.xsd">
  <variables>
    <variable model="PA:Boolean" name="GPU_NODES_ONLY" value="False"/>
    <variable name="GPU_CUDA_PATH" value="/usr/local/cuda"/>
    <variable model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
    <variable model="PA:LIST(CSV, HTML)" name="OUTPUT_FORMAT" value="HTML"/>
    <variable model="PA:LIST(languages, sentiment, keyPhrases)" name="FUNCTION" value="sentiment"/>
    <variable name="DOCUMENTS_JSON" value=""/>
    <variable name="SEARCH_TERM" value="Donald Trump"/>
    <variable model="PA:Integer" name="COUNT" value="20"/>
    <variable name="FRESHNESS" value=""/>
    <variable name="MARKET" value="en-US"/>
    <variable name="CATEGORY" value=""/>
    <variable name="SORT_BY" value=""/>
  </variables>
  <description>
    <![CDATA[ Custom Sentiment Analysis workflow searches for news related to a given search term using Azure Bing News API then performs a sentiment analysis using a custom deep learning based pretrained model. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="deep-learning-workflows"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_bing_news_search.png"/>
<info name="Documentation" value="https://ow2-proactive.github.io/proactive-examples/MachineLearning/resources/doc/V1/automated-machine-learning-activeeon.pdf"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task name="BingNews">
      <description>
        <![CDATA[ This task wraps the Bing News Search API of Microsoft which provides an experience similar to Bing.com/search by returning search results that Bing determines are relevant to a user's query. The results include Web pages and may also include images, videos, and more. The task requires this third-party credential : $BING_SEARCH_API_KEY which provides access to this API. Please refer to the User documentation to learn how to add third-party credentials.
$COUNT (optional) is the number of news articles to return in the response.
$FRESHNESS (optional) filter news articles by {"Day", "Week", "Month"}.
$MARKET is the market where the results come from. Market codes are listed here: https://docs.microsoft.com/en-us/rest/api/cognitiveservices/bing-news-api-v7-reference#market-codes
$CATEGORY (optional) is the category of articles to return. For example, Sports articles or Entertainment articles.
$SORTED_BY (optional) is the order to return the trending topics in. For example, sorted by "Date"
The task's output $BING_NEWS_SEARCH_OUTPUT is the result of the API call in a JSON format. ]]>
      </description>
      <variables>
        <variable inherited="true" name="SEARCH_TERM" value="activeeon"/>
        <variable inherited="true" model="PA:Integer" name="COUNT" value="10"/>
        <variable inherited="true" name="FRESHNESS" value=""/>
        <variable inherited="true" name="MARKET" value=""/>
        <variable inherited="true" name="CATEGORY" value=""/>
        <variable inherited="true" name="SORT_BY" value=""/>
        <variable inherited="true" model="PA:LIST(CSV, HTML)" name="OUTPUT_FORMAT" value="HTML"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_bing_news_search.png"/>
      </genericInformation>
      <forkEnvironment>
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="bash">
            <![CDATA[

]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import requests
import json
import urllib
from pprint import pprint
import pandas as pd
import re

# You can customize the api server location
#api_location="westcentralus"

# Congitive Services - Bing Video Search API URL:
#bing_news_search_url = "https://{0}.api.cognitive.microsoft.com/bing/v7.0/news/search".format(api_location)
bing_news_search_url = "https://api.cognitive.microsoft.com/bing/v7.0/news/search"

# READ TASK VARIABLES
if 'variables' in locals():
    if variables.get("SEARCH_TERM") is not None:
        SEARCH_TERM = variables.get("SEARCH_TERM")
    else:
        print("You first need to specify the search term")
        sys.exit(1)
    if variables.get("COUNT") is not None:
        COUNT = int(variables.get("COUNT"))
    if variables.get("FRESHNESS") is not None:
        FRESHNESS = variables.get("FRESHNESS")
    if variables.get("MARKET") is not None:
        MARKET = variables.get("MARKET")
    if variables.get("CATEGORY") is not None:
        CATEGORY = variables.get("CATEGORY")
    if variables.get("SORT_BY") is not None:
        SORT_BY = variables.get("SORT_BY")
    if variables.get("OUTPUT_FORMAT") is not None:
        OUTPUT_FORMAT = variables.get("OUTPUT_FORMAT")
    # Provide a valid subscription API token
    if credentials.get("BING_SEARCH_API_KEY") is not None:
        subscription_key = credentials.get("BING_SEARCH_API_KEY")
    else:
        print("You first need to add your Azure Cognitive Services API key to the third party credentials")
        sys.exit(1)

# Set API request parameters
#params  = {"q": SEARCH_TERM, "count":COUNT, "mkt": MARKET_CODE, "freshness": FRESHNESS, "category":CATEGORY}
params={'q':SEARCH_TERM}
if COUNT >0:
    params['count'] = COUNT
# Market Code: https://docs.microsoft.com/en-us/rest/api/cognitiveservices/bing-news-api-v7-reference#market-codes
if MARKET is not None and len(MARKET)>0:
    params['mkt'] = MARKET
# Freshness values: 'Day', 'Week', 'Month'
if FRESHNESS is not None and len(FRESHNESS)>0:
    params['freshness'] = FRESHNESS
if CATEGORY is not None and len(CATEGORY)>0:
    params['category'] = CATEGORY
if SORT_BY is not None and len(SORT_BY)>0:
    params['sortBy'] = SORT_BY

# Send API request
headers   = {
    "Ocp-Apim-Subscription-Key": subscription_key,
    'Content-Type': 'text/plain'
}
response = requests.get(bing_news_search_url, headers=headers, params=params)
response.raise_for_status()

# Get a JSON response
search_results = response.json()

# Print the results
#pprint(search_results)

if 'variables' in locals():
    variables.put('BING_NEWS_SEARCH_OUTPUT', search_results)

print("BEGIN Export_Results")

OUTPUT_DATA = search_results["value"]
table = []
for document in OUTPUT_DATA:
    try:
        image=document["image"]
    except KeyError:
        video=None
        pass
    description= document["description"]
    name= document["name"]
    url= document["url"]
    table.append("""<tr><td>{0}</td><td>{1}</td><td><a href="{2}">{2}</a></td>""".format(name, description, url))

css_style="""table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%;
}
td {
  border: 1px solid #999999;
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  border-left: 2px solid #999999;
}"""
html = ("""<table><tr><th>Title</th><th>Snippet</th><th>Url</th></tr>{0}</table>""").format("\n".join(table))
html_container="""<!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <meta name="description" content="Bing Video Search API Results">
                    <style>{0}</style>
                  </head>
                  <body>{1}</body></html>""".format(css_style,html)

if OUTPUT_DATA != None and 'resultMetadata' in locals(): 
    dataframe=pd.read_html(html_container,header=0, encoding='utf-8')[0]
    
    if OUTPUT_FORMAT == 'CSV':
        result = dataframe.to_csv(index=False).encode('utf-8')
        resultMetadata.put("file.extension", ".csv")
        resultMetadata.put("file.name", "result.csv")
        resultMetadata.put("content.type", "text/csv")
    elif OUTPUT_FORMAT == 'HTML':
        result = html_container.encode('utf-8')
        resultMetadata.put("file.extension", ".html")
        resultMetadata.put("file.name", "result.html")
        resultMetadata.put("content.type", "text/html")
    print("END Export_Results")  
else:
    print('It is not possible to export the data')

# Uncomment this to render the HTML result locally in your python notebook
#from IPython.display import HTML
#HTML(html_container)
]]>
                </code>
              </script>
            </scriptExecutable>
          </task>
          <task name="NewsTextExtractor">
            <description>
              <![CDATA[ This task extracts news snippets from Bing News then prepares them as an custom format input for Predict_Text_Model task. ]]>
            </description>
            <depends>
              <task ref="BingNews"/>
            </depends>
            <forkEnvironment javaHome="/usr">
              <envScript>
                <script>
                  <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars_dlm3/raw"/>
                </script>
              </envScript>
            </forkEnvironment>
            <scriptExecutable>
              <script>
                <code language="cpython">
                  <![CDATA[
import json
import pandas as pd
import os
from os.path import basename, splitext, exists, join
from os import remove, listdir, makedirs
import shutil

# Create output directory
GLOBALSPACE = 'text_data/'
DATASET_PATH = os.path.join(GLOBALSPACE,"BING_NEWS_TEXT")
if exists(DATASET_PATH):
    shutil.rmtree(DATASET_PATH)
makedirs(DATASET_PATH)

train_path=os.path.join(DATASET_PATH,"train.csv")
test_path=os.path.join(DATASET_PATH,"test.csv")
val_path=os.path.join(DATASET_PATH,"val.csv")

# Get Google News search results
OUTPUT_DATA=variables.get("BING_NEWS_SEARCH_OUTPUT")["value"]
text_json={'documents':[]}
# Extract news snippets and add 'unlabeled' as a label
for document in OUTPUT_DATA:
    description=document["description"]
    text_json['documents'].append({'text': description, 'label':'unlabeled'})

# Create a dataframe to prepare data export
dataframe=pd.read_json(json.dumps(text_json['documents']).encode('utf-8'),orient='records')
# Export data as a csv to the file "text.csv"
dataframe.to_csv(test_path,columns=["text","label"],index=False,header=False)

# Create empty "train.csv" and "val.csv"
# THIS IS A TEMPORARY WORKAROUD
# TODO avoid creating empty csv files
open(train_path, 'a').close()
open(val_path, 'a').close()


# CODE TO EXPORT
DATASET_ITERATOR_UNL="""
text_field = data.Field(lower=True)#, tokenize=TOKENIZER)
print("text_field=",text_field)
label_field = data.Field(sequential=False)
print("label_field",label_field)
#Dataset of columns stored in CSV, TSV, or JSON format
train, val, test = data.TabularDataset.splits(path=DATASET_PATH, train='train.csv',
                                                  validation='val.csv', test='test.csv', format='csv',
                                                  fields=[('text', text_field), ('label', label_field)])
train_iter, val_iter, test_iter = data.BucketIterator.splits((train, val, test),
                                                                  repeat=False,
                                                             batch_sizes=(BATCH_SIZE,len(val),len(test)), sort_key=lambda x: len(x.text), device=DEVICE)


text_field.build_vocab(test)
label_field.build_vocab(test)
"""

variables.put("DATASET_ITERATOR_UNL",DATASET_ITERATOR_UNL)
variables.put("DATASET_PATH",DATASET_PATH)
variables.put("IS_LABELED_DATA","False")
]]>
                </code>
              </script>
            </scriptExecutable>
            <controlFlow block="none"/>
            <outputFiles>
              <files accessMode="transferToGlobalSpace" includes="$DATASET_PATH/**"/>
            </outputFiles>
          </task>
          <task name="RNN">
            <description>
              <![CDATA[ Simple RNN Model ]]>
            </description>
            <variables>
              <variable inherited="false" name="EMBEDDING_DIM" value="50"/>
              <variable inherited="false" name="HIDDEN_DIM" value="40"/>
              <variable inherited="false" name="BATCH_SIZE" value="2"/>
              <variable inherited="false" name="DROPOUT" value="0.5"/>
            </variables>
            <genericInformation>
              <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/ml_classification.png"/>
            </genericInformation>
            <selection>
              <script type="static">
                <code language="javascript">
                  <![CDATA[
selected = ((variables.get("GPU_NODES_ONLY").equalsIgnoreCase("false")) || (variables.get("GPU_NODES_ONLY").equalsIgnoreCase("true") && org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkFileExist(variables.get("GPU_CUDA_PATH"))));
]]>
                </code>
              </script>
            </selection>
            <forkEnvironment javaHome="/usr">
              <envScript>
                <script>
                  <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars_dlm3/raw"/>
                </script>
              </envScript>
            </forkEnvironment>
            <scriptExecutable>
              <script>
                <code language="cpython">
                  <![CDATA[
print("BEGIN Create a RNN model")
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F

BATCH_SIZE=2
HIDDEN_DIM=50
EMBEDDING_DIM=50
DROPOUT=0.5

if 'variables' in locals():  
  if variables.get("BATCH_SIZE") is not None:
    BATCH_SIZE = variables.get("BATCH_SIZE")
  else:
    print("BATCH_SIZE not defined by the user. Using the default value:"+BATCH_SIZE)
  if variables.get("HIDDEN_DIM") is not None:
    HIDDEN_DIM = variables.get("HIDDEN_DIM")
  else:
    print("HIDDEN_DIM not defined by the user. Using the default value:"+HIDDEN_DIM)
  if variables.get("EMBEDDING_DIM") is not None:
    EMBEDDING_DIM = variables.get("EMBEDDING_DIM")
  else:
    print("EMBEDDING_DIM not defined by the user. Using the default value:"+EMBEDDING_DIM)
  if variables.get("DROPOUT") is not None:
    DROPOUT = variables.get("DROPOUT")
  else:
    print("DROPOUT not defined by the user. Using the default value:"+DROPOUT)


MODEL_TYPE = 'RNN'
MODEL_CLASS = """
class RNN(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, use_gpu, batch_size, dropout=0.5):
        super(RNN, self).__init__()
        self.hidden_dim = hidden_dim
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        self.dropout = dropout
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.RNN = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim)
        self.hidden2label = nn.Linear(hidden_dim, label_size)
        self.hidden = self.init_hidden()

    def init_hidden(self):
        if self.use_gpu:
            return (Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda()))
        else:
            return (Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))

    def forward(self, sentence):
        x = self.embeddings(sentence).view(len(sentence), self.batch_size, -1)
        rnn_out, self.hidden = self.RNN(x, self.hidden)
        y = self.hidden2label(rnn_out[-1])
        log_probs = F.log_softmax(y)
        return log_probs"""
    
MODEL_DEF = """
MODEL = RNN(embedding_dim="""+str(EMBEDDING_DIM)+""", hidden_dim="""+str(HIDDEN_DIM)+""", vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,use_gpu=USE_GPU, batch_size=BATCH_SIZE)
"""
print(MODEL_DEF)

# Forward model
try:
    variables.put("MODEL_CLASS", MODEL_CLASS)
    variables.put("MODEL_DEF", MODEL_DEF)
    variables.put("BATCH_SIZE", BATCH_SIZE)
    variables.put("HIDDEN_DIM", HIDDEN_DIM)
    variables.put("EMBEDDING_DIM", EMBEDDING_DIM)
    variables.put("DROPOUT", DROPOUT)
except NameError as err:
    print("{0}".format(err))
    print("Warning: this script is running outside from ProActive.")
    pass

print("END RNN model built")
]]>
                </code>
              </script>
            </scriptExecutable>
            <controlFlow block="none"/>
          </task>
          <task name="Import_Model">
            <description>
              <![CDATA[ Import a trained model by a deep learning algorithm. ]]>
            </description>
            <variables>
              <variable inherited="true" model="PA:Boolean" name="GPU_NODES_ONLY" value="False"/>
              <variable inherited="true" name="GPU_CUDA_PATH" value="/usr/local/cuda"/>
              <variable inherited="true" name="DOCKER_ENABLED" value="True"/>
              <variable inherited="false" name="MODEL_URL" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/models/basic_sentiment_analysis.zip"/>
            </variables>
            <genericInformation>
              <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_deep_model.png"/>
            </genericInformation>
            <selection>
              <script type="static">
                <code language="javascript">
                  <![CDATA[
selected = ((variables.get("GPU_NODES_ONLY").equalsIgnoreCase("false")) || (variables.get("GPU_NODES_ONLY").equalsIgnoreCase("true") && org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkFileExist(variables.get("GPU_CUDA_PATH"))));
]]>
                </code>
              </script>
            </selection>
            <forkEnvironment javaHome="/usr">
              <envScript>
                <script>
                  <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars_dlm3/raw"/>
                </script>
              </envScript>
            </forkEnvironment>
            <scriptExecutable>
              <script>
                <code language="cpython">
                  <![CDATA[
print("BEGIN Import_Model")

import wget
import uuid
import shutil
import zipfile

from os.path import join, exists
from os import remove, listdir, makedirs

# Load a trained model on ResNet-18 with two classes [ants, bees]
MODEL_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/model_resnet18.zip'

if 'variables' in locals():
  GLOBALSPACE = str(variables.get("PA_SCHEDULER_HOME")) + '/data/defaultglobal/'
  MODEL_URL = variables.get("MODEL_URL")

print("MODEL_URL:   " + MODEL_URL)
assert MODEL_URL is not None

# Get an unique ID
ID = str(uuid.uuid4())


# Create an empty dir
MODEL_FOLDER = 'text_models/'
MODEL_FOLDER =  join(MODEL_FOLDER, ID)

if exists(MODEL_FOLDER):
  shutil.rmtree(MODEL_FOLDER)
makedirs(MODEL_FOLDER)
print("MODEL_FOLDER: " + MODEL_FOLDER)

print("Downloading...")
filename = wget.download(MODEL_URL, MODEL_FOLDER)
print("FILENAME: " + filename)
print("OK")

print("Extracting...")
dataset_zip = zipfile.ZipFile(filename)
dataset_zip.extractall(MODEL_FOLDER)
dataset_zip.close()
remove(filename)
print("OK")

MODEL_PATH = None
LABELS_PATH = None
TEXT_PATH = None 
for file in listdir(MODEL_FOLDER):
  if file.endswith(".pt"):
    MODEL_PATH = join(MODEL_FOLDER, file)
  if file.endswith("label.pkl"):
    LABELS_PATH = join(MODEL_FOLDER, file)
  if file.endswith("text.pkl"):
    TEXT_PATH = join(MODEL_FOLDER, file)
  if file.endswith(".txt"):
    LABELS_PATH = join(MODEL_FOLDER, file)

print("Model information: ")
print("MODEL_PATH:  " + MODEL_PATH)
print("LABELS_PATH: " + LABELS_PATH)
print("TEXT_PATH: " + TEXT_PATH)
assert MODEL_PATH is not None
assert LABELS_PATH is not None

if 'variables' in locals():
  variables.put("MODEL_PATH", MODEL_PATH)
  variables.put("LABELS_PATH", LABELS_PATH)
  variables.put("TEXT_PATH", TEXT_PATH)
  variables.put("MODEL_FOLDER", MODEL_FOLDER)

print("END Import_Model")
]]>
                </code>
              </script>
            </scriptExecutable>
            <controlFlow block="none"/>
            <outputFiles>
              <files accessMode="transferToGlobalSpace" includes="$MODEL_FOLDER/**"/>
            </outputFiles>
          </task>
          <task name="Predict_Text_Model">
            <description>
              <![CDATA[ Predict results based on new data ]]>
            </description>
            <variables>
              <variable inherited="true" model="PA:Boolean" name="GPU_NODES_ONLY" value="False"/>
              <variable inherited="true" name="GPU_CUDA_PATH" value="/usr/local/cuda"/>
              <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
              <variable inherited="false" model="PA:List(L1Loss, MSELoss, CrossEntropyLoss, NLLLoss)" name="LOSS_FUNCTION" value="NLLLoss"/>
            </variables>
            <genericInformation>
              <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
            </genericInformation>
            <depends>
              <task ref="NewsTextExtractor"/>
              <task ref="RNN"/>
              <task ref="Import_Model"/>
            </depends>
            <inputFiles>
              <files accessMode="transferFromGlobalSpace" includes="$DATASET_PATH/**"/>
              <files accessMode="transferFromGlobalSpace" includes="$MODEL_FOLDER/**"/>
            </inputFiles>
            <selection>
              <script type="static">
                <code language="javascript">
                  <![CDATA[
selected = ((variables.get("GPU_NODES_ONLY").equalsIgnoreCase("false")) || (variables.get("GPU_NODES_ONLY").equalsIgnoreCase("true") && org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkFileExist(variables.get("GPU_CUDA_PATH"))));
]]>
                </code>
              </script>
            </selection>
            <forkEnvironment javaHome="/usr">
              <envScript>
                <script>
                  <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars_dlm3/raw"/>
                </script>
              </envScript>
            </forkEnvironment>
            <scriptExecutable>
              <script>
                <code language="cpython">
                  <![CDATA[
print("BEGIN Prediction")

from torchtext import data
from torchtext import datasets
from torchtext import vocab
from torchtext.vocab import Vectors, FastText, GloVe, CharNGram
from tqdm import tqdm
import time, random
import os
from torch.autograd import Variable
import torch.optim as optim
import time
#import spacy
from itertools import *
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import dill as pickle

pd.options.display.max_colwidth = 500
DEVICE = -1
#-------------------------main code --------------------------
DATASET_ITERATOR_UNL = None
DATASET_ITERATOR = None
#--------Get varaiables from previous tasks--------
if 'variables' in locals():
    if variables.get("MODEL_PATH") is not None:
        MODEL_PATH = variables.get("MODEL_PATH")
    if variables.get("LOSS_FUNCTION") is not None:
        LOSS_FUNCTION = variables.get("LOSS_FUNCTION")
    if variables.get("BATCH_SIZE") is not None:
        BATCH_SIZE = variables.get("BATCH_SIZE")
    if variables.get("USE_GPU") is not None:
        USE_GPU = variables.get("USE_GPU")
    if variables.get("DEVICE") is not None:
        DEVICE = variables.get("DEVICE") 
    if variables.get("IS_LABELED_DATA") is not None:
        IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
    if variables.get("vocab_size") is not None:
        vocab_size = variables.get("vocab_size")
    if variables.get("label_size") is not None:
        label_size = variables.get("label_size")
    if variables.get("BATCH_SIZE") is not None:
        label_size = variables.get("BATCH_SIZE")
    if variables.get("MODEL_CLASS") is not None:
        MODEL_CLASS = variables.get("MODEL_CLASS")
    if variables.get("MODEL_DEF") is not None:
        MODEL_DEF = variables.get("MODEL_DEF")


#--------Load Dataset--------

if 'variables' in locals():
    if variables.get("DATASET_PATH") is not None:
        DATASET_PATH = variables.get("DATASET_PATH")
    if variables.get("DATASET_ITERATOR") is not None:
        DATASET_ITERATOR = variables.get("DATASET_ITERATOR")
        DATASET_PATH = variables.get("DATASET_PATH")
        exec(DATASET_ITERATOR)
    if variables.get("DATASET_ITERATOR_UNL") is not None:
        DATASET_ITERATOR_UNL = variables.get("DATASET_ITERATOR_UNL")
        DATASET_PATH = variables.get("DATASET_PATH")
        exec(DATASET_ITERATOR_UNL)
    #Load model files
    if variables.get("LABELS_PATH") is not None:
        LABELS_PATH = variables.get("LABELS_PATH")
    if variables.get("TEXT_PATH") is not None:
        TEXT_PATH = variables.get("TEXT_PATH")
   
#-------Main--------
def evaluate(model, test, data_iter, label_field, loss_function, name):
    model.eval()
    avg_loss = 0.0
    truth_res = []
    pred_res = []
    i=0
    acc = 0
    pd.options.display.max_colwidth = 500
    result =  pd.DataFrame(columns=['text','prediction','label'])
    for batch in data_iter:
        i=i+1
        sent, label = batch.text, batch.label
        label.data.sub_(1)
        truth_res += list(label.data)
        model.batch_size = len(test)
        model.hidden = model.init_hidden()
        pred = model(sent)
        pred_label = pred.data.max(1)[1].numpy()
        for i in range(model.batch_size):
            test_fields = vars(test[i])
            test_text = test_fields["text"]
            test_label = test_fields["label"]
            prede_label = pred_label[i]
            gd_label = label[i]
            result.loc[i] = [' '.join(test_text),label_field.vocab.itos[prede_label+1], test_label]
        pred_res += [x for x in pred_label]
        if name is 'test_labeled':
            loss = loss_function(pred, label)
            avg_loss += loss.data[0]
    if name is 'test_labeled':
        avg_loss /= len(test)
        acc = get_accuracy(truth_res, pred_res)
        print(name + ': loss %.2f acc %.1f' % (avg_loss, acc*100))
    return acc, result
    
def get_accuracy(truth, pred):
     assert len(truth)==len(pred)
     right = 0
     for i in range(len(truth)):
         if truth[i]==pred[i]:
             right += 1.0
     return right/len(truth)
     
LOSS ="""loss_function = nn."""+LOSS_FUNCTION+"""()"""

exec(LOSS)


label_field = pickle.load(open(LABELS_PATH,'rb'))
text_field = pickle.load(open(TEXT_PATH,'rb'))
exec(MODEL_CLASS)

MODEL=torch.load(MODEL_PATH)
if variables.get("DATASET_ITERATOR_UNL") is not None:
    print('I am testing the unlabeled dataset')
    test_acc, results = evaluate(MODEL, test, test_iter, label_field, loss_function, 'test_unlabeled')
else:
    print('I am testing the labeled dataset')
    test_acc, results = evaluate(MODEL, test, test_iter, label_field, loss_function, 'test_labeled')

#------plot results-----
# Forward results for preview 
try:
    variables.put("PREDICT_DATA_JSON", results.to_json(orient='split'))
except NameError as err:
    print("{0}".format(err))
    print("Warning: this script is running outside from ProActive.")
    pass

print("END Prediction")
]]>
                </code>
              </script>
            </scriptExecutable>
            <controlFlow block="none"/>
          </task>
          <task name="Export_Results">
            <description>
              <![CDATA[ Preview the predicted results ]]>
            </description>
            <variables>
              <variable inherited="false" name="OUTPUT_FILE" value="HTML"/>
              <variable inherited="true" model="PA:Boolean" name="GPU_NODES_ONLY" value="False"/>
              <variable inherited="true" name="GPU_CUDA_PATH" value="/usr/local/cuda"/>
              <variable inherited="true" name="DOCKER_ENABLED" value="True"/>
            </variables>
            <genericInformation>
              <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_results.png"/>
            </genericInformation>
            <depends>
              <task ref="Predict_Text_Model"/>
            </depends>
            <selection>
              <script type="static">
                <code language="javascript">
                  <![CDATA[
selected = ((variables.get("GPU_NODES_ONLY").equalsIgnoreCase("false")) || (variables.get("GPU_NODES_ONLY").equalsIgnoreCase("true") && org.ow2.proactive.scripting.helper.selection.SelectionUtils.checkFileExist(variables.get("GPU_CUDA_PATH"))));
]]>
                </code>
              </script>
            </selection>
            <forkEnvironment javaHome="/usr">
              <envScript>
                <script>
                  <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars_dlm3/raw"/>
                </script>
              </envScript>
            </forkEnvironment>
            <scriptExecutable>
              <script>
                <code language="cpython">
                  <![CDATA[
print("BEGIN Export_Results")

import base64
import os.path
import pandas as pd
import numpy as np

from pandas.io.json import json_normalize
from PIL import Image
from io import BytesIO

PREDICT_DATA = variables.get("PREDICT_DATA_JSON")
OUTPUT_FILE = variables.get("OUTPUT_FILE")
   
if PREDICT_DATA != None: 
  prediction_result = pd.read_json(PREDICT_DATA, orient='split')      
 
def get_thumbnail(path):
  i = Image.open(path)
  i.thumbnail((150, 150), Image.LANCZOS)
  return i

def image_base64(im):
  if isinstance(im, str):
    im = get_thumbnail(im)
  with BytesIO() as buffer:
    im.save(buffer, 'jpeg')
    return base64.b64encode(buffer.getvalue()).decode()

def image_formatter(im):
  return f'<img src="data:image/jpeg;base64,{image_base64(im)}" height="150" width="150">'
    
df = pd.DataFrame(prediction_result)

result = ''
with pd.option_context('display.max_colwidth', -1):
  #result = df.to_html(escape=False)
  result = df.to_html(escape=False, formatters=dict(Image=image_formatter), columns=["text","prediction"])

css_style="""
table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%; 
}
td {
  border: 1px solid #999999;         
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
  #border-bottom: 1px solid #FF8C00;  
  border-bottom: 1px solid #0B6FA4;   
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  #background: #E7702A;       
  #border-left: 2px solid #999999
  border-bottom: 1px solid #FF8C00;            
}
"""
result = """<!DOCTYPE html>
                  <html>
                    <head>
                      <meta charset="UTF-8">
                        <meta name="description" content="Face API">
                          <style>{0}</style>
                        </head>
                        <body>{1}</body></html>
""".format(css_style, result)
print(result)

if OUTPUT_FILE == 'HTML':
  result = result.encode('utf-8')
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", "result.html")
  resultMetadata.put("content.type", "text/html")
elif OUTPUT_FILE == 'CSV':
  #result = prediction_result.to_csv()
  result = df.to_csv()    
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", "result.csv")
  resultMetadata.put("content.type", "text/csv")    
else:
  print('It is not possible to export the data')

print("END Export_Results")
]]>
                      </code>
                    </script>
                  </scriptExecutable>
                  <controlFlow block="none"/>
                </task>
              </taskFlow>
              <metadata>
                <visualization>
                  <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:1122px;
            height:582px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-142px;left:-357.5px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_1997" style="top: 147px; left: 362.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_bing_news_search.png" width="20px">&nbsp;<span class="name">BingNews</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_2000" style="top: 275px; left: 362.5px;"><a class="task-name"><img src="/studio/images/Python.png" width="20px">&nbsp;<span class="name">NewsTextExtractor</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_2003" style="top: 275px; left: 509.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/ml_classification.png" width="20px">&nbsp;<span class="name">RNN</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_2006" style="top: 275px; left: 647.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/import_deep_model.png" width="20px">&nbsp;<span class="name">Import_Model</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_2009" style="top: 403px; left: 509.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/predict.png" width="20px">&nbsp;<span class="name">Predict_Text_Model</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_2012" style="top: 531px; left: 509.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/export_results.png" width="20px">&nbsp;<span class="name">Export_Results</span></a></div><svg style="position:absolute;left:401.5px;top:186.5px" width="30" height="89" pointer-events="none" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 9 88 C 19 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M11.064096,66.303232 L15.016942635045325,45.485571144855605 L8.985401777301874,52.45841237934327 L1.1721230143885997,47.56426536755374 L11.064096,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M11.064096,66.303232 L15.016942635045325,45.485571144855605 L8.985401777301874,52.45841237934327 L1.1721230143885997,47.56426536755374 L11.064096,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:410.5px;top:314.5px" width="171" height="89" pointer-events="none" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 150 88 C 160 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M129.375,60.1875 L113.52667074459112,46.122278358939695 L116.42194006160713,54.87541582969529 L108.21458657428641,59.075338297332564 L129.375,60.1875" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M129.375,60.1875 L113.52667074459112,46.122278358939695 L116.42194006160713,54.87541582969529 L108.21458657428641,59.075338297332564 L129.375,60.1875" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:549.5px;top:314.5px" width="32" height="89" pointer-events="none" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 11 88 C 21 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M12.893024,66.303232 L16.31083689897854,45.39106915184926 L10.460052179499373,52.5162581434697 L2.523863042448242,47.824040972349884 L12.893024,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M12.893024,66.303232 L16.31083689897854,45.39106915184926 L10.460052179499373,52.5162581434697 L2.523863042448242,47.824040972349884 L12.893024,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:560.5px;top:314.5px" width="147" height="89" pointer-events="none" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 136 50 126 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M15.443711999999998,60.999424000000005 L36.4670701527531,58.35019543661222 L27.975818644897245,54.75872667991452 L30.226372832667614,45.81808879171496 L15.443711999999998,60.999424000000005" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M15.443711999999998,60.999424000000005 L36.4670701527531,58.35019543661222 L27.975818644897245,54.75872667991452 L30.226372832667614,45.81808879171496 L15.443711999999998,60.999424000000005" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:550.5px;top:442.5px" width="31" height="89" pointer-events="none" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 20 50 10 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M-1.97856,66.303232 L8.153339129365806,47.69289562387942 L0.27782929033988846,52.486260285096634 L-5.663632585537553,45.43650633353954 L-1.97856,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" d="M-1.97856,66.303232 L8.153339129365806,47.69289562387942 L0.27782929033988846,52.486260285096634 L-5.663632585537553,45.43650633353954 L-1.97856,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 402px; top: 177px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 411px; top: 305px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 411px; top: 265px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 550px; top: 305px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 687px; top: 305px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 561px; top: 433px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 561px; top: 393px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 551px; top: 561px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 551px; top: 521px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
                  xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
                </visualization>
              </metadata>
            </job>
