<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="Sentiment_Analysis_In_Bing_News" projectName="1. Azure Cognitive Services"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="OUTPUT_FORMAT" value="HTML" model="PA:LIST(CSV, HTML)"/>
    <variable name="FUNCTION" value="sentiment" model="PA:LIST(languages, sentiment, keyPhrases)"/>
    <variable name="DOCUMENTS_JSON" value="" />
    <variable name="SEARCH_TERM" value="Donald Trump" />
    <variable name="COUNT" value="3" model="PA:Integer"/>
    <variable name="FRESHNESS" value="" />
    <variable name="MARKET" value="en-US" />
    <variable name="CATEGORY" value="" />
    <variable name="SORT_BY" value="" />
  </variables>
  <description>
    <![CDATA[ This workflow is a mashup that searches for news related to a given search term using Azure Bing News API then performs a sentiment analysis using Azure Text Analytics API. ]]>
  </description>
  <genericInformation>
    <info name="pca.action.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/cognitive_services.png"/>
  </genericInformation>
  <taskFlow>
    <task name="TextAnalytics">
      <description>
        <![CDATA[ This task wraps the Text Analytics API of Microsoft which is a cloud-based service that provides advanced natural language processing over raw text, and includes three main functions: sentiment analysis, key phrase extraction, and language detection.
The task requires this third-party credential : $TEXT_ANALYTICS_API_KEY which provides access to this API. Please refer to the User documentation to learn how to add third-party credentials.
$FUNCTION (required) is a list containing different types of text analysis. Possible values are {"sentiment, "language, "keyPhrases"}
$DOCUMENT_JSON (required) is the input json document.
The task's output $TEXT_ANALYTICS_OUTPUT is the result of the API call in a JSON format. ]]>
      </description>
      <variables>
        <variable name="FUNCTION" value="sentiment" inherited="true" model="PA:LIST(languages, sentiment, keyPhrases)"/>
        <variable name="DOCUMENTS_JSON" value="" inherited="true" />
        <variable name="OUTPUT_FORMAT" value="HTML" inherited="true" model="PA:LIST(CSV, HTML)"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_textanalytics.svg"/>
      </genericInformation>
      <depends>
        <task ref="NewsTextExtractor"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="bash">
            <![CDATA[

]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import requests
import json
from pprint import pprint
import pandas as pd
import csv

# You can customize the api server location
api_location="westcentralus"

# Congitive Services - Text Analytics API URL:
text_analytics_base_url = "https://{0}.api.cognitive.microsoft.com/text/analytics/v2.0/".format(api_location)

functions_list = ["languages", "sentiment", "keyPhrases"]

# READ TASK VARIABLES
if 'variables' in locals():
    if variables.get('FUNCTION') is not None:
        FUNCTION = variables.get('FUNCTION')
        if FUNCTION in functions_list:
            function_url = text_analytics_base_url + FUNCTION
        else:
            print("You must provide a valid Azure Text Analytics function name.")
            sys.exit(1)
    if variables.get("DOCUMENTS_JSON") is not None:
        DOCUMENTS_JSON = variables.get("DOCUMENTS_JSON")        
    if variables.get("OUTPUT_FORMAT") is not None:
        OUTPUT_FORMAT = variables.get("OUTPUT_FORMAT")
    # Provide a valid subscription API token
    if credentials.get("TEXT_ANALYTICS_API_KEY") is not None:
        subscription_key = credentials.get("TEXT_ANALYTICS_API_KEY")
    else:
        print("You first need to add your Azure Text Analytics Service API key to the third party credentials")
        sys.exit(1)

# Example Input documents with different languages:
# Structure:
#           {
#            "documents": [
#                {
#                  "id": "string",
#                  "text": "string"
#                }
#            ]
#           }
documents=json.loads(DOCUMENTS_JSON)

function_url = text_analytics_base_url + FUNCTION

# Send API request
headers   = {
    "Ocp-Apim-Subscription-Key": subscription_key,
    'Content-Type': 'application/json'
}
response  = requests.post(function_url, headers=headers, data=DOCUMENTS_JSON.encode('utf-8'))

# Get a JSON response
api_results = response.json()

if 'variables' in locals():
    variables.put('TEXT_ANALYTICS_OUTPUT', api_results)

# Print the results
#pprint(api_results)

print("BEGIN Export_Results")

OUTPUT_DATA = api_results["documents"]

# Convert the results into HTML
table = []
for document in OUTPUT_DATA:
    text  = next(filter(lambda d: d["id"] == document["id"], documents["documents"]))["text"]
    id  = next(filter(lambda d: d["id"] == document["id"], documents["documents"]))["id"]
    if(FUNCTION == functions_list[0]):
        res = ", ".join(["{0}({1})".format(lang["name"], lang["score"]) for lang in document["detectedLanguages"]])
    elif(FUNCTION == functions_list[1]):
        res = "({0})".format(document["score"]) 
    elif(FUNCTION == functions_list[2]):
        res = ", ".join(['"{0}"'.format(lang) for lang in document["keyPhrases"]])
    table.append("""<tr><td><a href="{0}">{0}</a></td><td>{1}</td><td>{2}</td>""".format(id, text, res))

if(FUNCTION == functions_list[0]):
    table_header_column = "Detected languages (scores)"
elif(FUNCTION == functions_list[1]):
    table_header_column = "Sentiment score"
elif(FUNCTION == functions_list[2]):
    table_header_column = "Keyphrases"
    

css_style="""table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%;
}
td {
  border: 1px solid #999999;
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  border-left: 2px solid #999999;
}"""
html = ("""<table><tr><th>id</th><th width=65%>Text</th><th>"""+table_header_column+"</th></tr>{0}</table>").format("\n".join(table))
html_container="""<!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <meta name="description" content="{2}">
                    <style>{0}</style>
                  </head>
                  <body>{1}</body></html>""".format(css_style,html,table_header_column)
if OUTPUT_DATA != None and 'resultMetadata' in locals(): 
    #dataframe = pd.read_json(json.dumps(OUTPUT_DATA), orient='table', encoding='utf-8')
    dataframe=pd.read_html(html_container,header=0, encoding='utf-8')[0]
    
    if OUTPUT_FORMAT == 'CSV':
        result = dataframe.to_csv(index=False).encode('utf-8')
        resultMetadata.put("file.extension", ".csv")
        resultMetadata.put("file.name", "result.csv")
        resultMetadata.put("content.type", "text/csv")
    elif OUTPUT_FORMAT == 'HTML':
        result = html_container.encode('utf-8')
        resultMetadata.put("file.extension", ".html")
        resultMetadata.put("file.name", "result.html")
        resultMetadata.put("content.type", "text/html")
    print("END Export_Results")  
else:
    print('It is not possible to export the data')

# Uncomment this to render the HTML result locally in your python notebook
#from IPython.display import HTML
#HTML(html_container)
]]>
                </code>
              </script>
            </scriptExecutable>
          </task>
          <task name="BingNews">
            <description>
              <![CDATA[ This task wraps the Bing News Search API of Microsoft which provides an experience similar to Bing.com/search by returning search results that Bing determines are relevant to a user's query. The results include Web pages and may also include images, videos, and more. The task requires this third-party credential : $BING_SEARCH_API_KEY which provides access to this API. Please refer to the User documentation to learn how to add third-party credentials.
$COUNT (optional) is the number of news articles to return in the response.
$FRESHNESS (optional) filter news articles by {"Day", "Week", "Month"}.
$MARKET is the market where the results come from. Market codes are listed here: https://docs.microsoft.com/en-us/rest/api/cognitiveservices/bing-news-api-v7-reference#market-codes
$CATEGORY (optional) is the category of articles to return. For example, Sports articles or Entertainment articles.
$SORTED_BY (optional) is the order to return the trending topics in. For example, sorted by "Date"
The task's output $BING_NEWS_SEARCH_OUTPUT is the result of the API call in a JSON format. ]]>
            </description>
            <variables>
              <variable name="SEARCH_TERM" value="activeeon" inherited="true" />
              <variable name="COUNT" value="-1" inherited="true" model="PA:Integer"/>
              <variable name="FRESHNESS" value="" inherited="true" />
              <variable name="MARKET" value="" inherited="true" />
              <variable name="CATEGORY" value="" inherited="true" />
              <variable name="SORT_BY" value="" inherited="true" />
              <variable name="OUTPUT_FORMAT" value="HTML" inherited="true" model="PA:LIST(CSV, HTML)"/>
            </variables>
            <genericInformation>
              <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_bing_news_search.svg"/>
            </genericInformation>
            <forkEnvironment >
              <envScript>
                <script>
                  <code language="python">
                    <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
                  </code>
                </script>
              </envScript>
            </forkEnvironment>
            <pre>
              <script>
                <code language="bash">
                  <![CDATA[

]]>
                </code>
              </script>
            </pre>
            <scriptExecutable>
              <script>
                <code language="cpython">
                  <![CDATA[
import requests
import json
import urllib
from pprint import pprint
import pandas as pd
import re

# You can customize the api server location
#api_location="westcentralus"

# Congitive Services - Bing Video Search API URL:
#bing_news_search_url = "https://{0}.api.cognitive.microsoft.com/bing/v7.0/news/search".format(api_location)
bing_news_search_url = "https://api.cognitive.microsoft.com/bing/v7.0/news/search"

# READ TASK VARIABLES
if 'variables' in locals():
    if variables.get("SEARCH_TERM") is not None:
        SEARCH_TERM = variables.get("SEARCH_TERM")
    else:
        print("You first need to specify the search term")
        sys.exit(1)
    if variables.get("COUNT") is not None:
        COUNT = int(variables.get("COUNT"))
    if variables.get("FRESHNESS") is not None:
        FRESHNESS = variables.get("FRESHNESS")
    if variables.get("MARKET") is not None:
        MARKET = variables.get("MARKET")
    if variables.get("CATEGORY") is not None:
        CATEGORY = variables.get("CATEGORY")
    if variables.get("SORT_BY") is not None:
        SORT_BY = variables.get("SORT_BY")
    if variables.get("OUTPUT_FORMAT") is not None:
        OUTPUT_FORMAT = variables.get("OUTPUT_FORMAT")
    # Provide a valid subscription API token
    if credentials.get("BING_SEARCH_API_KEY") is not None:
        subscription_key = credentials.get("BING_SEARCH_API_KEY")
    else:
        print("You first need to add your Azure Cognitive Services API key to the third party credentials")
        sys.exit(1)

# Set API request parameters
#params  = {"q": SEARCH_TERM, "count":COUNT, "mkt": MARKET_CODE, "freshness": FRESHNESS, "category":CATEGORY}
params={'q':SEARCH_TERM}
if COUNT >0:
    params['count'] = COUNT
# Market Code: https://docs.microsoft.com/en-us/rest/api/cognitiveservices/bing-news-api-v7-reference#market-codes
if MARKET is not None and len(MARKET)>0:
    params['mkt'] = MARKET
# Freshness values: 'Day', 'Week', 'Month'
if FRESHNESS is not None and len(FRESHNESS)>0:
    params['freshness'] = FRESHNESS
if CATEGORY is not None and len(CATEGORY)>0:
    params['category'] = CATEGORY
if SORT_BY is not None and len(SORT_BY)>0:
    params['sortBy'] = SORT_BY

# Send API request
headers   = {
    "Ocp-Apim-Subscription-Key": subscription_key,
    'Content-Type': 'text/plain'
}
response = requests.get(bing_news_search_url, headers=headers, params=params)
response.raise_for_status()

# Get a JSON response
search_results = response.json()

# Print the results
#pprint(search_results)

if 'variables' in locals():
    variables.put('BING_NEWS_SEARCH_OUTPUT', search_results)

print("BEGIN Export_Results")

OUTPUT_DATA = search_results["value"]
table = []
for document in OUTPUT_DATA:
    try:
        image=document["image"]
    except KeyError:
        video=None
        pass
    description= document["description"]
    name= document["name"]
    url= document["url"]
    table.append("""<tr><td>{0}</td><td>{1}</td><td><a href="{2}">{2}</a></td>""".format(name, description, url))

css_style="""table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%;
}
td {
  border: 1px solid #999999;
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  border-left: 2px solid #999999;
}"""
html = ("""<table><tr><th>Title</th><th>Snippet</th><th>Url</th></tr>{0}</table>""").format("\n".join(table))
html_container="""<!DOCTYPE html>
                  <html>
                    <head>
                      <meta charset="UTF-8">
                        <meta name="description" content="Bing Video Search API Results">
                          <style>{0}</style>
                        </head>
                        <body>{1}</body></html>""".format(css_style,html)

if OUTPUT_DATA != None and 'resultMetadata' in locals(): 
    dataframe=pd.read_html(html_container,header=0, encoding='utf-8')[0]
    
    if OUTPUT_FORMAT == 'CSV':
        result = dataframe.to_csv(index=False).encode('utf-8')
        resultMetadata.put("file.extension", ".csv")
        resultMetadata.put("file.name", "result.csv")
        resultMetadata.put("content.type", "text/csv")
    elif OUTPUT_FORMAT == 'HTML':
        result = html_container.encode('utf-8')
        resultMetadata.put("file.extension", ".html")
        resultMetadata.put("file.name", "result.html")
        resultMetadata.put("content.type", "text/html")
    print("END Export_Results")  
else:
    print('It is not possible to export the data')

# Uncomment this to render the HTML result locally in your python notebook
#from IPython.display import HTML
#HTML(html_container)
]]>
                      </code>
                    </script>
                  </scriptExecutable>
                </task>
                <task name="NewsTextExtractor">
                  <description>
                    <![CDATA[ The simplest task, ran by a python engine. ]]>
                  </description>
                  <depends>
                    <task ref="BingNews"/>
                  </depends>
                  <scriptExecutable>
                    <script>
                      <code language="cpython">
                        <![CDATA[
import re
import json

print(variables.get("BING_NEWS_SEARCH_OUTPUT"))
OUTPUT_DATA=variables.get("BING_NEWS_SEARCH_OUTPUT")["value"]
text_json={'documents':[]}
for document in OUTPUT_DATA:
    description=document["description"]
    news_url=document["url"]
    language=re.search(r"([\w]{2})-[\w]+", variables.get("MARKET")).group(1)
    text_json['documents'].append({'id':news_url, 'language':language, 'text': description})
    

variables.put("DOCUMENTS_JSON", json.dumps(text_json))
result=json.dumps(text_json)  
print(result)
#print("image_urls= ", image_urls)
]]>
                      </code>
                    </script>
                  </scriptExecutable>
                </task>
              </taskFlow>
            </job>