<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="1" name="MLflow_Training_Example" onTaskError="continueJobExecution" priority="normal" projectName="7. Templates LXP MeluXina" tags="mlflow" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="true" description="If not empty, the workflow tasks will be run only on nodes belonging to the specified node source." group="Resource Management" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/nodesources)" name="NODE_SOURCE" value=""/>
    <variable advanced="true" description="Parameters given to the native scheduler (SLURM, LSF, etc) while requesting a ProActive node used to deploy the workflow tasks." group="Resource Management" name="NATIVE_SCHEDULER_PARAMS" value=""/>
    <variable advanced="true" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." group="Resource Management" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/tokens)" name="NODE_ACCESS_TOKEN" value=""/>
    <variable advanced="false" description="Container platform used for executing the workflow tasks." group="Container Parameters" model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable advanced="false" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" model="PA:LIST(docker://activeeon/mlflow:latest)" name="CONTAINER_IMAGE" value="docker://activeeon/mlflow:latest"/>
    <variable advanced="false" description="If True, it will activate the use of GPU for the workflow tasks on the selected container platform." group="Container Parameters" model="PA:Boolean" name="CONTAINER_GPU_ENABLED" value="false"/>
    <variable advanced="false" description="It specifies the docker image to be used for the MLFlow Server." group="MLFlow Server Parameters" hidden="false" model="PA:LIST(activeeon/mlflow:latest)" name="MLFLOW_SERVER_DOCKER_IMAGE" value="activeeon/mlflow:latest"/>
    <variable advanced="false" description="If specified, it specifies the port number for the MLFlow Server (default port is 5000)." group="MLFlow Server Parameters" hidden="false" model="PA:INTEGER" name="MLFLOW_SERVER_PORT" value="-1"/>
    <variable advanced="false" description="Name of the Docker container running the MLflow server." group="MLFlow Server Parameters" hidden="false" name="MLFLOW_SERVER_CONTAINER_NAME" value="mlflow-tracking-server-${PA_JOB_ID}"/>
    <variable advanced="true" description="If not empty, the workflow tasks will be run only on nodes belonging to the specified node source." group="MLFlow Server Parameters" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/nodesources)" name="MLFLOW_SERVER_NODE_SOURCE" value=""/>
    <variable advanced="true" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." group="MLFlow Server Parameters" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/tokens)" name="MLFLOW_SERVER_NODE_ACCESS_TOKEN" value=""/>
  </variables>
  <description>
    <![CDATA[ A workflow that demonstrates machine learning model training with MLflow integration. The workflow consists of three main tasks:

1. MLflow Server Deployment:
   - Starts an MLflow tracking server in a Docker container
   - Configurable server parameters (port, image)
   - Automatic endpoint registration with the job

2. Model Training:
   - Trains multiple regression models using scikit-learn
   - Models include: Decision Tree, Random Forest, SVM, KNN, and Ridge Regression
   - Logs metrics (MSE, RÂ²), parameters, and artifacts to MLflow
   - Generates and stores visualization artifacts
   - Runs in a containerized environment with customizable settings

3. Cleanup and Resource Management:
   - Proper server shutdown and container cleanup
   - Automatic endpoint deregistration
   - Signal-based workflow control

Key Features:
- Containerized execution for both server and training tasks
- Configurable resource allocation and node selection
- Support for GPU acceleration (when enabled)
- Comprehensive logging and experiment tracking
- Clean resource management and shutdown ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-deep-learning-workflows"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/lxp_logo.svg"/>
<info name="documentation" value="PAIO/PAIOUserGuide.html"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="Start_MLflow_Tracking_Server">
      <description>
        <![CDATA[ A task that initializes and starts the MLflow tracking server. It:
- Deploys an MLflow server in a Docker container
- Configures server settings (port, host, database)
- Creates an external endpoint for access
- Sets up storage for experiment tracking
- Provides endpoint information to subsequent tasks

Key Features:
- Dynamic port configuration
- Persistent SQLite storage
- Artifact storage configuration
- Container lifecycle management
- Automatic endpoint registration ]]>
      </description>
      <variables>
        <variable advanced="false" description="It specifies the docker image to be used for the MLFlow Server." hidden="false" inherited="true" name="MLFLOW_SERVER_DOCKER_IMAGE" value="activeeon/mlflow:latest"/>
        <variable advanced="false" description="If specified, it specifies the port number for the MLFlow Server (default port is 5000)." hidden="false" inherited="true" model="PA:INTEGER" name="MLFLOW_SERVER_PORT" value="-1"/>
        <variable advanced="false" description="Name of the Docker container running the MLflow server." hidden="false" inherited="true" name="MLFLOW_SERVER_CONTAINER_NAME" value="mlflow-tracking-server-${PA_JOB_ID}"/>
        <variable advanced="false" description="If not empty, the workflow tasks will be run only on nodes belonging to the specified node source." group="Resource Management" inherited="true" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/nodesources)" name="MLFLOW_SERVER_NODE_SOURCE" value=""/>
        <variable advanced="false" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." group="Resource Management" inherited="true" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/tokens)" name="MLFLOW_SERVER_NODE_ACCESS_TOKEN" value=""/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mlflow_logo.png"/>
        <info name="TASK.DOCUMENTATION" value="PAIO/PAIOUserGuide.html"/>
        <info name="NODE_SOURCE" value="${MLFLOW_SERVER_NODE_SOURCE}"/>
        <info name="NODE_ACCESS_TOKEN" value="${MLFLOW_SERVER_NODE_ACCESS_TOKEN}"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import com.google.common.base.Splitter;

schedulerapi.connect()

jobId = variables.get("PA_JOB_ID") as String
println("jobId: " + jobId)

hostname = variables.get("PA_NODE_HOST") as String
println("hostname: " + hostname)

port = variables.get("MLFLOW_SERVER_PORT") as String
println("port: " + port)

// If port is -1, use a default port (e.g., 5000)
if (port == "-1") {
    port = "5000"
    println("Using default port: " + port)
}

docker_image = variables.get("MLFLOW_SERVER_DOCKER_IMAGE") as String
println("docker_image: " + docker_image)

// Get container name from variables
containerName = variables.get("MLFLOW_SERVER_CONTAINER_NAME") as String
println("containerName: " + containerName)

endpointName = "MLflow Tracking Server"
println("endpointName: " + endpointName)

externalEndpointUrl = "http://" + hostname + ":" + port + "/"
println("externalEndpointUrl: " + externalEndpointUrl)

endpointIconUri = "/automation-dashboard/styles/patterns/img/wf-icons/lxp_logo.svg"
println("endpointIconUri: " + endpointIconUri)

// Start MLflow container
def startMLflow = ["docker", "run", "-d", "--rm", "-p", port+":5000", "--name", containerName, 
                   docker_image, "/bin/bash", "-c", 
                   "mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mydb.sqlite --default-artifact-root /app/mlartifacts"]
println("docker command: " + startMLflow)
def process = startMLflow.execute()
def exitCode = process.waitFor()

if (exitCode != 0) {
    println "Error starting MLflow container: ${process.err.text}"
    System.exit(1)
}

println "MLflow container started successfully"

// Add an external endpoint URL to the job
try {
    println("Adding external endpoint URL...")
    schedulerapi.addExternalEndpointUrl(jobId, endpointName, externalEndpointUrl, endpointIconUri)
    println("External endpoint URL added successfully")
} catch (Exception e) {
    println("Error adding external endpoint URL: " + e.message)
    e.printStackTrace()
}

variables.put("MLFLOW_SERVER_ENDPOINT_NAME", endpointName)
variables.put("MLFLOW_SERVER_ENDPOINT_URL", externalEndpointUrl)
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            154.1328125
        </positionTop>
        <positionLeft>
            249.609375
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Stop_MLflow_Tracking_Server">
      <description>
        <![CDATA[ A task responsible for graceful shutdown and cleanup of MLflow resources. It:
- Waits for the training task completion
- Handles stop signals for controlled shutdown
- Removes registered endpoints
- Stops the MLflow container
- Ensures proper resource cleanup

Features:
- Signal-based control flow
- Proper container cleanup
- Endpoint deregistration
- Error handling for cleanup operations
- Coordinated shutdown sequence ]]>
      </description>
      <variables>
        <variable advanced="true" description="List of comma-separated signals expected by this task." hidden="false" inherited="false" model="PA:REGEXP(((\w|-|_)+,?\s?)+)" name="SIGNALS" value="Stop_MLflow_Tracking_Server"/>
        <variable advanced="false" description="Name of the Docker container running the MLflow server." hidden="false" inherited="true" name="MLFLOW_SERVER_CONTAINER_NAME" value="mlflow-tracking-server-${PA_JOB_ID}"/>
        <variable advanced="false" description="If not empty, the workflow tasks will be run only on nodes belonging to the specified node source." group="Resource Management" inherited="true" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/nodesources)" name="MLFLOW_SERVER_NODE_SOURCE" value=""/>
        <variable advanced="false" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." group="Resource Management" inherited="true" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/tokens)" name="MLFLOW_SERVER_NODE_ACCESS_TOKEN" value=""/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mlflow_logo.png"/>
        <info name="TASK.DOCUMENTATION" value="PAIO/PAIOUserGuide.html"/>
        <info name="NODE_SOURCE" value="${MLFLOW_SERVER_NODE_SOURCE}"/>
        <info name="NODE_ACCESS_TOKEN" value="${MLFLOW_SERVER_NODE_ACCESS_TOKEN}"/>
      </genericInformation>
      <depends>
        <task ref="MLFlow_Training_Script"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import com.google.common.base.Splitter;

schedulerapi.connect()

jobId = variables.get("PA_JOB_ID") as String
println("jobId: " + jobId)

// Get container name from variables
containerName = variables.get("MLFLOW_SERVER_CONTAINER_NAME") as String
println("containerName: " + containerName)

endpointName = variables.get("MLFLOW_SERVER_ENDPOINT_NAME")
println("endpointName: " + endpointName)

externalEndpointUrl = variables.get("MLFLOW_SERVER_ENDPOINT_URL")
println("externalEndpointUrl: " + externalEndpointUrl)


// Read the variable SIGNALS
signals = variables.get("SIGNALS")

// Split the value of the variable SIGNALS and transform it into a list
Set signalsSet = new HashSet<>(Splitter.on(',').trimResults().omitEmptyStrings().splitToList(signals))

// Send a ready notification for each signal in the set
println("Ready for signals "+ signalsSet)
signalsSet.each { signal ->
    signalapi.readyForSignal(signal)
}

// Wait until one signal among those specified is received
println("Waiting for any signal among "+ signalsSet)
receivedSignal = signalapi.waitForAny(signalsSet)

// Remove ready signals
signalapi.removeManySignals(new HashSet<>(signalsSet.collect { signal -> "ready_"+signal }))

// Display the received signal and add it to the job result
println("Received signal: "+ receivedSignal)
result = receivedSignal

// Remove the external endpoint URL
try {
    println("Removing external endpoint URL...")
    schedulerapi.removeExternalEndpointUrl(jobId, endpointName)
    println("Removed external endpoint URL")
} catch (Exception e) {
    println("Error removing external endpoint URL: " + e.message)
    // Continue with container cleanup
}

try {
    // Stop MLflow container
    def stopMLflow = ["docker", "stop", containerName]
    process = stopMLflow.execute()
    exitCode = process.waitFor()

    if (exitCode != 0) {
        println "Error stopping MLflow container: ${process.err.text}"
        System.exit(1)
    }

    println "MLflow container stopped successfully"
} catch (Exception e) {
    println "Error during cleanup: ${e.message}"
    System.exit(1)
}
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            363
        </positionTop>
        <positionLeft>
            272
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="MLFlow_Training_Script">
      <description>
        <![CDATA[ A task that demonstrates MLflow experiment tracking capabilities. This task:
- Uses scikit-learn to train multiple regression models
- Tracks experiments using MLflow's Python API
- Logs model parameters, metrics, and artifacts
- Generates performance visualizations
- Creates model comparisons

Models included:
- Decision Tree
- Random Forest
- Support Vector Machine (SVM)
- K-Nearest Neighbors (KNN)
- Ridge Regression

Metrics tracked:
- Mean Squared Error (MSE)
- R-squared (RÂ²)
- Model parameters and hyperparameters ]]>
      </description>
      <variables>
        <variable advanced="false" description="Name of the container image being used to run the workflow tasks." hidden="false" inherited="true" name="CONTAINER_IMAGE" value="activeeon/mlflow:latest"/>
        <variable advanced="false" description="If True, it will activate the use of GPU for the workflow tasks on the selected container platform." hidden="false" inherited="true" model="PA:Boolean" name="CONTAINER_GPU_ENABLED" value="False"/>
        <variable advanced="false" description="Container platform used for executing the workflow tasks." hidden="false" inherited="true" model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
        <variable advanced="false" description="If not empty, the workflow tasks will be run only on nodes belonging to the specified node source." hidden="false" inherited="true" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/nodesources)" name="NODE_SOURCE" value=""/>
        <variable advanced="false" description="Parameters given to the native scheduler (SLURM, LSF, etc) while requesting a ProActive node used to deploy the workflow tasks." hidden="false" inherited="true" name="NATIVE_SCHEDULER_PARAMS" value=""/>
        <variable advanced="false" description="If not empty, the workflow tasks will be run only on nodes that contains the specified token." hidden="false" inherited="true" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/tokens)" name="NODE_ACCESS_TOKEN" value=""/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mlflow_logo.png"/>
        <info name="NODE_SOURCE" value="${NODE_SOURCE}"/>
        <info name="NODE_ACCESS_TOKEN" value="${NODE_ACCESS_TOKEN}"/>
        <info name="NS_BATCH" value="${NATIVE_SCHEDULER_PARAMS}"/>
        <info name="PYTHON_COMMAND" value="python3"/>
      </genericInformation>
      <depends>
        <task ref="Start_MLflow_Tracking_Server"/>
      </depends>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pathlib
import mlflow
import mlflow.sklearn
from mlflow.models import infer_signature
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import make_regression

def simulate():
    # Create simulated regression data
    X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    return X_train, X_test, y_train, y_test

externalEndpointUrl = variables.get("MLFLOW_SERVER_ENDPOINT_URL")

# Set the MLflow tracking URI (replace with your server address)
# mlflow.set_tracking_uri("http://your-remote-server-address:8080")
mlflow.set_tracking_uri(externalEndpointUrl)
print("Tracking URI set to:", mlflow.get_tracking_uri())

# Create and set the experiment
mlflow.create_experiment('my_first_experiment')
mlflow.set_experiment('my_first_experiment')

# Generate the data
X_train, X_test, y_train, y_test = simulate()

# Define models to train
models = {
    "Decision Tree": DecisionTreeRegressor(max_depth=5),
    "Random Forest": RandomForestRegressor(n_estimators=100, max_depth=5),
    "Support Vector Machine (SVM)": SVR(C=1.0, kernel='linear', epsilon=0.1),
    "K-Nearest Neighbors": KNeighborsRegressor(n_neighbors=5),
    "Ridge Regression": Ridge(alpha=1.0)
}

# Train and log individual models
for model_name, model in models.items():
    with mlflow.start_run():
        print(model_name)
        
        # Create and save feature scatter plot
        fig, ax = plt.subplots()
        ax.scatter(X_train[0,:], X_train[1,:])
        ax.set_title("Feature Scatter Plot", fontsize=14)
        plt.tight_layout()
        save_path = pathlib.Path("/tmp/scatter_plot.png")
        fig.savefig(save_path)
        
        # Train the model
        model.fit(X_train, y_train)
        signature = infer_signature(X_train, model.predict(X_train))
        
        # Make predictions
        y_pred = model.predict(X_test)
        
        # Calculate Mean Squared Error (MSE) and RÂ²
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # Log the model to MLflow
        mlflow.sklearn.log_model(model, model_name, signature=signature)
        
        # Log hyperparameters and metrics
        mlflow.log_param("model_type", model_name)
        mlflow.log_param("parameters", model.get_params())
        mlflow.log_metric("mse", mse)
        mlflow.log_metric("r2", r2)
        mlflow.log_artifact("/tmp/scatter_plot.png")

# Compare all models in a single run
model_comparison = {
    "Decision Tree": {"mse": 0, "r2": 0},
    "Random Forest": {"mse": 0, "r2": 0},
    "Support Vector Machine (SVM)": {"mse": 0, "r2": 0},
    "K-Nearest Neighbors": {"mse": 0, "r2": 0},
    "Ridge Regression": {"mse": 0, "r2": 0}
}

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    model_comparison[model_name]["mse"] = mse
    model_comparison[model_name]["r2"] = r2

with mlflow.start_run():
    # Convert to DataFrame for visualization
    comparison_df = pd.DataFrame(model_comparison).T
    comparison_df.plot(kind="bar", figsize=(10, 6), title="Model Comparison: MSE and RÂ²")
    plt.ylabel("Value")
    plt.savefig("/tmp/model_comparison.png")
    mlflow.log_artifact("/tmp/model_comparison.png")
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            255
        </positionTop>
        <positionLeft>
            292.5
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2464px;
            height:3428px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-149.1328125px;left:-244.609375px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_908" style="top: 154.133px; left: 249.609px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A task that initializes and starts the MLflow tracking server. It:
- Deploys an MLflow server in a Docker container
- Configures server settings (port, host, database)
- Creates an external endpoint for access
- Sets up storage for experiment tracking
- Provides endpoint information to subsequent tasks

Key Features:
- Dynamic port configuration
- Persistent SQLite storage
- Artifact storage configuration
- Container lifecycle management
- Automatic endpoint registration"><img src="https://github.com/mlflow-automation.png" width="20px">&nbsp;<span class="name">Start_MLflow_Tracking_Server</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i title="Workflows being Called by this Task" id="called-icon"></i></a><a title="Scripts being Called by this Task" id="reference-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: -7px; right: 3px;"><i id="reference-icon"></i></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_911" style="top: 363px; left: 272px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A task responsible for graceful shutdown and cleanup of MLflow resources. It:
- Waits for the training task completion
- Handles stop signals for controlled shutdown
- Removes registered endpoints
- Stops the MLflow container
- Ensures proper resource cleanup

Features:
- Signal-based control flow
- Proper container cleanup
- Endpoint deregistration
- Error handling for cleanup operations
- Coordinated shutdown sequence"><img src="https://github.com/mlflow-automation.png" width="20px">&nbsp;<span class="name">Stop_MLflow_Tracking_Server</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i title="Workflows being Called by this Task" id="called-icon"></i></a><a title="Scripts being Called by this Task" id="reference-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: -7px; right: 3px;"><i id="reference-icon"></i></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_914" style="top: 255px; left: 292.5px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A task that demonstrates MLflow experiment tracking capabilities. This task:
- Uses scikit-learn to train multiple regression models
- Tracks experiments using MLflow's Python API
- Logs model parameters, metrics, and artifacts
- Generates performance visualizations
- Creates model comparisons

Models included:
- Decision Tree
- Random Forest
- Support Vector Machine (SVM)
- K-Nearest Neighbors (KNN)
- Ridge Regression

Metrics tracked:
- Mean Squared Error (MSE)
- R-squared (RÂ²)
- Model parameters and hyperparameters"><img src="https://github.com/mlflow-automation.png" width="20px">&nbsp;<span class="name">MLFlow_Training_Script</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i title="Workflows being Called by this Task" id="called-icon"></i></a><a title="Scripts being Called by this Task" id="reference-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: -7px; right: 3px;"><i id="reference-icon" class="glyphicon glyphicon-list-alt"></i></a></div><svg style="position:absolute;left:350px;top:294.5px" width="27.5" height="69" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 68 C -10 18 16.5 50 6.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style="--darkreader-inline-stroke: #a8a095;" data-darkreader-inline-stroke=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.2703121250000002,50.922175500000016 L6.932165541145093,31.835153605421993 L-0.6975602300067265,37.010797010547755 L-6.979212948307168,30.26240171042872 L-2.2703121250000002,50.922175500000016" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)" data-darkreader-inline-fill="" data-darkreader-inline-stroke="" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: #a8a095;"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.2703121250000002,50.922175500000016 L6.932165541145093,31.835153605421993 L-0.6975602300067265,37.010797010547755 L-6.979212948307168,30.26240171042872 L-2.2703121250000002,50.922175500000016" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)" data-darkreader-inline-fill="" data-darkreader-inline-stroke="" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: #a8a095;"></path></svg><svg style="position:absolute;left:328px;top:193.5px" width="49.5" height="62" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 28.5 61 C 38.5 11 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style="--darkreader-inline-stroke: #a8a095;" data-darkreader-inline-stroke=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M29.446125000000002,43.90675000000001 L26.302471729098553,22.95162069683534 L22.911992288279414,31.525105936886945 L13.920827665985488,29.48575340855593 L29.446125000000002,43.90675000000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)" data-darkreader-inline-fill="" data-darkreader-inline-stroke="" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: #a8a095;"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M29.446125000000002,43.90675000000001 L26.302471729098553,22.95162069683534 L22.911992288279414,31.525105936886945 L13.920827665985488,29.48575340855593 L29.446125000000002,43.90675000000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)" data-darkreader-inline-fill="" data-darkreader-inline-stroke="" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: #a8a095;"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 328.5px; top: 184px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 350.5px; top: 393px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 350.5px; top: 353px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 357px; top: 285px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 357px; top: 245px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
