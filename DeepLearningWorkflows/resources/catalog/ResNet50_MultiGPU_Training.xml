<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="ResNet50_MultiGPU_Training" onTaskError="continueJobExecution" priority="normal" projectName="7. Templates LXP MeluXina" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="false" description="Account/project number for job billing and resource tracking" group="SLURM JOB PARAMS" hidden="false" name="PROJECT_ACCOUNT" value="[p200xxx-replace-with-your-project-number]"/>
    <variable advanced="false" description="Total number of compute nodes allocated to this job" group="SLURM RESOURCE ALLOCATION" hidden="false" model="PA:LIST(1,2,4,6,8)" name="NODE_COUNT" value="4"/>
    <variable advanced="false" description="Number of processes to run on each allocated node" group="SLURM RESOURCE ALLOCATION" hidden="false" model="PA:LIST(1,2,4,6,8)" name="TASKS_PER_NODE" value="1"/>
    <variable advanced="false" description="Specifies which partition/queue to run the job on (here: GPU partition)" group="SLURM SCHEDULING PARAMETERS" hidden="false" name="PARTITION_NAME" value="gpu"/>
    <variable advanced="false" description="Defines the QOS (Quality of Service) level for the job" group="SLURM SCHEDULING PARAMETERS" hidden="false" name="QUEUE_TYPE" value="default"/>
    <variable advanced="true" description="Maximum time limit for job completion (format: HH:MM:SS)" group="SLURM SCHEDULING PARAMETERS" hidden="false" name="WALL_TIME" value="01:10:00"/>
    <variable advanced="true" description="Path to file for capturing standard output (%x=job name, %j=job ID)" group="SLURM JOB PARAMS" hidden="false" name="OUTPUT_FILE" value="%x%j.out"/>
    <variable advanced="true" description="Path to file for capturing standard error (%x=job name, %j=job ID)" group="SLURM JOB PARAMS" hidden="false" name="ERROR_FILE" value="%x%j.err"/>
    <variable advanced="false" description="CPUs allocated per task" group="SLURM EXECUTION PARAMETERS" hidden="false" model="PA:LIST(1,2,4,6,8,10,12,14,16)" name="CPU_PER_TASK" value="8"/>
    <variable advanced="false" description="Number of GPUs per node" group="GPU CONFIGURATION" hidden="false" model="PA:LIST(1,2,4,6,8)" name="GPU_COUNT" value="4"/>
    <variable advanced="false" description="List of GPU devices to use" group="GPU CONFIGURATION" hidden="false" name="GPU_DEVICES" value="0,1,2,3"/>
    <variable advanced="false" description="Number of tasks per node" group="SLURM EXECUTION PARAMETERS" hidden="false" model="PA:LIST(1,2,4,6,8)" name="NODE_TASKS" value="1"/>
    <variable advanced="true" description="Maximum wait time for task startup" group="SLURM EXECUTION PARAMETERS" hidden="false" model="PA:Integer" name="WAIT_TIMEOUT" value="60"/>
    <variable advanced="true" description="Kill job on any task failure" group="SLURM EXECUTION PARAMETERS" hidden="false" model="PA:LIST(0,1)" name="KILL_POLICY" value="1"/>
    <variable advanced="true" description="Maximum number of restart attempts" group="DISTRIBUTED TRAINING SETUP" hidden="false" model="PA:Integer" name="MAX_RESTARTS" value="3"/>
    <variable advanced="true" description="Backend for distributed communication" group="DISTRIBUTED TRAINING SETUP" hidden="false" name="DIST_BACKEND" value="c10d"/>
    <variable advanced="true" description="Network interface for NCCL communication" group="DISTRIBUTED TRAINING SETUP" hidden="false" name="NCCL_SOCKET_IFNAME" value="ib0"/>
    <variable advanced="true" description="Enable async error handling for NCCL" group="DISTRIBUTED TRAINING SETUP" hidden="false" name="NCCL_ASYNC_ERROR_HANDLING" value="1"/>
    <variable advanced="true" description="Number of OpenMP threads" group="DISTRIBUTED TRAINING SETUP" hidden="false" name="OMP_NUM_THREADS" value="8"/>
    <variable advanced="false" description="Total training epochs" group="TRAINING PARAMETERS" hidden="false" model="PA:Integer" name="MODEL_EPOCHS" value="4"/>
    <variable advanced="false" description="Checkpoint save frequency" group="TRAINING PARAMETERS" hidden="false" model="PA:Integer" name="SAVE_FREQUENCY" value="1"/>
    <variable advanced="false" description="Training batch size" group="TRAINING PARAMETERS" hidden="false" model="PA:Integer" name="BATCH_SIZE" value="32"/>
    <variable advanced="false" description="Name of model checkpoint file" group="CHECKPOINT MANAGEMENT" hidden="false" name="CHECKPOINT_FILE" value="${PA_JOB_ID}/snapshot.pt"/>
    <variable advanced="false" description="Whether to remove existing checkpoint" group="CHECKPOINT MANAGEMENT" hidden="false" model="PA:Boolean" name="CLEAN_CHECKPOINT" value="true"/>
    <variable advanced="false" description="Enable/disable detailed logging" group="DEBUG AND LOGGING" hidden="false" model="PA:Boolean" name="DEBUG_MODE" value="true"/>
    <variable advanced="true" description="Set logging verbosity" group="DEBUG AND LOGGING" hidden="false" name="LOG_LEVEL" value="INFO"/>
    <variable advanced="true" description="C++ logging level" group="DEBUG AND LOGGING" hidden="false" name="CPP_LOG_LEVEL" value="INFO"/>
    <variable advanced="true" description="Directory for torch logs" group="DEBUG AND LOGGING" hidden="false" name="LOG_DIRECTORY" value="log_torch"/>
    <variable advanced="false" description="Name of the native scheduler (if any) that will execute the current experiment" group="RUNTIME PARAMETERS" hidden="false" model="PA:MODEL_FROM_URL(${PA_SCHEDULER_REST_PUBLIC_URL}/rm/model/nodesources?infrastructure=%5E(%3F!(Default)).*$)" name="TARGET_INFRASTRUCTURE" value=""/>
    <variable advanced="true" description="Parameters to be passed to the native scheduler (if any) that will execute the current experiment" group="RUNTIME PARAMETERS" hidden="false" name="SLURM_SBATCH_PARAMS" value="--nodes=${NODE_COUNT} --ntasks-per-node=${TASKS_PER_NODE} --output=${OUTPUT_FILE} --error=${ERROR_FILE} -p ${PARTITION_NAME} -q ${QUEUE_TYPE} -A ${PROJECT_ACCOUNT}"/>
  </variables>
  <description>
    <![CDATA[ #### PyTorch Distributed Training Workflow (MWE)

This workflow offers a **MWE (Minimal Working Example)** of a PyTorch Distributed Training on multi-node/multi-GPU.

It distributes a **Resnet50 training** (from the `torchvision` module) with PyTorch over several GPUs. It also demonstrates the integration of **SLURM** with PyTorch for distributed training across GPUs on **LXP Meluxina**. It might be used as a basis for users willing to move their PyTorch training script from serial to distributed.

---

##### **Features**:
- **Standard and Advanced Parameters**: 
  Use these to easily test various configurations (CPUs, GPUs, SLURM parameters, etc.).
- **Dynamic Infrastructure Selection**: 
  If you have access to several infrastructures, you can easily select the one you want to use with the dynamic variable `TARGET_INFRASTRUCTURE`.
- **Script Editing**: 
  You can easily edit the 2 main scripts in the Studio (`Right Click > Open in Studio`).

---

##### **Outputs**:
The workflow produces a binary snapshot file that are stored in locally with the **Job-Id** added in the path, allowing you to easily track your various experiments. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-deep-learning-workflows"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/lxp_logo.svg"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="ResNet50_MultiGPU_Training">
      <description>
        <![CDATA[ #### PyTorch Distributed Training Workflow (MWE)

This workflow offers a **MWE (Minimal Working Example)** of a PyTorch Distributed Training on multi-node/multi-GPU.

It distributes a **Resnet50 training** (from the `torchvision` module) with PyTorch over several GPUs. It also demonstrates the integration of **SLURM** with PyTorch for distributed training across GPUs on **LXP Meluxina**. It might be used as a basis for users willing to move their PyTorch training script from serial to distributed.

---

##### **Features**:
- **Standard and Advanced Parameters**: 
  Use these to easily test various configurations (CPUs, GPUs, SLURM parameters, etc.).
- **Dynamic Infrastructure Selection**: 
  If you have access to several infrastructures, you can easily select the one you want to use with the dynamic variable `TARGET_INFRASTRUCTURE`.
- **Script Editing**: 
  You can easily edit the 2 main scripts in the Studio (`Right Click > Open in Studio`).

---

##### **Outputs**:
The workflow produces a binary snapshot file that are stored in locally on ${PWD}/${CHECKPOINT_FILE}. ]]>
      </description>
      <variables>
        <variable advanced="false" description="Temporally file path where the task pre-scrit will be stored for execution." hidden="false" inherited="false" name="SCRIPT_PATH" value="resnet50_LXP.py"/>
      </variables>
      <genericInformation>
        <info name="PRE_SCRIPT_AS_FILE" value="${SCRIPT_PATH}"/>
        <info name="NODE_SOURCE" value="${TARGET_INFRASTRUCTURE}"/>
        <info name="NS_BATCH" value="${SLURM_SBATCH_PARAMS}"/>
        <info name="DISABLE_PTK" value="true"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/lxp_logo.svg"/>
      </genericInformation>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
# from datautils import MyTrainDataset

import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import torch.distributed as dist
import os
from torchvision import models

def ddp_setup():
    init_process_group(backend="nccl")
    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))
    world_size = dist.get_world_size()
    rank = dist.get_rank()
    return world_size, rank


class Trainer:
    def __init__(
        self,
        model: torch.nn.Module,
        train_data: DataLoader,
        optimizer: torch.optim.Optimizer,
        save_every: int,
        snapshot_path: str,
    ) -> None:

        self.local_rank = int(os.environ["LOCAL_RANK"])
        self.global_rank = int(os.environ["RANK"])
        self.model = model.to(self.local_rank)
        self.train_data = train_data
        self.optimizer = optimizer
        self.save_every = save_every
        self.epochs_run = 0
        self.snapshot_path = snapshot_path
        if os.path.exists(snapshot_path):
            print("Loading snapshot")
            self._load_snapshot(snapshot_path)

        self.model = DDP(self.model, device_ids=[self.local_rank])

    def _load_snapshot(self, snapshot_path):
        loc = f"cuda:{self.local_rank}"
        snapshot = torch.load(snapshot_path, map_location=loc)
        self.model.load_state_dict(snapshot["MODEL_STATE"])
        self.epochs_run = snapshot["EPOCHS_RUN"]
        print(f"Resuming training from snapshot at Epoch {self.epochs_run}")

    def _run_batch(self, source, targets):
        self.optimizer.zero_grad()
        output = self.model(source)
        loss = F.cross_entropy(output, targets)
        loss.backward()
        self.optimizer.step()

    def _run_epoch(self, epoch):
        b_sz = len(next(iter(self.train_data))[0])
        print(f"[GPU{self.global_rank}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}")
        self.train_data.sampler.set_epoch(epoch)
        for source, targets in self.train_data:
            source = source.to(self.local_rank)
            targets = targets.to(self.local_rank)
            self._run_batch(source, targets)

    def _save_snapshot(self, epoch):
        snapshot = {
            "MODEL_STATE": self.model.module.state_dict(),
            "EPOCHS_RUN": epoch,
        }
        torch.save(snapshot, self.snapshot_path)
        print(f"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}")


    def train(self, max_epochs: int):
        for epoch in range(self.epochs_run, max_epochs):
            self._run_epoch(epoch)
            if self.local_rank == 0 and epoch % self.save_every == 0:
                self._save_snapshot(epoch)


def load_train_objs(num_epochs, batch_size_per_gpu, world_size):
    train_set = SyntheticDataset(num_epochs, batch_size_per_gpu, world_size)
    model = models.resnet50(pretrained=False)
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
    return train_set, model, optimizer


def prepare_dataloader(dataset: Dataset, batch_size: int):
    return DataLoader(
        dataset,
        batch_size=batch_size,
        pin_memory=True,
        shuffle=False,
        sampler=DistributedSampler(dataset),
        num_workers=32
    )


def main(save_every: int,\
          total_epochs: int,\
              batch_size: int,\
                  snapshot_path: str = os.path.join(os.getcwd(), 'snapshot.pt')):
    world_size, rank = ddp_setup()
    batch_size_per_gpu = int(batch_size / 4 )
    dataset, model, optimizer = load_train_objs(total_epochs, batch_size_per_gpu, world_size)
    train_data = prepare_dataloader(dataset, batch_size)
    trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)
    trainer.train(total_epochs)
    print('Will now destroy the process group\n')
    destroy_process_group()
    print('Process group destroyed')
    import sys
    # https://github.com/pytorch/pytorch/issues/76287
    sys.exit(0)


import random 
class SyntheticDataset(Dataset):
    def __init__(self, num_epochs, batch_size_per_gpu, world_size):
        self.num_epochs = num_epochs 
        self.batch_size_per_gpu = batch_size_per_gpu 
        self.num_iters = world_size 
        self.len = self.num_epochs * self.batch_size_per_gpu * self.num_iters

    def __getitem__(self, idx):
        data = torch.randn(3, 224, 224)
        target = random.randint(0, 999)
        return (data, target)

    def __len__(self):
        return self.len 


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='simple distributed training job')
    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')
    parser.add_argument('save_every', type=int, help='How often to save a snapshot')
    parser.add_argument('batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')
    parser.add_argument('snapshot_path', type=str, help='Path for the snapshot file')
    args = parser.parse_args()

    main(args.save_every, args.total_epochs, args.batch_size, args.snapshot_path)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
echo "SLURM_JOB_ID:$SLURM_JOB_ID SLURM_JOB_NAME:$SLURM_JOB_NAME SLURM_JOB_NODELIST:$SLURM_JOB_NODELIST SLURMD_NODENAME:$SLURMD_NODENAME SLURM_JOB_NUM_NODES:$SLURM_JOB_NUM_NODES"

echo "[$(date '+%Y-%m-%d %H:%M:%S')] Job started on $SLURM_JOB_NODELIST"

# Validate SLURM environment
if [ -z "$SLURM_JOB_ID" ] || [ -z "$SLURM_NNODES" ]; then
    echo "Required SLURM environment variables not set"
    exit 1
fi

echo "---------------------------------------"
echo "SLURM JOB PARAMS"
echo "PROJECT_ACCOUNT: $variables_PROJECT_ACCOUNT"
echo "OUTPUT_FILE:     $variables_OUTPUT_FILE"
echo "ERROR_FILE:      $variables_ERROR_FILE"

echo "SLURM RESOURCE ALLOCATION"
echo "NODE_COUNT:     $variables_NODE_COUNT"
echo "TASKS_PER_NODE: $variables_TASKS_PER_NODE"

echo "SLURM SCHEDULING PARAMETERS"
echo "PARTITION_NAME: $variables_PARTITION_NAME"
echo "QUEUE_TYPE:     $variables_QUEUE_TYPE"
echo "WALL_TIME:      $variables_WALL_TIME"

echo "SLURM EXECUTION PARAMETERS"
echo "CPU_PER_TASK: $variables_CPU_PER_TASK"
echo "NODE_TASKS:   $variables_NODE_TASKS"
echo "WAIT_TIMEOUT: $variables_WAIT_TIMEOUT"
echo "KILL_POLICY:  $variables_KILL_POLICY"

echo "GPU CONFIGURATION"
echo "GPU_COUNT:   $variables_GPU_COUNT"
echo "GPU_DEVICES: $variables_GPU_DEVICES"

echo "DISTRIBUTED TRAINING SETUP"
echo "MAX_RESTARTS: $variables_MAX_RESTARTS"
echo "DIST_BACKEND: $variables_DIST_BACKEND"
echo "NCCL_SOCKET_IFNAME: $variables_NCCL_SOCKET_IFNAME"
echo "NCCL_ASYNC_ERROR_HANDLING: $variables_NCCL_ASYNC_ERROR_HANDLING"
echo "OMP_NUM_THREADS: $variables_OMP_NUM_THREADS"

echo "TRAINING PARAMETERS"
echo "MODEL_EPOCHS:  $variables_MODEL_EPOCHS"
echo "SAVE_FREQUENCY: $variables_SAVE_FREQUENCY"
echo "BATCH_SIZE:    $variables_BATCH_SIZE"

echo "CHECKPOINT MANAGEMENT"
echo "CHECKPOINT_FILE: $variables_CHECKPOINT_FILE"
echo "CLEAN_CHECKPOINT: $variables_CLEAN_CHECKPOINT"

echo "DEBUG AND LOGGING"
echo "DEBUG_MODE: $variables_DEBUG_MODE"
echo "LOG_LEVEL:  $variables_LOG_LEVEL"
echo "CPP_LOG_LEVEL: $variables_CPP_LOG_LEVEL"
echo "LOG_DIRECTORY: $variables_LOG_DIRECTORY"
echo "---------------------------------------"

echo "[$(date '+%Y-%m-%d %H:%M:%S')] Loading required modules"
module load env/staging/2023.1
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
module load torchvision/

nodes=($(scontrol show hostnames $SLURM_JOB_NODELIST)) || { echo "Failed to get node list"; exit 1; }
if [ ${#nodes[@]} -eq 0 ]; then
    echo "No nodes found in allocation"
    exit 1
fi
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address) || {
    echo "Failed to get head node IP address"
    exit 1
}
echo "The head node is ${head_node}"

# OPTIONAL: set to true if you want more details on NCCL communications
DEBUG=$variables_DEBUG_MODE
if [ "$DEBUG" == "true" ]; then
    export LOGLEVEL=$variables_LOG_LEVEL
    export NCCL_DEBUG=TRACE
    export TORCH_CPP_LG_LEVEL=$variables_CPP_LOG_LEVEL
    echo "Debug mode enabled with:"
    echo "  LOGLEVEL=$LOGLEVEL"
    echo "  NCCL_DEBUG=TRACE"
    echo "  TORCH_CPP_LG_LEVEL=$TORCH_CPP_LG_LEVEL"
else
    echo "Debug mode is off."
fi

# Define the file where PyTorch will make a snapshot in case a training is interrupted and will have to be restarted
snapshot_name=$variables_CHECKPOINT_FILE
snapshot_file="${PWD}/${snapshot_name}"
mkdir -p "$(dirname "$snapshot_file")"
if [ "$variables_CLEAN_CHECKPOINT" == "true" ] && [ -f "$snapshot_file" ]; then
    echo "Removing existing checkpoint: $snapshot_file"
    rm "$snapshot_file" || echo "Warning: Failed to remove checkpoint file"
fi

export NCCL_SOCKET_IFNAME=$variables_NCCL_SOCKET_IFNAME
export NCCL_ASYNC_ERROR_HANDLING=$variables_NCCL_ASYNC_ERROR_HANDLING
export OMP_NUM_THREADS=$variables_OMP_NUM_THREADS

# Verify network interface exists
if ! ip link show $variables_NCCL_SOCKET_IFNAME >/dev/null 2>&1; then
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] Warning: Network interface $variables_NCCL_SOCKET_IFNAME not found"
fi

# get free port
if ! random_port=$(python - <<EOF
import socket
def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        return s.getsockname()[1]
print(find_free_port())
EOF
); then
    echo "Failed to find free port"
    exit 1
fi
echo "Random free port: $random_port"

echo "rdvz-endpoint is ${head_node_ip}:${random_port}"
endpoint="${head_node_ip}:${random_port}"

# Validate critical variables
if [ -z "$variables_GPU_COUNT" ] || [ -z "$variables_GPU_DEVICES" ]; then
    echo "GPU configuration variables not set"
    exit 1
fi

# Check write permissions for checkpoint and log directories
if ! touch "$variables_LOG_DIRECTORY/.write_test" 2>/dev/null; then
    echo "No write permission in log directory: $variables_LOG_DIRECTORY"
    exit 1
fi
rm -f "$variables_LOG_DIRECTORY/.write_test"

# Create log directory if it doesn't exist
mkdir -p $variables_LOG_DIRECTORY || {
    echo "Failed to create log directory: $variables_LOG_DIRECTORY"
    exit 1
}

export NGPUS_PER_NODE=$variables_GPU_COUNT

echo "[$(date '+%Y-%m-%d %H:%M:%S')] Launching training command"

CMD="CUDA_VISIBLE_DEVICES=$variables_GPU_DEVICES \
     srun --cpus-per-task=$variables_CPU_PER_TASK \
          --wait=$variables_WAIT_TIMEOUT \
          --ntasks-per-node=$variables_NODE_TASKS \
          --kill-on-bad-exit=$variables_KILL_POLICY \
     torchrun --max_restarts $variables_MAX_RESTARTS \
              --nnodes ${SLURM_NNODES} \
              --nproc_per_node ${NGPUS_PER_NODE} \
              --rdzv_id 10000 \
              --rdzv_backend $variables_DIST_BACKEND \
              --rdzv_endpoint $endpoint \
              --log_dir ${PWD}/$variables_LOG_DIRECTORY \
              $variables_SCRIPT_PATH \
              $variables_MODEL_EPOCHS \
              $variables_SAVE_FREQUENCY \
              $variables_BATCH_SIZE \
              $snapshot_file"

echo "Executing: $CMD"
eval $CMD
]]>
          </code>
        </script>
      </scriptExecutable>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
snapshot_name = variables.get("CHECKPOINT_FILE")
snapshot_file = "${pwd()}/${snapshot_name}"
resultMap.put("snapshot_file", snapshot_file)
]]>
          </code>
        </script>
      </post>
      <metadata>
        <positionTop>
            136.5234375
        </positionTop>
        <positionLeft>
            198.1640625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
</job>
