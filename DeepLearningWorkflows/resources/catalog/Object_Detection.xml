<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.11"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.11 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.11/schedulerjob.xsd"
    name="Object_Detection" projectName="4. Custom AI workflows"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="GPU_NODES_ONLY" value="False" model="PA:Boolean"/>
    <variable name="DOCKER_ENABLED" value="True" model="PA:Boolean"/>
  </variables>
  <description>
    <![CDATA[ Use a pre-trained deep neural network to detect objects an image dataset. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="deep-learning-workflows"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_object_detection.png"/>
    <info name="Documentation" value="http://activeeon.com/resources/automated-machine-learning-activeeon.pdf"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Export_Results" >
      <description>
        <![CDATA[ Preview the predicted results ]]>
      </description>
      <variables>
        <variable name="OUTPUT_FILE" value="HTML" inherited="true" />
        <variable name="GPU_NODES_ONLY" value="False" inherited="true" model="PA:Boolean"/>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_export_results.png"/>
        <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_export_results"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Object_Detection_Model"/>
      </depends>
      <inputFiles>
        <files  includes="$DATASET_PATH/**" accessMode="transferFromGlobalSpace"/>
        <files  includes="$OUTPUT_FOLDER/**" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <selection>
        <script type="static">
          <code language="python">
            <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
          </code>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Export_Results")

import base64
import pandas as pd
from PIL import Image
from io import BytesIO

if 'variables' in locals():
  PREDICT_DATA = variables.get("PREDICT_DATA_JSON")
  OUTPUT_FILE = variables.get("OUTPUT_FILE")

assert PREDICT_DATA is not None
df = pd.read_json(PREDICT_DATA, orient='split')  

# check the predictions
if {'Predictions','Targets'}.issubset(df.columns):
	pred_result =[]
	for indice in range(len(df)):
		if df['Predictions'][indice] == df['Targets'][indice]:
			result = 'https://github.com/ow2-proactive/automation-dashboard/blob/master/app/styles/patterns/img/wf-icons/tick_green.png?raw=true'
			pred_result.append(result)
		else:
			result = 'https://github.com/ow2-proactive/automation-dashboard/blob/master/app/styles/patterns/img/wf-icons/close_red.png?raw=true'
			pred_result.append(result)
	df_pred_image_url = pd.DataFrame(pred_result)
	df['Results'] = df_pred_image_url
 
def get_thumbnail(path):
  i = Image.open(path)
  extension = i.format
  i.thumbnail((150, 150), Image.LANCZOS)
  return i, extension

def image_base64(im):
  if isinstance(im, str):
    im, extension = get_thumbnail(im)
  with BytesIO() as buffer:
    im.save(buffer, extension)
    return base64.b64encode(buffer.getvalue()).decode()

def image_formatter(im):
  extension = im.format
  return f'<img src="data:image/extension;base64,{image_base64(im)}" height="200" width="200">'

def image_formatter_url(im_url):
  return """<img src="{0}" height="50" width="50"/>""".format(im_url)
  

result = ''
with pd.option_context('display.max_colwidth', -1):
  result = df.to_html(escape=False, formatters=dict(Images=image_formatter, Outputs=image_formatter, Results=image_formatter_url))

css_style="""
table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%; 
}
td {
  border: 1px solid #999999;         
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
  #border-bottom: 1px solid #FF8C00;  
  border-bottom: 1px solid #0B6FA4;   
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  #background: #E7702A;       
  #border-left: 2px solid #999999
  border-bottom: 1px solid #FF8C00;            
}
"""
result = """
               
            
            
            
            
            
            
            
            
            
            
            
            <!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <style>{0}</style>
                </head>
                <body>{1}</body></html>
""".format(css_style, result)

if OUTPUT_FILE == 'HTML':  
    result = result.encode('utf-8')
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "result.html")
    resultMetadata.put("content.type", "text/html")
    print("END Export_Results")
elif OUTPUT_FILE == 'CSV':
    if 'Images' in df.columns:
        df.pop('Images')
    if 'Results' in df.columns:        
        df.pop('Results')  
    if 'Outputs' in df.columns:        
        df.pop('Outputs') 
    result = df.to_csv()    
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", "result.csv")
    resultMetadata.put("content.type", "text/csv") 
    print("END Export_Results")
else:
  print('It is not possible to export the data')
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
          <metadata>
            <positionTop>
            611
        </positionTop>
            <positionLeft>
            758.5
        </positionLeft>
          </metadata>
        </task>
        <task name="Export_Images" >
          <description>
            <![CDATA[ Download a zip file of your results ]]>
          </description>
          <variables>
            <variable name="GPU_NODES_ONLY" value="False" inherited="true" model="PA:Boolean"/>
            <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_images.png"/>
            <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_export_images"/>
          </genericInformation>
          <depends>
            <task ref="Predict_Object_Detection_Model"/>
          </depends>
          <inputFiles>
            <files  includes="$DATASET_PATH/**" accessMode="transferFromGlobalSpace"/>
            <files  includes="$OUTPUT_FOLDER/**" accessMode="transferFromGlobalSpace"/>
          </inputFiles>
          <selection>
            <script type="static">
              <code language="python">
                <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
              </code>
            </script>
          </selection>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN Export_Images")

import os
import cv2
import json
import glob
import uuid  
import torch 
import numpy
import torch
import shutil
import random
import zipfile
import pandas as pd

from PIL import Image
from os.path import join, exists
from os import remove, listdir, makedirs

import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F
from torch.utils.data import Dataset 

from torch.utils import model_zoo
from ast import literal_eval as make_tuple
from os.path import basename, splitext, exists, join, isfile

from torch.optim import SGD, Adam
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision.transforms import Compose
from torchvision import datasets, models, transforms
from torchvision.transforms import ToTensor, ToPILImage, Normalize, Scale

if 'variables' in locals():
  DATASET_PATH   = variables.get("DATASET_PATH")
  NET_TRANSFORM  = variables.get("NET_TRANSFORM")
  CNN_TRANSFORM  = variables.get("CNN_TRANSFORM")
  PREDICT_DATA   = variables.get("PREDICT_DATA_JSON")
  SHUFFLE = variables.get("SHUFFLE")
  BATCH_SIZE = int(str(variables.get("BATCH_SIZE")))
  NUM_WORKERS = int(str(variables.get("NUM_WORKERS")))
  DATASET_TYPE = variables.get("DATASET_TYPE")  
  IMG_SIZE   = variables.get("IMG_SIZE") 

#CLASSIFICATION
if DATASET_TYPE == 'CLASSIFICATION':
	# Load CNN transform
	# data_transforms
	if CNN_TRANSFORM != None:
		assert CNN_TRANSFORM is not None
		exec(CNN_TRANSFORM)   
        
	# Load dataset
	image_dataset = {x: 
		datasets.ImageFolder(join(DATASET_PATH, x), data_transforms[x]) 
  		for x in ['test']}

	data_loader = {x: 
		DataLoader(image_dataset[x], batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS) 
		for x in ['test']}        
        
	# Get an unique ID
	ID = str(uuid.uuid4())

	# Define localspace
	LOCALSPACE = join('results', ID)
	os.makedirs(LOCALSPACE, exist_ok=True)
    
	if PREDICT_DATA != None: 
		prediction_result  = pd.read_json(PREDICT_DATA, orient='split')
		df = pd.DataFrame(prediction_result)
		preds = df['Predictions']
        
		for index, elem in enumerate(preds):
			# check if folder exist
			os.makedirs(join(LOCALSPACE,  preds[index]), exist_ok=True)       
			shutil.copy2(data_loader['test'].dataset.imgs[index][0], LOCALSPACE + '/' + preds[index])
            
		FILE_NAME = '.zip'  
		FILE_PATH = join(LOCALSPACE, FILE_NAME)
		print("FILE_PATH: " + FILE_PATH)  
             
		def zipdir(_path, _ziph):
			# ziph is zipfile handle
			for root, dirs, files in os.walk(_path):
				for file in files:
					_ziph.write(join(root, file))   
            
		zipf = zipfile.ZipFile(FILE_PATH, 'w', zipfile.ZIP_DEFLATED)
		zipdir(LOCALSPACE, zipf)
		zipf.close()
  
		assert isfile(FILE_PATH) == True   
        
		# Read the whole file at once
		FILE_BIN = None
		with open(FILE_PATH, "rb") as binary_file:
			FILE_BIN = binary_file.read()
		assert FILE_BIN is not None  
               
		if 'variables' in locals():
			result = FILE_BIN
			resultMetadata.put("file.extension", ".zip")
			resultMetadata.put("file.name", "result.zip")
			resultMetadata.put("content.type", "application/octet-stream") 
			print("END Export_Images")
	else:
		print("It is not possible to export the images")   
        
#DETECTION / SEGMENTATION     
if DATASET_TYPE == 'DETECTION' or DATASET_TYPE == 'SEGMENTATION':
    IMG_SIZE = tuple(IMG_SIZE)
    # Get an unique ID
    ID = str(uuid.uuid4())
    # Define localspace
    LOCALSPACE = join('results', ID)
    os.makedirs(LOCALSPACE, exist_ok=True)
    if PREDICT_DATA != None: 
        prediction_result  = pd.read_json(PREDICT_DATA, orient='split')
        df = pd.DataFrame(prediction_result)
        os.makedirs(join(LOCALSPACE, 'images'), exist_ok=True)
        os.makedirs(join(LOCALSPACE, 'outputs'), exist_ok=True)
        imgs = df['Image Paths']
        preds = df['Outputs']
        for index, elem in enumerate(preds): 
        	shutil.copy2(imgs[index], LOCALSPACE + '/' + 'images') 
        	shutil.copy2(elem, LOCALSPACE + '/' + 'outputs') 
            
        FILE_NAME = '.zip' 
        FILE_PATH = join(LOCALSPACE, FILE_NAME)
        print("FILE_PATH: " + FILE_PATH) 
        
        def zipdir(_path, _ziph):
        	# ziph is zipfile handle
        	for root, dirs, files in os.walk(_path):
        		for file in files:
        			_ziph.write(join(root, file))  
                    
        zipf = zipfile.ZipFile(FILE_PATH, 'w', zipfile.ZIP_DEFLATED)
        zipdir(LOCALSPACE, zipf)
        zipf.close()
        
        assert isfile(FILE_PATH) == True 
        
		# Read the whole file at once
        FILE_BIN = None
        with open(FILE_PATH, "rb") as binary_file:
        	FILE_BIN = binary_file.read()
        assert FILE_BIN is not None  
        
        if 'variables' in locals():
        	result = FILE_BIN
        	resultMetadata.put("file.extension", ".zip")
        	resultMetadata.put("file.name", "result.zip")
        	resultMetadata.put("content.type", "application/octet-stream") 
        	print("END Export_Images")
    else:
        print("It is not possible to export the images")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
          <metadata>
            <positionTop>
            611
        </positionTop>
            <positionLeft>
            898.5
        </positionLeft>
          </metadata>
        </task>
        <task name="Import_Model" >
          <description>
            <![CDATA[ Import a trained model by a deep learning algorithm. ]]>
          </description>
          <variables>
            <variable name="GPU_NODES_ONLY" value="False" inherited="true" model="PA:Boolean"/>
            <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
            <variable name="MODEL_URL" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/models/yolo3_coco.zip" inherited="true" />
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_deep_model.png"/>
            <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_import_model"/>
          </genericInformation>
          <selection>
            <script type="static">
              <code language="python">
                <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
              </code>
            </script>
          </selection>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN Import_Model")

import wget
import uuid
import shutil
import zipfile

from os.path import join, exists
from os import remove, listdir, makedirs

# Pre-trained models
# Text:  https://s3.eu-west-2.amazonaws.com/activeeon-public/models/basic_sentiment_analysis.zip
# Image: https://s3.eu-west-2.amazonaws.com/activeeon-public/models/model_resnet18.zip

# Load a trained model on ResNet-18 with two classes [ants, bees]
#MODEL_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/model_resnet18.zip' ##CLASSIFICATION MODEL

# Load a trained model on SegNet with five classes 
#MODEL_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/model_segnet.zip'##SEGMENTATION MODEL

# Load a trained model on SDD with twenty-one classes 
#MODEL_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/sdd_voc.zip'##OBJECT DETECTION (SSD MODEL ON PASCAL VOC DATASET)

# Load a trained model on SDD with twenty-one classes 
MODEL_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/yolo3_coco.zip'##OBJECT DETECTION (YOLO MODEL ON COCO DATASET)

if 'variables' in locals():
  MODEL_URL = variables.get("MODEL_URL")

print("MODEL_URL:   " + MODEL_URL)
assert MODEL_URL is not None

# Get an unique ID
ID = str(uuid.uuid4())

# Create an empty dir
MODEL_FOLDER = join('models', ID)
if exists(MODEL_FOLDER):
  shutil.rmtree(MODEL_FOLDER)
makedirs(MODEL_FOLDER)
print("MODEL_FOLDER: " + MODEL_FOLDER)

print("Downloading...")
filename = wget.download(MODEL_URL, MODEL_FOLDER)
print("FILENAME: " + filename)
print("OK")

print("Extracting...")
dataset_zip = zipfile.ZipFile(filename)
dataset_zip.extractall(MODEL_FOLDER)
dataset_zip.close()
remove(filename)
print("OK")

MODEL_PATH = None
LABELS_PATH = None
TEXT_PATH = None 
for file in listdir(MODEL_FOLDER):
  if file.endswith(".pt") or file.endswith(".pth") or file.endswith(".weights"):
    MODEL_PATH = join(MODEL_FOLDER, file)
  if file.endswith("label.pkl"):
    LABELS_PATH = join(MODEL_FOLDER, file)
  if file.endswith("text.pkl"):
    TEXT_PATH = join(MODEL_FOLDER, file)
  if file.endswith(".txt"):
    LABELS_PATH = join(MODEL_FOLDER, file)

assert MODEL_PATH is not None
#assert LABELS_PATH is not None
print(LABELS_PATH)
print("Model information: ")
print("MODEL_PATH:  " + MODEL_PATH)
if LABELS_PATH != None:
	print("LABELS_PATH: " + LABELS_PATH)
print("TEXT_PATH:   " + str(TEXT_PATH))

if 'variables' in locals():
  variables.put("MODEL_PATH", MODEL_PATH)
  variables.put("LABELS_PATH", LABELS_PATH)
  variables.put("TEXT_PATH", TEXT_PATH)
  variables.put("MODEL_FOLDER", MODEL_FOLDER)

print("END Import_Model")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
          <outputFiles>
            <files  includes="$MODEL_FOLDER/**" accessMode="transferToGlobalSpace"/>
          </outputFiles>
          <metadata>
            <positionTop>
            355
        </positionTop>
            <positionLeft>
            672.5
        </positionLeft>
          </metadata>
        </task>
        <task name="Import_Image_Dataset" >
          <description>
            <![CDATA[ Load and return an image dataset. ]]>
          </description>
          <variables>
            <variable name="GPU_NODES_ONLY" value="False" inherited="true" model="PA:Boolean"/>
            <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
            <variable name="DATASET_URL" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/coco.zip" inherited="true" />
            <variable name="SPLIT_TRAIN" value="0.60" inherited="true" />
            <variable name="SPLIT_VAL" value="0.15" inherited="true" />
            <variable name="SPLIT_TEST" value="0.25" inherited="true" />
            <variable name="DATASET_TYPE" value="Detection" inherited="true" model="PA:LIST(Classification, Detection, Segmentation)"/>
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/import_image.png"/>
            <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_import_image_dataset"/>
          </genericInformation>
          <selection>
            <script type="static">
              <code language="python">
                <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
              </code>
            </script>
          </selection>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN Import_Image_Dataset")

import re
import os
import json
import wget
import uuid 
import shutil
import zipfile
from os import remove, listdir, makedirs
from os.path import basename, splitext, exists, join
from sklearn.model_selection import train_test_split
 
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/ants_vs_bees.zip'  #CLASSIFICATION DATASET
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/pascal_voc.zip'     #DTECTION DATASET
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/pascal_coco.zip'     #DTECTION DATASET
#DATASET_URL = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/oxford.zip'        #SEGMENTATION DATASET

SPLIT_SETS  = ['train','val','test']

if 'variables' in locals():
  if variables.get("DATASET_URL") is not None:
    DATASET_URL = variables.get("DATASET_URL")
  if variables.get("SPLIT_TRAIN") is not None:
    SPLIT_TRAIN = float(str(variables.get("SPLIT_TRAIN")))
  if variables.get("SPLIT_VAL") is not None:
    SPLIT_VAL = float(str(variables.get("SPLIT_VAL")))
  if variables.get("SPLIT_TEST") is not None:
    SPLIT_TEST = float(str(variables.get("SPLIT_TEST")))
  
DATASET_TYPE = variables.get("DATASET_TYPE")
DATASET_TYPE = DATASET_TYPE.upper()
print(DATASET_TYPE)
# Get an unique ID
ID = str(uuid.uuid4())

# Define localspace
LOCALSPACE = join('data', ID)
if exists(LOCALSPACE):
  shutil.rmtree(LOCALSPACE)
makedirs(LOCALSPACE)

print("LOCALSPACE:  " + LOCALSPACE)
print("DATASET_URL: " + DATASET_URL)
assert DATASET_URL is not None

print("Split information: ")
print("SPLIT_TRAIN: " + str(SPLIT_TRAIN))
print("SPLIT_VAL:   " + str(SPLIT_VAL))
print("SPLIT_TEST:  " + str(SPLIT_TEST))

assert SPLIT_TRAIN >= 0.0
assert SPLIT_VAL >= 0.0
assert SPLIT_TEST >= 0.0
assert (SPLIT_TRAIN + SPLIT_VAL + SPLIT_TEST) == 1.0
if SPLIT_TRAIN == 0.0 and SPLIT_VAL > 0.0:
  raise AssertionError("SPLIT_VAL cannot be defined when SPLIT_TRAIN equals zero") 

DATASET_NAME = splitext(DATASET_URL[DATASET_URL.rfind("/")+1:])[0]
DATASET_PATH = join(LOCALSPACE, DATASET_NAME)

if exists(DATASET_PATH):
  shutil.rmtree(DATASET_PATH)
makedirs(DATASET_PATH)

print("Dataset information: ")
print("DATASET_NAME: " + DATASET_NAME)
print("DATASET_PATH: " + DATASET_PATH)

print("Downloading...")
filename = wget.download(DATASET_URL, DATASET_PATH)
print("FILENAME: " + filename)
print("OK")

print("Extracting...")
dataset_zip = zipfile.ZipFile(filename)
dataset_zip.extractall(DATASET_PATH)
dataset_zip.close()
remove(filename)
print("OK")

#CLASSIFICATION DATASET 
if DATASET_TYPE == 'CLASSIFICATION':  
    k = 0
    images_list = []
    images_label = []
    labels_name = []
    for root in listdir(DATASET_PATH):
        if (not root.startswith('.')):
            labels_name.append(root)
            label_dir = join(DATASET_PATH, root)
            print(label_dir)
            files = listdir(label_dir)
            files[:] = [join(label_dir,file) for file in files]
            files_size = len(files)
            files_label = [k] * files_size
            images_list = images_list + files
            images_label = images_label + files_label
            k = k + 1


    print("Splitting the dataset into train and test")
    images_train, images_test, labels_train, labels_test = train_test_split(images_list, images_label, test_size=SPLIT_TEST, random_state=1)

    images_val = []
    labels_val = []
    if SPLIT_TRAIN != 0.0 and SPLIT_VAL != 0.0:
        print("Splitting the train into train and val")
        images_train, images_val, labels_train, labels_val = train_test_split(images_train, labels_train, test_size=SPLIT_VAL, random_state=1)

    images_split = {SPLIT_SETS[0] : images_train, SPLIT_SETS[1] : images_val, SPLIT_SETS[2] : images_test}
    labels_split = {SPLIT_SETS[0] : labels_train, SPLIT_SETS[1] : labels_val, SPLIT_SETS[2] : labels_test}

    def move_images(_path, _images_list, _labels_list, _labels_name):
        if exists(_path):
            shutil.rmtree(_path)
        makedirs(_path)
        for _label_name in _labels_name:
            makedirs(join(_path, _label_name))
        for _image_path, _image_label in zip(_images_list, _labels_list):
            _image_path_dest = join(_path, _labels_name[_image_label], basename(_image_path))
            print("Moving " + _image_path + " to " + _image_path_dest)
            shutil.move(_image_path, _image_path_dest)

    DATASET_LABELS = json.dumps(labels_name)
    print("DATASET_LABELS: " + DATASET_LABELS)


    for SPLIT_NAME in SPLIT_SETS:
        SPLIT_PATH = join(DATASET_PATH, SPLIT_NAME)
        print("SPLIT_PATH: " + SPLIT_PATH)
        move_images(SPLIT_PATH, images_split[SPLIT_NAME], labels_split[SPLIT_NAME], labels_name)

    if 'variables' in locals():
        variables.put("DATASET_NAME", DATASET_NAME)
        variables.put("DATASET_PATH", DATASET_PATH)
        variables.put("DATASET_LABELS", DATASET_LABELS)
        variables.put("DATASET_TYPE", DATASET_TYPE)        
    
    print("END Import_Image_Dataset")
    
#DETECTION / SEGMENTATION DATASET    
elif DATASET_TYPE == 'DETECTION' or DATASET_TYPE == 'SEGMENTATION':
    print(DATASET_TYPE)
    k = 0
    images_list = []
    images_list_gt = []
    labels_name = []              
    num_folder = len(next(os.walk(DATASET_PATH))[1])


    if num_folder == 2:
        for root in listdir(DATASET_PATH):
            labels_name.append(root)
            label_dir = join(DATASET_PATH, root)
            print(label_dir)
            files = listdir(label_dir)
            files[:] = [join(label_dir,file) for file in files]
            files_size = len(files)
            if k == 0:
                images_list = images_list + files
            if k == 1:
                images_list_gt = images_list_gt + files  
            k = k + 1   

    try:
        new_list = [x for x in images_list if re.search('.DS_Store', x)]
        images_list.remove(''.join(new_list))
    except:
        pass            

    try:
        new_list_gt = [x for x in images_list_gt if re.search('.DS_Store', x)]
        images_list_gt.remove(''.join(new_list_gt))    
    except:
        pass    
    
    images_list = sorted(images_list)
    images_list_gt = sorted(images_list_gt)

    print("Splitting the dataset into train and test")
    images_train, images_test, images_train_gt, images_test_gt = train_test_split(images_list, images_list_gt, test_size=SPLIT_TEST, random_state=1)
    
    images_val = []

    if SPLIT_TRAIN != 0.0 and SPLIT_VAL != 0.0:
        print("Splitting the train into train and val")
        images_train, images_val, images_train_gt, images_val_gt = train_test_split(images_train, images_train_gt, test_size=SPLIT_VAL, random_state=1)

        images_split = {SPLIT_SETS[0] : images_train, SPLIT_SETS[1] : images_val, SPLIT_SETS[2] : images_test}
        images_split_gt = {SPLIT_SETS[0] : images_train_gt, SPLIT_SETS[1] : images_val_gt, SPLIT_SETS[2] : images_test_gt}

    for SPLIT_NAME in SPLIT_SETS:
        SPLIT_PATH = join(DATASET_PATH, SPLIT_NAME)
        print("SPLIT_PATH: " + SPLIT_PATH)

            
    def move_seg_images(_path, _images_list, _labels_name):
        if exists(_path):
            shutil.rmtree(_path)
        makedirs(_path)
        for _image_path in _images_list:
            _image_path_dest = join(_path, basename(_image_path))
            print("Moving " + _image_path + " to " + _image_path_dest)
            shutil.move(_image_path, _image_path_dest)

    k = 0
    for bx in labels_name:
        for SPLIT_NAME in SPLIT_SETS:
            SPLIT_PATH = join(DATASET_PATH, SPLIT_NAME)
            if k == 0:
                print("SPLIT_PATH: " + SPLIT_PATH)
                move_seg_images(SPLIT_PATH + '/' + labels_name[0], images_split[SPLIT_NAME], labels_name)           
            if k == 1:
                print("SPLIT_PATH: " + SPLIT_PATH)
                move_seg_images(SPLIT_PATH + '/' + labels_name[1], images_split_gt[SPLIT_NAME], labels_name)
        k = k + 1            
        
    
    if 'variables' in locals():
        variables.put("DATASET_NAME", DATASET_NAME)
        variables.put("DATASET_PATH", DATASET_PATH)
        variables.put("DATASET_TYPE", DATASET_TYPE) 
        
    print("END Import_Image_Dataset")

    if num_folder != 2:    
        print('Please, check your dataset!')
        
else: 
    print('Please, check your dataset type variable!')
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
          <outputFiles>
            <files  includes="$DATASET_PATH/**" accessMode="transferToGlobalSpace"/>
          </outputFiles>
          <metadata>
            <positionTop>
            355
        </positionTop>
            <positionLeft>
            815.5
        </positionLeft>
          </metadata>
        </task>
        <task name="Predict_Object_Detection_Model" >
          <description>
            <![CDATA[ Predict a model using a deep learning algorithm. ]]>
          </description>
          <variables>
            <variable name="GPU_NODES_ONLY" value="False" inherited="true" model="PA:Boolean"/>
            <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
            <variable name="BATCH_SIZE" value="1" inherited="true" />
            <variable name="NUM_WORKERS" value="1" inherited="true" />
            <variable name="SHUFFLE" value="True" inherited="true" />
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_predict.png"/>
            <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_predict_image_segmentation_model"/>
          </genericInformation>
          <depends>
            <task ref="Import_Model"/>
            <task ref="Import_Image_Dataset"/>
            <task ref="YOLO"/>
          </depends>
          <inputFiles>
            <files  includes="$DATASET_PATH/**" accessMode="transferFromGlobalSpace"/>
            <files  includes="$MODEL_FOLDER/**" accessMode="transferFromGlobalSpace"/>
          </inputFiles>
          <selection>
            <script type="static">
              <code language="python">
                <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
              </code>
            </script>
          </selection>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN Predict_Object_Detection_Model")

import os
import cv2
import sys
import wget
import uuid
import glob
import torch
import numpy as np
import pandas as pd
from numpy import random

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.init as init
import torch.nn.functional as F
from torchvision import transforms
from torch.autograd import Variable
from torch.autograd import Function
import torch.backends.cudnn as cudnn

from math import sqrt as sqrt
from skimage.transform import resize
from os import remove, listdir, makedirs
from itertools import product as product
from ast import literal_eval as make_tuple
from torch.utils.data import Dataset, DataLoader
from os.path import basename, splitext, exists, join

if sys.version_info[0] == 2:
    import xml.etree.cElementTree as ET
else:
    import xml.etree.ElementTree as ET

if 'variables' in locals():
  
  NUM_CLASSES = variables.get("NUM_CLASSES")
  NET_MODEL     = variables.get("NET_MODEL")
  NET_TRANSFORM = variables.get("NET_TRANSFORM")
  NET_CRITERION = variables.get("NET_CRITERION")
  DATASET_PATH  = variables.get("DATASET_PATH")
  MODEL_PATH     = variables.get("MODEL_PATH")
  LABEL_PATH  = variables.get("LABEL_PATH")
  IMG_SIZE = variables.get("IMG_SIZE")
  LEARNING_RATE = variables.get("LEARNING_RATE")
  MOMENTUM = float(str(variables.get("MOMENTUM")))    
  WEIGHT_DECAY = float(str(variables.get("WEIGHT_DECAY"))) 
  BATCH_SIZE = int(str(variables.get("BATCH_SIZE")))
  NUM_WORKERS = int(str(variables.get("NUM_WORKERS")))
  NET_NAME = variables.get("NET_NAME")

assert DATASET_PATH is not None
assert NET_MODEL is not None
assert NET_TRANSFORM is not None
assert MODEL_PATH is not None

IMG_SIZE = tuple(IMG_SIZE)

##  DOWNLOAD CLASSES FILE
print("Downloading...")
filename = wget.download(LABEL_PATH)
print("FILENAME: " + filename)
print("OK")

# Class names
CLASSES  = tuple(open(filename).read().splitlines())
num_pred = NUM_CLASSES

DATASET_TEST_PATH = join(DATASET_PATH, 'train')
loader = join(DATASET_TEST_PATH, 'images')

# http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available
# Returns a bool indicating if CUDA is currently available.
use_gpu = torch.cuda.is_available()

# Get an unique ID
ID = str(uuid.uuid4())

# Create an empty dir
OUTPUT_FOLDER = join('output', ID)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    
def random_color():
    levels = range(0,255)
    return tuple(random.choice(levels) for _ in range(3))

def genetate_color(num_pred):
    color_list = []
    for color in range(num_pred):
        color_list.append(random_color())
    return color_list

def base_transform(image, size, mean):
    x = cv2.resize(image, (size, size)).astype(np.float32)
    x -= mean
    x = x.astype(np.float32)
    return x

class BaseTransform:
    def __init__(self, size, mean):
        self.size = size
        self.mean = np.array(mean, dtype=np.float32)

    def __call__(self, image, boxes=None, labels=None):
        return base_transform(image, self.size, self.mean), boxes, labels

    
# ======================================================================================

##################################  BEGIN SSD NET ###################################### 

# ======================================================================================

if (NET_NAME == 'SSD'):

    USE_PRETRAINED_MODEL = variables.get("USE_PRETRAINED_MODEL")
    LR_STEPS = variables.get("LR_STEPS")
    MEANS = variables.get("MEANS")
    START_ITERATION  = int(str(variables.get("START_ITERATION")))
    MAX_ITERATION = int(str(variables.get("MAX_ITERATION")))
    MIN_SIZES = variables.get("MIN_SIZES")
    MAX_SIZES = variables.get("MAX_SIZES")

    BUILD_TYPE = 'test'
    LR_STEPS = tuple(LR_STEPS)
    MEANS = make_tuple(MEANS)
    MEANS = tuple(MEANS)
    
    MIN_SIZES  = make_tuple(MIN_SIZES)
    MIN_SIZES  = tuple(MIN_SIZES)
    MIN_SIZES  = list(MIN_SIZES)

    MAX_SIZES  = make_tuple(MAX_SIZES)
    MAX_SIZES  = tuple(MAX_SIZES)
    MAX_SIZES  = list(MAX_SIZES)

    # Load NET model
    exec(NET_MODEL)

    MODEL_NAME = 'SSD'
    ssd_net = build_ssd(BUILD_TYPE, IMG_SIZE[0], NUM_CLASSES)
    Net = ssd_net
    assert Net is not None, f'model {MODEL_NAME} not available'
    model = Net

    model.eval()

    # Load trained model
    model.load_state_dict(torch.load(MODEL_PATH, map_location=lambda storage, loc: storage))
    print('Finished loading model!')
 
    if use_gpu:
        model = model.cuda()
    
    # Load NET criterion
    exec(NET_CRITERION)

    # Load NET transform
    exec(NET_TRANSFORM)

    transform = BaseTransform(model.size, (104, 117, 123))

    def predict_model(_model, loader, _use_gpu, _num_pred):
        bbox_colors = []
        unique_labels = []
        image_name = []
        label_name = []
        color_list = genetate_color(_num_pred) 
    
        for img_paths in glob.glob(os.path.join(loader, "*")):
             img = cv2.imread(img_paths)
             img_name = os.path.basename(img_paths) # name file
             transform = BaseTransform(model.size, (104, 117, 123))
             x = torch.from_numpy(transform(img)[0]).permute(2, 0, 1)
             x = Variable(x.unsqueeze(0))
             if use_gpu:
                 x = x.cuda()     
             y = model(x)      # forward 
             detections = y.data
    
             # scale each detection back up to the image
             scale = torch.Tensor([img.shape[1], img.shape[0],
                                     img.shape[1], img.shape[0]])    
            
             for i in range(detections.size(1)):
                 j = 0
                 while detections[0, i, j, 0] >= 0.6:
                        score = detections[0, i, j, 0]
                        pt = (detections[0, i, j, 1:]*scale).cpu().numpy()
                        coords = (pt[0], pt[1], pt[2], pt[3])             
                        x1 = pt[0]
                        y1 = pt[1]
                        box_w = pt[2]
                        box_h = pt[3]
                        class_name = CLASSES[i-1]
                        cls_pred  = i
                        print ('\t+ Label: %s, Conf: %.5f' % (class_name, score))
    
                        color = color_list[cls_pred]
                        color = tuple(map(int, color))
                       
                        cv2.rectangle(img , (x1, y1), (x1+box_w, y1+box_h), color, 6) 
                        cv2.putText(img, class_name, (pt[0], pt[1]), cv2.FONT_HERSHEY_COMPLEX, 1.5, color,2, cv2.LINE_AA)
                        j += 1 
                        
             label_paths = os.path.join(OUTPUT_FOLDER, img_name)
             cv2.imwrite(label_paths, img) 
             cv2.imwrite(os.path.join(OUTPUT_FOLDER) + '/' +  img_name, img)
            
             image_name.append(img_paths)
             label_name.append(label_paths)
        return image_name, label_name


    if os.path.isfile(filename):
        image_name, label_name = predict_model(model, loader, use_gpu, num_pred)
    else:
        print("Please, you need to add a class file")
        
# ======================================================================================

##################################  END SSD NET ######################################## 

# ======================================================================================




# ======================================================================================

##################################  BEGIN  YOLO  NET ###################################

# ======================================================================================

class ImageFolder(Dataset):
    def __init__(self, list_path, img_size=416):
        
        folder_path = join(list_path, 'images')
        
        self.files = sorted(glob.glob('%s/*.*' % folder_path))
        self.img_shape = (img_size, img_size)
        self.img_name  = os.listdir(folder_path)
        

    def __getitem__(self, index):
        img_path = self.files[index % len(self.files)]
        
        # Extract image
        img = np.array(Image.open(img_path))
        h, w, _ = img.shape
        dim_diff = np.abs(h - w)
        # Upper (left) and lower (right) padding
        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2
        # Determine padding
        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))
        # Add padding
        input_img = np.pad(img, pad, 'constant', constant_values=127.5) / 255.
        # Resize and normalize
        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')
        # Channels-first
        input_img = np.transpose(input_img, (2, 0, 1))
        # As pytorch tensor
        input_img = torch.from_numpy(input_img).float()

        return img_path, input_img

    def __len__(self):
        return len(self.files)
    

if (NET_NAME == 'YOLO'):
    
    CONF_THRESHOLD = variables.get("CONF_THRESHOLD")  
    NMS_THRESHOLD = variables.get("NMS_THRESHOLD")      
    
    ##  Download Model config
    print("Downloading...")
    MODEL_CONFIG_PATH = 'https://s3.eu-west-2.amazonaws.com/activeeon-public/models/yolov3.cfg'
    filename = wget.download(MODEL_CONFIG_PATH)
    print("MODEL_CONFIG_PATH: " + filename)
    print("OK")
    model_config_path = os.path.realpath(filename)
    
    # Load NET transforms
    exec(NET_TRANSFORM)
    # Load NET model
    exec(NET_MODEL)

    Net = Darknet(model_config_path)
    model = Net

    # Load trained model
    model.load_weights(MODEL_PATH)
    
    model.eval()
    
    print('Finished loading model!')

    DATASET_TEST_PATH = join(DATASET_PATH, 'test')
    loader = DataLoader(ImageFolder(DATASET_TEST_PATH , IMG_SIZE[0]),num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)
    
    if use_gpu:
        Tensor = torch.cuda.FloatTensor
    Tensor = torch.FloatTensor
    
    def predict_model(_model, loader, _use_gpu):
           
        imgs = []           # Stores image paths
        img_detections = [] # Stores detections for each image index
        
        for batch_i, (img_paths, images) in enumerate(loader):
    
            if use_gpu:
                images = images.cuda()
            inputs = Variable(images.type(Tensor))
           
            # Get detections
            with torch.no_grad():
                outputs = model(inputs)
                outputs = non_max_suppression(outputs, NUM_CLASSES, CONF_THRESHOLD, NMS_THRESHOLD) 
                
            imgs.extend(img_paths)
            img_detections.extend(outputs) 
        return imgs, img_detections

    def img_labeled(imgs, img_detections, num_pred): 
        print ('\nSaving images:')
        
        color_list = genetate_color(num_pred)  
        image_name = []
        label_name = []
        
        for img_i, (path, detections) in enumerate(zip(imgs, img_detections)): 
            #print ("(%d) Image: '%s'" % (img_i, path))
            img = cv2.imread(path)
            #img_name = path.split('/')           
            img_name = os.path.basename(path) 
           
            # The amount of padding that was added
            pad_x = max(img.shape[0] - img.shape[1], 0) * (IMG_SIZE[0] / max(img.shape))
            pad_y = max(img.shape[1] - img.shape[0], 0) * (IMG_SIZE[0]/ max(img.shape))
            
            # Image height and width after padding is removed
            unpad_h = IMG_SIZE[0] - pad_y # check depois essa variavel para colocar a entrada resize
            unpad_w = IMG_SIZE[0] - pad_x    
               
            # Draw bounding boxes and labels of detections
            if detections is not None:
             
                for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:            
    
                    print ('\t+ Label: %s, Conf: %.5f' % (CLASSES[int(cls_pred)], cls_conf.item()))
        
                    # Rescale coordinates to original dimensions
                    box_h = ((y2 - y1) / unpad_h) * img.shape[0]
                    box_w = ((x2 - x1) / unpad_w) * img.shape[1]
                    y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]
                    x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]  
                    
                    id_label = CLASSES[int(cls_pred)]
                    findcolor = CLASSES.index(id_label)
                    color = color_list[int(findcolor)]
                    color = tuple(map(int, color))
                    
                    cv2.rectangle(img , (x1, y1), (x1+box_w, y1+box_h), color, 6)
                    cv2.putText(img, CLASSES[int(cls_pred)], (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 1.5, color,2, cv2.LINE_AA)
                    cv2.imwrite(os.path.join(OUTPUT_FOLDER) + '/' + img_name, img)
                    
            else: 
               cv2.imwrite(os.path.join(OUTPUT_FOLDER) + '/' + img_name, img)
            
            label_paths = os.path.join(OUTPUT_FOLDER, img_name)
            image_name.append(path)
            label_name.append(label_paths)
        return image_name, label_name
                
    if os.path.isfile(filename):
        imgs, img_detections = predict_model(model, loader, use_gpu)
        image_name, label_name = img_labeled(imgs, img_detections, num_pred)
    else:
        print("Please, you need to add a class file")
        
df_name = pd.DataFrame(image_name)
df_image_name = pd.DataFrame(image_name)
df_label_name = pd.DataFrame(label_name)
df_name.columns = ['Image Paths']
df_image_name.columns = ['Images']
df_label_name.columns = ['Outputs']

df = pd.concat([df_name, df_image_name, df_label_name], axis=1)

if 'variables' in locals():
  variables.put("PREDICT_DATA_JSON", df.to_json(orient='split'))
  variables.put("BATCH_SIZE", BATCH_SIZE)
  variables.put("NUM_WORKERS", NUM_WORKERS)
  variables.put("OUTPUT_FOLDER", OUTPUT_FOLDER)

print("END Predict_Object_Detection_Model")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
          <outputFiles>
            <files  includes="$OUTPUT_FOLDER/**" accessMode="transferToGlobalSpace"/>
          </outputFiles>
          <metadata>
            <positionTop>
            483
        </positionTop>
            <positionLeft>
            828.5
        </positionLeft>
          </metadata>
        </task>
        <task name="YOLO" >
          <description>
            <![CDATA[ You only look once (YOLO) is a  single neural network to predict bounding boxes and class probabilities.
You can see more details in: https://pjreddie.com/media/files/papers/YOLOv3.pdf
https://github.com/eriklindernoren/PyTorch-YOLOv3 ]]>
          </description>
          <variables>
            <variable name="GPU_NODES_ONLY" value="False" inherited="true" model="PA:Boolean"/>
            <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
            <variable name="LEARNING_RATE" value="0.001" inherited="true" />
            <variable name="MOMENTUM" value="0.9" inherited="true" />
            <variable name="WEIGHT_DECAY" value="0.0005" inherited="true" />
            <variable name="IMG_SIZE" value="(416, 416)" inherited="true" />
            <variable name="NUM_CLASSES" value="81" inherited="true" />
            <variable name="CONF_THRESHOLD" value="0.5" inherited="true" />
            <variable name="NMS_THRESHOLD" value="0.45" inherited="true" />
            <variable name="LABEL_PATH" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/coco.names" inherited="true" />
            <variable name="USE_PRETRAINED_MODEL" value="True" inherited="true" />
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/deep_detection.png"/>
            <info name="task.documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_segnet"/>
          </genericInformation>
          <selection>
            <script type="static">
              <code language="python">
                <![CDATA[
import os

GPU_NODES_ONLY = False
if variables.get("GPU_NODES_ONLY") is not None:
  if str(variables.get("GPU_NODES_ONLY")).lower() == 'true':
    GPU_NODES_ONLY = True

CUDA_ENABLED = False
CUDA_HOME = os.getenv('CUDA_HOME', None)
CUDA_HOME_DEFAULT = '/usr/local/cuda'
if CUDA_HOME is not None:
  if os.path.isdir(CUDA_HOME) == True:
    CUDA_ENABLED = True
else:
  if os.path.isdir(CUDA_HOME_DEFAULT) == True:
    CUDA_ENABLED = True

selected = ((GPU_NODES_ONLY == False) or (GPU_NODES_ONLY == True and CUDA_ENABLED == True))
]]>
              </code>
            </script>
          </selection>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN YOLO")

import json
import numpy as np
from ast import literal_eval as make_tuple

IMG_SIZE = variables.get("IMG_SIZE") 
NUM_CLASSES = int(str(variables.get("NUM_CLASSES")))  
LEARNING_RATE = float(str(variables.get("LEARNING_RATE")))
MOMENTUM = float(str(variables.get("MOMENTUM")))
WEIGHT_DECAY = float(str(variables.get("WEIGHT_DECAY"))) 
CONF_THRESHOLD  = float(str(variables.get("CONF_THRESHOLD"))) 
NMS_THRESHOLD = float(str(variables.get("NMS_THRESHOLD")))
LABEL_PATH  = variables.get("LABEL_PATH")
USE_PRETRAINED_MODEL = variables.get("USE_PRETRAINED_MODEL") 

IMG_SIZE = make_tuple(IMG_SIZE)
IMG_SIZE = tuple(IMG_SIZE)
NET_NAME = 'YOLO'

# Define the TRANSFORM functions
NET_TRANSFORM = """

from __future__ import division
import math
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

def load_classes(path):
    fp = open(path, "r")
    names = fp.read().split("\\n")[:-1]
    return names

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm2d') != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

def compute_ap(recall, precision):
# =============================================================================
#       Compute the average precision, given the recall and precision curves.
#    Code originally from https://github.com/rbgirshick/py-faster-rcnn.
    # Arguments
#        recall:    The recall curve (list).
#        precision: The precision curve (list).
    # Returns
#        The average precision as computed in py-faster-rcnn.
# =============================================================================

    # correct AP calculation
    # first append sentinel values at the end
    mrec = np.concatenate(([0.], recall, [1.]))
    mpre = np.concatenate(([0.], precision, [0.]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap

def bbox_iou(box1, box2, x1y1x2y2=True):
# =============================================================================
#   Returns the IoU of two bounding boxes
# =============================================================================
    if not x1y1x2y2:
        # Transform from center and width to exact coordinates
        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2
        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2
        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2
        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2
    else:
        # Get the coordinates of bounding boxes
        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]
        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]

    # get the corrdinates of the intersection rectangle
    inter_rect_x1 =  torch.max(b1_x1, b2_x1)
    inter_rect_y1 =  torch.max(b1_y1, b2_y1)
    inter_rect_x2 =  torch.min(b1_x2, b2_x2)
    inter_rect_y2 =  torch.min(b1_y2, b2_y2)
    # Intersection area
    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \
                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)
    # Union Area
    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)
    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)

    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)

    return iou


def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):
# =====================================================================================
#  Removes detections with lower object confidence score than 'conf_thres' and performs
#  Non-Maximum Suppression to further filter detections.
#  Returns detections with shape:
#   (x1, y1, x2, y2, object_conf, class_score, class_pred)
# =====================================================================================

    # From (center x, center y, width, height) to (x1, y1, x2, y2)
    box_corner = prediction.new(prediction.shape)
    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2
    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2
    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2
    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2
    prediction[:, :, :4] = box_corner[:, :, :4]

    output = [None for _ in range(len(prediction))]
    for image_i, image_pred in enumerate(prediction):
        # Filter out confidence scores below threshold
        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()
        image_pred = image_pred[conf_mask]
        # If none are remaining => process next image
        if not image_pred.size(0):
            continue
        # Get score and class with highest confidence
        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)
        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)
        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)
        # Iterate through all predicted classes
        unique_labels = detections[:, -1].cpu().unique()
        if prediction.is_cuda:
            unique_labels = unique_labels.cuda()
        for c in unique_labels:
            # Get the detections with the particular class
            detections_class = detections[detections[:, -1] == c]
            # Sort the detections by maximum objectness confidence
            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)
            detections_class = detections_class[conf_sort_index]
            # Perform non-maximum suppression
            max_detections = []
            while detections_class.size(0):
                # Get detection with highest confidence and save as max detection
                max_detections.append(detections_class[0].unsqueeze(0))
                # Stop if we're at the last detection
                if len(detections_class) == 1:
                    break
                # Get the IOUs for all boxes with lower confidence
                ious = bbox_iou(max_detections[-1], detections_class[1:])
                # Remove detections with IoU >= NMS threshold
                detections_class = detections_class[1:][ious < nms_thres]

            max_detections = torch.cat(max_detections).data
            # Add max detections to outputs
            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))

    return output

def build_targets(pred_boxes, target, anchors, num_anchors, num_classes, dim, ignore_thres, img_dim):
    nB = target.size(0)
    nA = num_anchors
    nC = num_classes
    dim = dim
    mask        = torch.zeros(nB, nA, dim, dim)
    tx         = torch.zeros(nB, nA, dim, dim)
    ty         = torch.zeros(nB, nA, dim, dim)
    tw         = torch.zeros(nB, nA, dim, dim)
    th         = torch.zeros(nB, nA, dim, dim)
    tconf      = torch.zeros(nB, nA, dim, dim)
    tcls       = torch.zeros(nB, nA, dim, dim, num_classes)

    nGT = 0
    nCorrect = 0
    for b in range(nB):
        for t in range(target.shape[1]):
            if target[b, t].sum() == 0:
                continue
            nGT += 1
            # Convert to position relative to box
            gx = target[b, t, 1] * dim
            gy = target[b, t, 2] * dim
            gw = target[b, t, 3] * dim
            gh = target[b, t, 4] * dim
            # Get grid box indices
            gi = int(gx)
            gj = int(gy)
            # Get shape of gt box
            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)
            # Get shape of anchor box
            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))
            # Calculate iou between gt and anchor shape
            anch_ious = bbox_iou(gt_box, anchor_shapes)
            # Find the best matching anchor box
            best_n = np.argmax(anch_ious)
            best_iou = anch_ious[best_n]
            # Get the ground truth box and corresponding best prediction
            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)
            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)

            # Masks
            mask[b, best_n, gj, gi] = 1
            # Coordinates
            tx[b, best_n, gj, gi] = gx - gi
            ty[b, best_n, gj, gi] = gy - gj
            # Width and height
            tw[b, best_n, gj, gi] = math.log(gw/anchors[best_n][0] + 1e-16)
            th[b, best_n, gj, gi] = math.log(gh/anchors[best_n][1] + 1e-16)
            # One-hot encoding of label
            tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1
            # Calculate iou between ground truth and best matching prediction
            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)
            tconf[b, best_n, gj, gi] = 1

            if iou > 0.5:
                nCorrect += 1

    return nGT, nCorrect, mask, tx, ty, tw, th, tconf, tcls
    
def to_categorical(y, NUM_CLASSES):
# =============================================================================
#    1-hot encodes a tensor
# =============================================================================
    return torch.from_numpy(np.eye(NUM_CLASSES, dtype='uint8')[y])

"""
print(NET_TRANSFORM)

# Define the NET model
NET_MODEL = """

from __future__ import division

import torch
import numpy as np
from PIL import Image
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from collections import defaultdict

def parse_model_config(path):
# =============================================================================
#   Parses the yolo-v3 layer configuration file and returns module definitions
# =============================================================================
    file = open(path, 'r')
    lines = file.read().split('\\n')
    lines = [x for x in lines if x and not x.startswith('#')]
    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces
    module_defs = []
    for line in lines:
        if line.startswith('['): # This marks the start of a new block
            module_defs.append({})
            module_defs[-1]['type'] = line[1:-1].rstrip()
            if module_defs[-1]['type'] == 'convolutional':
                module_defs[-1]['batch_normalize'] = 0
        else:
            key, value = line.split("=")
            value = value.strip()
            module_defs[-1][key.rstrip()] = value.strip()

    return module_defs

def parse_data_config(path):
# =============================================================================
#   Parses the data configuration file
# =============================================================================
    options = dict()
    options['gpus'] = '0,1,2,3'
    options['num_workers'] = '10'
    with open(path, 'r') as fp:
        lines = fp.readlines()
    for line in lines:
        line = line.strip()
        if line == '' or line.startswith('#'):
            continue
        key, value = line.split('=')
        options[key.strip()] = value.strip()
    return options


def create_modules(module_defs):
# =====================================================================================
#   Constructs module list of layer blocks from module configuration in module_defs
# =====================================================================================
    hyperparams = module_defs.pop(0)
    output_filters = [int(hyperparams['channels'])]
    module_list = nn.ModuleList()
    for i, module_def in enumerate(module_defs):
        modules = nn.Sequential()

        if module_def['type'] == 'convolutional':
            bn = int(module_def['batch_normalize'])
            filters = int(module_def['filters'])
            kernel_size = int(module_def['size'])
            pad = (kernel_size - 1) // 2 if int(module_def['pad']) else 0
            modules.add_module('conv_%d' % i, nn.Conv2d(in_channels=output_filters[-1],
                                                        out_channels=filters,
                                                        kernel_size=kernel_size,
                                                        stride=int(module_def['stride']),
                                                        padding=pad,
                                                        bias=not bn))
            if bn:
                modules.add_module('batch_norm_%d' % i, nn.BatchNorm2d(filters))
            if module_def['activation'] == 'leaky':
                modules.add_module('leaky_%d' % i, nn.LeakyReLU(0.1))

        elif module_def['type'] == 'upsample':
            upsample = nn.Upsample( scale_factor=int(module_def['stride']),
                                    mode='nearest')
            modules.add_module('upsample_%d' % i, upsample)

        elif module_def['type'] == 'route':
            layers = [int(x) for x in module_def["layers"].split(',')]
            filters = sum([output_filters[layer_i] for layer_i in layers])
            modules.add_module('route_%d' % i, EmptyLayer())

        elif module_def['type'] == 'shortcut':
            filters = output_filters[int(module_def['from'])]
            modules.add_module("shortcut_%d" % i, EmptyLayer())

        elif module_def["type"] == "yolo":
            anchor_idxs = [int(x) for x in module_def["mask"].split(",")]
            # Extract anchors
            anchors = [int(x) for x in module_def["anchors"].split(",")]
            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]
            anchors = [anchors[i] for i in anchor_idxs]
            num_classes = int(module_def['classes'])
            img_height = int(hyperparams['height'])
            # Define detection layer
            yolo_layer = YOLOLayer(anchors, num_classes, img_height)
            modules.add_module('yolo_%d' % i, yolo_layer)
        # Register module list and number of output filters
        module_list.append(modules)
        output_filters.append(filters)

    return hyperparams, module_list

class EmptyLayer(nn.Module):
# =====================================================================================
#   Placeholder for 'route' and 'shortcut' layers
# =====================================================================================
    def __init__(self):
        super(EmptyLayer, self).__init__()

class YOLOLayer(nn.Module):
# =====================================================================================
#   Detection layer
# =====================================================================================
    def __init__(self, anchors, num_classes, img_dim):
        super(YOLOLayer, self).__init__()
        self.anchors = anchors
        self.num_anchors = len(anchors)
        self.num_classes = num_classes
        self.bbox_attrs = 5 + num_classes
        self.img_dim = img_dim
        self.ignore_thres = 0.5
        self.lambda_coord = 5
        self.lambda_noobj = 0.5

        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCELoss()

    def forward(self, x, targets=None):
        bs = x.size(0)
        g_dim = x.size(2)
        stride =  self.img_dim / g_dim
        # Tensors for cuda support
        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor

        prediction = x.view(bs,  self.num_anchors, self.bbox_attrs, g_dim, g_dim).permute(0, 1, 3, 4, 2).contiguous()

        # Get outputs
        x = torch.sigmoid(prediction[..., 0])          # Center x
        y = torch.sigmoid(prediction[..., 1])          # Center y
        w = prediction[..., 2]                         # Width
        h = prediction[..., 3]                         # Height
        conf = torch.sigmoid(prediction[..., 4])       # Conf
        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.

        # Calculate offsets for each grid
        grid_x = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).repeat(bs*self.num_anchors, 1, 1).view(x.shape).type(FloatTensor)
        grid_y = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).t().repeat(bs*self.num_anchors, 1, 1).view(y.shape).type(FloatTensor)
        scaled_anchors = [(a_w / stride, a_h / stride) for a_w, a_h in self.anchors]
        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(w.shape)
        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(h.shape)

        # Add offset and scale with anchors
        pred_boxes = FloatTensor(prediction[..., :4].shape)
        pred_boxes[..., 0] = x.data + grid_x
        pred_boxes[..., 1] = y.data + grid_y
        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w
        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h

        # Training
        if targets is not None:

            if x.is_cuda:
                self.mse_loss = self.mse_loss.cuda()
                self.bce_loss = self.bce_loss.cuda()

            nGT, nCorrect, mask, tx, ty, tw, th, tconf, tcls = build_targets(pred_boxes.cpu().data,
                                                                            targets.cpu().data,
                                                                            scaled_anchors,
                                                                            self.num_anchors,
                                                                            self.num_classes,
                                                                            g_dim,
                                                                            self.ignore_thres,
                                                                            self.img_dim)

            nProposals = int((conf > 0.25).sum().item())
            recall = float(nCorrect / nGT) if nGT else 1

            mask = Variable(mask.type(FloatTensor))

            tx    = Variable(tx.type(FloatTensor), requires_grad=False)
            ty    = Variable(ty.type(FloatTensor), requires_grad=False)
            tw    = Variable(tw.type(FloatTensor), requires_grad=False)
            th    = Variable(th.type(FloatTensor), requires_grad=False)
            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)
            tcls  = Variable(tcls.type(FloatTensor), requires_grad=False)

            # Mask outputs to ignore non-existing objects (but keep confidence predictions)
            loss_x = self.lambda_coord * self.bce_loss(x * mask, tx * mask) / 2
            loss_y = self.lambda_coord * self.bce_loss(y * mask, ty * mask) / 2
            loss_w = self.lambda_coord * self.mse_loss(w * mask, tw * mask) / 2
            loss_h = self.lambda_coord * self.mse_loss(h * mask, th * mask) / 2
            loss_conf = self.bce_loss(conf * mask, mask) + \
                        self.lambda_noobj * self.bce_loss(conf * (1 - mask), mask * (1 - mask))
            loss_cls = self.bce_loss(pred_cls[mask == 1], tcls[mask == 1])
            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls

            return loss, loss_x.item(), loss_y.item(), loss_w.item(), loss_h.item(), loss_conf.item(), loss_cls.item(), recall

        else:
            # If not in training phase return predictions
            output = torch.cat((pred_boxes.view(bs, -1, 4) * stride, conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1)
            return output.data


class Darknet(nn.Module):
# =====================================================================================
#   YOLOv3 object detection model
# =====================================================================================
    def __init__(self, config_path, img_size=416):
        super(Darknet, self).__init__()
        self.module_defs = parse_model_config(config_path)
        self.hyperparams, self.module_list = create_modules(self.module_defs)
        self.img_size = img_size
        self.seen = 0
        self.header_info = np.array([0, 0, 0, self.seen, 0])
        self.loss_names = ['x', 'y', 'w', 'h', 'conf', 'cls', 'recall']

    def forward(self, x, targets=None):
        is_training = targets is not None
        output = []
        self.losses = defaultdict(float)
        layer_outputs = []
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if module_def['type'] in ['convolutional', 'upsample']:
                x = module(x)
            elif module_def['type'] == 'route':
                layer_i = [int(x) for x in module_def['layers'].split(',')]
                x = torch.cat([layer_outputs[i] for i in layer_i], 1)
            elif module_def['type'] == 'shortcut':
                layer_i = int(module_def['from'])
                x = layer_outputs[-1] + layer_outputs[layer_i]
            elif module_def['type'] == 'yolo':
                # Train phase: get loss
                if is_training:
                    x, *losses = module[0](x, targets)
                    for name, loss in zip(self.loss_names, losses):
                        self.losses[name] += loss
                # Test phase: Get detections
                else:
                    x = module(x)
                output.append(x)
            layer_outputs.append(x)

        self.losses['recall'] /= 3
        return sum(output) if is_training else torch.cat(output, 1)


    def load_weights(self, weights_path):
# =====================================================================================
#  Parses and loads the weights stored in 'weights_path
# =====================================================================================

        #Open the weights file
        fp = open(weights_path, "rb")
        header = np.fromfile(fp, dtype=np.int32, count=5)   # First five are header values

        # Needed to write header when saving weights
        self.header_info = header

        self.seen = header[3]
        weights = np.fromfile(fp, dtype=np.float32)         # The rest are weights
        fp.close()

        ptr = 0
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if module_def['type'] == 'convolutional':
                conv_layer = module[0]
                if module_def['batch_normalize']:
                    # Load BN bias, weights, running mean and running variance
                    bn_layer = module[1]
                    num_b = bn_layer.bias.numel() # Number of biases
                    # Bias
                    bn_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.bias)
                    bn_layer.bias.data.copy_(bn_b)
                    ptr += num_b
                    # Weight
                    bn_w = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.weight)
                    bn_layer.weight.data.copy_(bn_w)
                    ptr += num_b
                    # Running Mean
                    bn_rm = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_mean)
                    bn_layer.running_mean.data.copy_(bn_rm)
                    ptr += num_b
                    # Running Var
                    bn_rv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_var)
                    bn_layer.running_var.data.copy_(bn_rv)
                    ptr += num_b
                else:
                    # Load conv. bias
                    num_b = conv_layer.bias.numel()
                    conv_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(conv_layer.bias)
                    conv_layer.bias.data.copy_(conv_b)
                    ptr += num_b
                # Load conv. weights
                num_w = conv_layer.weight.numel()
                conv_w = torch.from_numpy(weights[ptr:ptr + num_w]).view_as(conv_layer.weight)
                conv_layer.weight.data.copy_(conv_w)
                ptr += num_w

# =====================================================================================
#  @:param path    - path of the new weights file
#  @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)
# =====================================================================================
    def save_weights(self, path, cutoff=-1):

        fp = open(path, 'wb')
        self.header_info[3] = self.seen
        self.header_info.tofile(fp)

        # Iterate through layers
        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):
            if module_def['type'] == 'convolutional':
                conv_layer = module[0]
                # If batch norm, load bn first
                if module_def['batch_normalize']:
                    bn_layer = module[1]
                    bn_layer.bias.data.cpu().numpy().tofile(fp)
                    bn_layer.weight.data.cpu().numpy().tofile(fp)
                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)
                    bn_layer.running_var.data.cpu().numpy().tofile(fp)
                # Load conv bias
                else:
                    conv_layer.bias.data.cpu().numpy().tofile(fp)
                # Load conv weights
                conv_layer.weight.data.cpu().numpy().tofile(fp)

        fp.close()
"""
print(NET_TRANSFORM)


if 'variables' in locals():
  variables.put("NET_MODEL", NET_MODEL)
  variables.put("NET_TRANSFORM", NET_TRANSFORM)    
  variables.put("IMG_SIZE", IMG_SIZE)
  variables.put("NUM_CLASSES", NUM_CLASSES)
  variables.put("LEARNING_RATE",  LEARNING_RATE) 
  variables.put("MOMENTUM",  MOMENTUM) 
  variables.put("WEIGHT_DECAY", WEIGHT_DECAY)
  variables.put("CONF_THRESHOLD", CONF_THRESHOLD)
  variables.put("NMS_THRESHOLD", NMS_THRESHOLD)
  variables.put("LABEL_PATH", LABEL_PATH)
  variables.put("USE_PRETRAINED_MODEL", USE_PRETRAINED_MODEL)
  variables.put("NET_NAME", NET_NAME)

print("END YOLO")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"></controlFlow>
          <metadata>
            <positionTop>
            355
        </positionTop>
            <positionLeft>
            984.5
        </positionLeft>
          </metadata>
        </task>
      </taskFlow>
    </job>