<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.12" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="CNTK_SimpleNet" onTaskError="continueJobExecution" priority="normal" projectName="2. Microsoft Cognitive Toolkit" xsi:schemaLocation="urn:proactive:jobdescriptor:3.12 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.12/schedulerjob.xsd">
  <variables>
    <variable name="NS" value=""/>
    <variable name="NS_BATCH" value=""/>
    <variable name="NODE_ACCESS_TOKEN" value=""/>
    <variable model="PA:LIST(,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable model="PA:LIST(,docker://activeeon/dlm3,docker://activeeon/cuda,docker://activeeon/cuda2,docker://activeeon/rapidsai,docker://activeeon/tensorflow:latest,docker://activeeon/tensorflow:latest-gpu,docker://activeeon/cntk:2.4-cpu-python3.5,docker://activeeon/cntk)" name="CONTAINER_IMAGE" value="docker://activeeon/cntk:2.4-cpu-python3.5"/>
    <variable model="PA:Boolean" name="CONTAINER_GPU_ENABLED" value="True"/>
    <variable model="PA:Boolean" name="CONTAINER_ROOTLESS_ENABLED" value="False"/>
    <variable name="PYTHON_COMMAND" value="/root/anaconda3/envs/cntk-py35/bin/python"/>
  </variables>
  <description>
    <![CDATA[ CNTK SimpleNet workflow trains a 2-layer fully connected deep neural network with 50 hidden dimensions per layer. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="deep-learning-workflows"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/cntk.png"/>
<info name="Documentation" value="MLOS/MLOSUserGuide.html#_microsoft_cognitive_toolkit"/>
<info name="NS" value="$NS"/>
<info name="NS_BATCH" value="$NS_BATCH"/>
<info name="NODE_ACCESS_TOKEN" value="$NODE_ACCESS_TOKEN"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="CNTK_SimpleNet">
      <description>
        <![CDATA[ Train a 2-layer fully connected deep neural network with 50 hidden dimensions per layer. 
https://www.cntk.ai/pythondocs/gettingstarted.html ]]>
      </description>
      <genericInformation>
        <info name="PYTHON_COMMAND" value="$PYTHON_COMMAND"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/cntk.png"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_cuda_universal/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
# Add 'PYTHON_COMMAND' Generic Info to run a specific Python version
# Go to http://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_python_script_engine for more info
import numpy as np
import cntk as C

from cntk.learners import sgd
from cntk.logging import ProgressPrinter
from cntk.layers import Dense, Sequential

# explicitly set the device to GPU
#from cntk.device import try_set_default_device, gpu
#try_set_default_device(gpu(0))

print('CNTK VERSION: ' + str(C.__version__))
print('CNTK DEVICES:')
print(C.all_devices())

def generate_random_data(sample_size, feature_dim, num_classes):
  # Create synthetic data using NumPy.
  Y = np.random.randint(size=(sample_size, 1), low=0, high=num_classes)

  # Make sure that the data is separable
  X = (np.random.randn(sample_size, feature_dim) + 3) * (Y + 1)
  X = X.astype(np.float32)
  # converting class 0 into the vector "1 0 0",
  # class 1 into vector "0 1 0", ...
  class_ind = [Y == class_number for class_number in range(num_classes)]
  Y = np.asarray(np.hstack(class_ind), dtype=np.float32)
  return X, Y

def ffnet():
  inputs = 2
  outputs = 2
  layers = 2
  hidden_dimension = 50

  # input variables denoting the features and label data
  features = C.input_variable((inputs), np.float32)
  label = C.input_variable((outputs), np.float32)

  # Instantiate the feedforward classification model
  my_model = Sequential ([
                  Dense(hidden_dimension, activation=C.sigmoid),
                  Dense(outputs)])
  z = my_model(features)

  ce = C.cross_entropy_with_softmax(z, label)
  pe = C.classification_error(z, label)

  # Instantiate the trainer object to drive the model training
  lr_per_minibatch = C.learning_parameter_schedule(0.125)
  progress_printer = ProgressPrinter(0)
  trainer = C.Trainer(z, (ce, pe), [sgd(z.parameters, lr=lr_per_minibatch)], [progress_printer])

  # Get minibatches of training data and perform model training
  minibatch_size = 25
  num_minibatches_to_train = 1024

  aggregate_loss = 0.0
  for i in range(num_minibatches_to_train):
    train_features, labels = generate_random_data(minibatch_size, inputs, outputs)
    # Specify the mapping of input variables in the model to actual minibatch data to be trained with
    trainer.train_minibatch({features : train_features, label : labels})
    sample_count = trainer.previous_minibatch_sample_count
    aggregate_loss += trainer.previous_minibatch_loss_average * sample_count

  last_avg_error = aggregate_loss / trainer.total_number_of_samples_seen

  test_features, test_labels = generate_random_data(minibatch_size, inputs, outputs)
  avg_error = trainer.test_minibatch({features : test_features, label : test_labels})
  print(' error rate on an unseen minibatch: {}'.format(avg_error))
  return last_avg_error, avg_error

np.random.seed(98052)
ffnet()
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            146.4453125
        </positionTop>
        <positionLeft>
            313.33984375
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2646px;
            height:3501px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-141.4453125px;left:-308.33984375px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_497" style="top: 146.453px; left: 313.356px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Train a 2-layer fully connected deep neural network with 50 hidden dimensions per layer. 
https://www.cntk.ai/pythondocs/gettingstarted.html"><img src="/automation-dashboard/styles/patterns/img/wf-icons/cntk.png" width="20px">&nbsp;<span class="name">CNTK_SimpleNet</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 366.5px; top: 176px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
