<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="MLOps_Model_Server" onTaskError="continueJobExecution" priority="normal" projectName="2. MLOps Model Server" tags="MLOps,Model Deployment,Triton,Dashboard,Service,Model Monitoring,Service Automation,Model Management" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="true" description="Name of the node on which the service will be deployed." group="Resource Management" name="NODE_NAME" value=""/>
    <variable advanced="true" description="Service instance name." group="Proactive Service Parameters" hidden="false" name="INSTANCE_NAME" value="model-server-$PA_JOB_ID"/>
    <variable advanced="true" description="The endpoint_id that will be used if PROXYFIED is set to True." group="Proactive Service Parameters" hidden="false" name="ENDPOINT_ID" value="model-server-gui-$PA_JOB_ID"/>
    <variable advanced="false" description="If True, container will run with NVIDIA GPU support." group="MLOps_Model_Server Service Configuration" hidden="false" model="PA:Boolean" name="GPU_ENABLED" value="false"/>
    <variable advanced="false" description="The index of the GPU to be used by Triton. If [all], all GPUs available are used." group="MLOps_Model_Server Service Configuration" hidden="false" model="PA:LIST(all,0,1,2,3,4,5,6,7)" name="GPU_INDEX" value="all"/>
    <variable advanced="true" description="True if a proxy is needed to protect the access to the service endpoint." group="Proactive Service Parameters" hidden="false" model="PA:Boolean" name="PROXYFIED" value="True"/>
    <variable advanced="true" description="True if an https endpoint will be exposed as the service endpoint." group="Proactive Service Parameters" hidden="false" model="PA:Boolean" name="HTTPS_ENABLED" value="False"/>
    <variable advanced="false" description="Path to the model repository." group="MLOps_Model_Server Service Configuration" hidden="false" name="MODEL_REGISTRY_PATH" value="/opt/models"/>
    <variable advanced="false" description="The model control mode determines how changes to the model repository are handled by Triton. Triton operates in one of three model control modes: NONE, EXPLICIT or POLL." group="MLOps_Model_Server Service Configuration" hidden="false" model="PA:LIST(none,explicit,poll)" name="MODEL_CONTROL_MODE" value="explicit"/>
    <variable advanced="true" description="If specified, it specifies the port number for the HTTP inference." group="MLOps_Model_Server Service Configuration" hidden="false" name="HTTP_INFERENCE_SERVICE_PORT" value="-1"/>
    <variable advanced="true" description="If specified, it specifies the port number for the GRPC inference." group="MLOps_Model_Server Service Configuration" hidden="false" name="GRPC_INFERENCE_SERVICE_PORT" value="-1"/>
    <variable advanced="true" description="If specified, it specifies the port number for the HTTP metrics." group="MLOps_Model_Server Service Configuration" hidden="false" name="METRICS_PORT" value="-1"/>
    <variable advanced="true" description="Docker image used to start the NVIDIA Triton Inference Server." group="MLOps_Model_Server Service Configuration" hidden="false" name="DOCKER_IMAGE" value="nvcr.io/nvidia/tritonserver:22.10-py3"/>
    <variable advanced="false" hidden="true" model="PA:SPEL(variables['GPU_ENABLED'].toLowerCase() == 'true' ? showVar('GPU_INDEX') : hideVar('GPU_INDEX'))" name="GPU_ENABLED_HANDLER" value=""/>
  </variables>
  <description>
    <![CDATA[ This service is based on NVIDIA Triton Inference Server which is part of the NVIDIA AI platform. Triton is an open-source inference serving software that helps standardize model deployment and execution and delivers fast and scalable AI in production. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-mlops-dashboard"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
<info name="pca.states" value="(VOID,RUNNING)"/>
<info name="Documentation" value="PSA/PSAUserGuide.html"/>
<info name="pca.service.id" value="MLOps_Model_Server"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="Start_MLOps_Model_Server_D" onTaskError="cancelJob">
      <description>
        <![CDATA[ Download NVIDIA Triton Server container image and start it. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
      </genericInformation>
      <inputFiles>
        <files accessMode="transferFromGlobalSpace" includes="certificate_mas.pem"/>
        <files accessMode="transferFromGlobalSpace" includes="key_mas.pem"/>
      </inputFiles>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_name_not_empty/raw">
            <arguments>
              <argument value="$NODE_NAME"/>
            </arguments>
          </file>
        </script>
        <script type="static">
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Check_Not_Used_By_Other_Service/raw" language="groovy"></file>
        </script>
      </selection>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
echo BEGIN "$variables_PA_TASK_NAME"

################################################################################
### THIS PART IS IMAGE SPECIFIC. IF YOU NEED TO MODIFY SOMETHING, DO IT HERE ###
DOCKER_IMAGE=$variables_DOCKER_IMAGE
GPU_ENABLED=$variables_GPU_ENABLED
GPU_INDEX=$variables_GPU_INDEX
################################################################################

HTTP_SERVER_INTERNAL_PORT=8000
GRPC_SERVER_INTERNAL_PORT=8001
PROMETHEUS_SERVER_INTERNAL_PORT=8002

HTTPS_ENABLED=$variables_HTTPS_ENABLED
INSTANCE_NAME=$variables_INSTANCE_NAME
MODEL_REGISTRY_PATH=$variables_MODEL_REGISTRY_PATH
MODEL_CONTROL_MODE=$variables_MODEL_CONTROL_MODE
HTTP_SERVER_PORT=$variables_HTTP_INFERENCE_SERVICE_PORT
GRPC_SERVER_PORT=$variables_GRPC_INFERENCE_SERVICE_PORT
PROMETHEUS_SERVER_PORT=$variables_METRICS_PORT

PATH=$PATH:/usr/sbin

GET_RANDOM_PORT(){
    PCA_SERVICES_PORT_RANGE_FILE=$variables_PA_SCHEDULER_HOME/config/pca_services_port_range
    if [[ -f "$PCA_SERVICES_PORT_RANGE_FILE" ]]; then
        read LOWERPORT UPPERPORT < $PCA_SERVICES_PORT_RANGE_FILE
    else
        read LOWERPORT UPPERPORT < /proc/sys/net/ipv4/ip_local_port_range
    fi
    while :
    do
        RND_PORT="`shuf -i $LOWERPORT-$UPPERPORT -n 1`"
        ss -lpn | grep -q ":$RND_PORT " || break
    done
    echo $RND_PORT
}

if [ "$HTTP_SERVER_PORT" -eq "-1" ]; then
    echo "[INFO] Picking a random port number for HTTP_SERVER_PORT"
    HTTP_SERVER_PORT=$(GET_RANDOM_PORT)
    echo "[INFO] HTTP_SERVER_PORT is $HTTP_SERVER_PORT"
fi

if [ "$GRPC_SERVER_PORT" -eq "-1" ]; then
    echo "[INFO] Picking a random port number for GRPC_SERVER_PORT"
    GRPC_SERVER_PORT=$(GET_RANDOM_PORT)
    echo "[INFO] GRPC_SERVER_PORT is $GRPC_SERVER_PORT"
fi

if [ "$PROMETHEUS_SERVER_PORT" -eq "-1" ]; then
    echo "[INFO] Picking a random port number for PROMETHEUS_PORT"
    PROMETHEUS_SERVER_PORT=$(GET_RANDOM_PORT)
    echo "[INFO] PROMETHEUS_SERVER_PORT is $PROMETHEUS_SERVER_PORT"
fi
echo "[INFO] The service will be initialized on port $PROMETHEUS_SERVER_PORT"

if [ -z "$INSTANCE_NAME" ]; then
    echo "[ERROR] The INSTANCE_NAME is not provided by the user. Empty value is not allowed".
    exit 1
fi

echo "Pulling $DOCKER_IMAGE"
docker pull $DOCKER_IMAGE

if [ "$(docker ps -a --format '{{.Names}}' | grep "^$INSTANCE_NAME$")" ]; then
    echo [ERROR] "$INSTANCE_NAME" is already used by another service instance.
    exit 128
else
    echo "Running $INSTANCE_NAME container"
    GPU_PARAMS=''
    if [ "${GPU_ENABLED,,}" = "true" ]; then
        if [ "${GPU_INDEX,,}" = "all" ]; then
            GPU_PARAMS="--gpus $GPU_INDEX"
        else
            GPU_PARAMS="--gpus device=$GPU_INDEX"
        fi
    fi
    MOUNT_PATH=''
    MODEL_REPOSITORY=''
    if [[ $MODEL_REGISTRY_PATH = s3://* ]]; then
        MODEL_REPOSITORY="--model-repository=$MODEL_REGISTRY_PATH"
    else
        MOUNT_PATH="-v $MODEL_REGISTRY_PATH:/models"
        MODEL_REPOSITORY="--model-repository=/models"
    fi
    MODEL_CONTROL_MODE_PARAM=''
    if [ "${MODEL_CONTROL_MODE,,}" = "none" ]; then
        MODEL_CONTROL_MODE_PARAM='--model-control-mode=none'
    fi
    if [ "${MODEL_CONTROL_MODE,,}" = "explicit" ]; then
        MODEL_CONTROL_MODE_PARAM='--model-control-mode=explicit --load-model=*'
    fi
    if [ "${MODEL_CONTROL_MODE,,}" = "poll" ]; then
        MODEL_CONTROL_MODE_PARAM='--model-control-mode=poll --repository-poll-secs=5'
    fi
    echo "docker run -d --rm --shm-size 16384m $GPU_PARAMS --name $INSTANCE_NAME -v /etc/timezone:/etc/timezone -v /etc/localtime:/etc/localtime -p $HTTP_SERVER_PORT:$HTTP_SERVER_INTERNAL_PORT -p $GRPC_SERVER_PORT:$GRPC_SERVER_INTERNAL_PORT -p $PROMETHEUS_SERVER_PORT:$PROMETHEUS_SERVER_INTERNAL_PORT $MOUNT_PATH $DOCKER_IMAGE tritonserver $MODEL_REPOSITORY $MODEL_CONTROL_MODE_PARAM"
    docker run -d --rm --shm-size 16384m $GPU_PARAMS --name $INSTANCE_NAME -v /etc/timezone:/etc/timezone -v /etc/localtime:/etc/localtime -p $HTTP_SERVER_PORT:$HTTP_SERVER_INTERNAL_PORT -p $GRPC_SERVER_PORT:$GRPC_SERVER_INTERNAL_PORT -p $PROMETHEUS_SERVER_PORT:$PROMETHEUS_SERVER_INTERNAL_PORT $MOUNT_PATH $DOCKER_IMAGE tritonserver $MODEL_REPOSITORY $MODEL_CONTROL_MODE_PARAM --response-cache-byte-size=1024

    if [ "$(docker ps -a --format '{{.Names}}' | grep "^$INSTANCE_NAME$")" ]; then
        RUNNING=$(docker inspect --format="{{ .State.Running }}" $INSTANCE_NAME 2> /dev/null)
        if [ "${RUNNING,,}" = "true" ]; then
            echo $INSTANCE_NAME > $INSTANCE_NAME"_status"
        fi
    else
        echo $INSTANCE_STATUS > $INSTANCE_NAME"_status"
    fi
fi

MODEL_SERVICE_PORT=$(docker inspect --format='{{(index (index .NetworkSettings.Ports "'$PROMETHEUS_SERVER_PORT'/tcp") 0).HostPort}}' $INSTANCE_NAME)
echo "$PROMETHEUS_SERVER_PORT" > $INSTANCE_NAME"_port_prometheus"
echo "$HTTP_SERVER_PORT" > $INSTANCE_NAME"_port_http"
echo "$GRPC_SERVER_PORT" > $INSTANCE_NAME"_port_grpc"

containerID=$(docker ps -aqf "name=^/$INSTANCE_NAME$")
echo "$containerID" > $INSTANCE_NAME"_containerID"

echo END "$variables_PA_TASK_NAME"
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <post>
        <script>
          <code language="groovy">
            <![CDATA[
/*********************************************************************************
* THIS POSTSCRIPT PROPAGATES USEFUL INFORMATION SUCH AS:                         *
* 1) SERVICE ENDPOINT (PROTOCOL://HOSTNAME:PORT)                                 *
* 2) CREDENTIALS (IF THERE ARE ANY) BY ADDING THEM TO 3RD PARTY CREDENTIALS      *
*********************************************************************************/

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.api.IpUtilsRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.Container
import org.ow2.proactive.pca.service.client.model.Endpoint
import org.ow2.proactive.pca.service.client.model.Deployment
import org.ow2.proactive.pca.service.client.model.Node
import java.net.URLEncoder;

// Acquire variables
def instanceId = variables.get("PCA_INSTANCE_ID") as long
def httpsEnabled = variables.get("HTTPS_ENABLED")
def instanceName = variables.get("INSTANCE_NAME")
def proxyfied = variables.get("PROXYFIED")
def endpointID = variables.get("ENDPOINT_ID")+"-"+instanceId
def engine = variables.get("ENGINE")

println("proxyfied:    " + proxyfied)
println("httpsEnabled: " + httpsEnabled)

try {
    // Determine Cloud Automation URL
    def pcaUrl = variables.get('PA_CLOUD_AUTOMATION_REST_URL')

    // Get schedulerapi access and acquire session id
    schedulerapi.connect()
    def sessionId = schedulerapi.getSession()

    // Connect to Cloud Automation API
    def apiClient = new ApiClient()
    apiClient.setBasePath(pcaUrl)
    def serviceInstanceRestApi = new ServiceInstanceRestApi(apiClient)

    // Implement service model
    def ipUtilsRestApi = new IpUtilsRestApi(apiClient)

    // Handle service parameters
    def hostname = ipUtilsRestApi.getMyRemotePublicAddrUsingGET(sessionId)
    if (hostname.equals("127.0.0.1")) {
        hostname = variables.get("PA_NODE_HOST")
    }
    println("hostname:    " + hostname)

    def port_prometheus = new File(instanceName+"_port_prometheus").text.trim()
    def containerUrlPrometheus = hostname+":"+port_prometheus
    def prometheusMetricsEndpoint = "/metrics"

    def port_http = new File(instanceName+"_port_http").text.trim()
    def containerUrlHttp = hostname+":"+port_http

    def port_grpc = new File(instanceName+"_port_grpc").text.trim()
    def containerUrlGrpc = hostname+":"+port_grpc

    def containerID = ""
    if (engine != null && "singularity".equalsIgnoreCase(engine)) {
        containerID = "0"
    } else {
        containerID = new File(instanceName+"_containerID").text.trim()
    }

    // Https
    if ("true".equalsIgnoreCase(httpsEnabled)){
        containerUrlPrometheusMetrics = "https://"+containerUrlPrometheus+prometheusMetricsEndpoint
    } else{
        containerUrlPrometheusMetrics = "http://"+containerUrlPrometheus+prometheusMetricsEndpoint
    }
    containerUrlPrometheusMetrics = URLDecoder.decode(containerUrlPrometheusMetrics, "UTF-8");
    containerUrlPrometheus = URLDecoder.decode(containerUrlPrometheus, "UTF-8");
    containerUrlHttp = URLDecoder.decode(containerUrlHttp, "UTF-8");
    containerUrlGrpc = URLDecoder.decode(containerUrlGrpc, "UTF-8");

    // Container
    def Container container = new Container()
    container.setId(containerID)
    container.setName(instanceName)

    // Node
    def Node node = new Node();
    node.setName(variables.get("PA_NODE_NAME"))
    node.setHost(variables.get("PA_NODE_HOST"))
    node.setNodeSourceName(variables.get("PA_NODE_SOURCE"))
    node.setUrl(variables.get("PA_NODE_URL"))

    // Endpoint (primary)
    def Endpoint endpoint = new Endpoint();
    endpointIDmetrics = endpointID + "-metrics"
    endpoint.setId(endpointIDmetrics);
    // Set the endpoint parameters according to the Proxy settings
    if ("true".equalsIgnoreCase(proxyfied)){
        proxyfiedURL = pcaUrl+"/services/"+instanceId+"/endpoints/"+endpointIDmetrics
        endpoint.setProxyfied(true);
        endpoint.setProxyfiedUrl(proxyfiedURL)
    }else{
        endpoint.setProxyfied(false)
    }
    endpoint.setUrl(containerUrlPrometheusMetrics);

    // Endpoint (secondary)
    // def Endpoint endpointPrometheusMetrics = new Endpoint();
    // endpointIDmetrics = endpointID + "-metrics"
    // endpointPrometheusMetrics.setId(endpointIDmetrics);
    // endpointPrometheusMetrics.setUrl(containerUrlPrometheusMetrics);

    def Endpoint endpointPrometheus = new Endpoint();
    endpointIDprom = endpointID + "-prom"
    endpointPrometheus.setId(endpointIDprom);
    endpointPrometheus.setUrl(containerUrlPrometheus);

    def Endpoint endpointHttp = new Endpoint();
    endpointIDhttp = endpointID + "-http"
    endpointHttp.setId(endpointIDhttp);
    endpointHttp.setUrl(containerUrlHttp);

    def Endpoint endpointGrpc = new Endpoint();
    endpointIDgrpc = endpointID + "-grpc"
    endpointGrpc.setId(endpointIDgrpc);
    endpointGrpc.setUrl(containerUrlGrpc);

    // Deployment (primary)
    def Deployment deployment = new Deployment()
    deployment.setNode(node)
    deployment.setContainer(container)
    deployment.setEndpoint(endpoint)

    // Deployment (secondary)

    // def Deployment deploymentPrometheusMetrics = new Deployment()
    // // deploymentPrometheusMetrics.setNode(node)
    // // deploymentPrometheusMetrics.setContainer(container)
    // deploymentPrometheusMetrics.setEndpoint(endpointPrometheusMetrics)

    def Deployment deploymentPrometheus = new Deployment()
    // deploymentPrometheus.setNode(node)
    // deploymentPrometheus.setContainer(container)
    deploymentPrometheus.setEndpoint(endpointPrometheus)

    def Deployment deploymentHttp = new Deployment()
    // deploymentHttp.setNode(node)
    // deploymentHttp.setContainer(container)
    deploymentHttp.setEndpoint(endpointHttp)

    def Deployment deploymentGrpc = new Deployment()
    // deploymentGrpc.setNode(node)
    // deploymentGrpc.setContainer(container)
    deploymentGrpc.setEndpoint(endpointGrpc)

    // Update service instance model (add Deployment, Groups)
    def serviceInstanceData = serviceInstanceRestApi.getServiceInstanceUsingGET(sessionId, instanceId)
    serviceInstanceData.setInstanceStatus("RUNNING")
    serviceInstanceData = serviceInstanceData.addDeploymentsItem(deployment)
    // serviceInstanceData = serviceInstanceData.addDeploymentsItem(deploymentPrometheusMetrics)
    serviceInstanceData = serviceInstanceData.addDeploymentsItem(deploymentPrometheus)
    serviceInstanceData = serviceInstanceData.addDeploymentsItem(deploymentHttp)
    serviceInstanceData = serviceInstanceData.addDeploymentsItem(deploymentGrpc)
    if ("true".equalsIgnoreCase(proxyfied)){
        serviceInstanceData = serviceInstanceData.addGroupsItem("scheduleradmins")
        serviceInstanceData = serviceInstanceData.addGroupsItem("rmcoreadmins")
    }
    serviceInstanceData = serviceInstanceRestApi.updateServiceInstanceUsingPUT(sessionId, instanceId, serviceInstanceData)
    println(serviceInstanceData)

    schedulerapi.registerService(variables.get("PA_JOB_ID"), instanceId as int, true)

    // Inform other platforms that service is running through Synchronization API
    channel = "Service_Instance_" + instanceId
    synchronizationapi.createChannelIfAbsent(channel, false)
    synchronizationapi.put(channel, "RUNNING_STATE", 1)
    synchronizationapi.put(channel, "INSTANCE_NAME", instanceName)

    // Add token to the current node
    token = "PSA_" + instanceName
    nodeUrl = variables.get("PA_NODE_URL")
    println("Current nodeUrl: " + nodeUrl)
    println("Adding token:    " + token)
    rmapi.connect()
    rmapi.addNodeToken(nodeUrl, token)

    // Log output
    println(variables.get("PA_JOB_NAME") + "_INSTANCE_ID: " + instanceId)
    println(variables.get("PA_JOB_NAME") + "_ENDPOINT: " + endpoint)
} catch (Exception e) {
    StackTraceUtils.printSanitizedStackTrace(e)
    throw e
}
println("END " + variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </post>
      <cleaning>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Clean_Start_Service/raw"/>
        </script>
      </cleaning>
      <metadata>
        <positionTop>
            166.43331909179688
        </positionTop>
        <positionLeft>
            742.566650390625
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Loop_Over_Instance_Status_D">
      <description>
        <![CDATA[ Loop over service instance status and fetch docker container logs.
It will run every minute. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_as_service.png"/>
        <info name="NODE_ACCESS_TOKEN" value="PSA_$INSTANCE_NAME"/>
      </genericInformation>
      <depends>
        <task ref="Start_MLOps_Model_Server_D"/>
      </depends>
      <scriptExecutable>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Check_Instance_Status/raw"/>
        </script>
      </scriptExecutable>
      <controlFlow>
        <loop target="Loop_Over_Instance_Status_D">
          <script>
            <code language="groovy">
              <![CDATA[
// Check if loop task has ordered to finish the loop
def isFinished = variables.get('IS_FINISHED').toBoolean()
loop = isFinished ? false : '*/1 * * * *'

// Set a time marker to fetch logs since this marker.
variables.put("LAST_TIME_MARKER",new Date().format("yyyy-MM-dd'T'HH:mm:ssXXX"))
]]>
            </code>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
            294.4333190917969
        </positionTop>
        <positionLeft>
            741.566650390625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2515px;
            height:3624px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-161.43331909179688px;left:-736.566650390625px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable active-task" style="top: 166.433px; left: 742.567px;" id="jsPlumb_1_16"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Download NVIDIA Triton Server container image and start it."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">Start_MLOps_Model_Server_D</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" style="top: 294.433px; left: 741.567px;" id="jsPlumb_1_19"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Loop over service instance status and fetch docker container logs.
It will run every minute."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_as_service.png" width="20px">&nbsp;<span class="name">Loop_Over_Instance_Status_D</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><svg style="position:absolute;left:821.5px;top:205.5px" width="22" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 88 C -10 38 11 50 1 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.73415625,66.78168750000002 L5.087187797721125,47.08837449057529 L-2.1550211532554755,52.793671109542124 L-8.900828592736769,46.50923939383077 L-2.73415625,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.73415625,66.78168750000002 L5.087187797721125,47.08837449057529 L-2.1550211532554755,52.793671109542124 L-8.900828592736769,46.50923939383077 L-2.73415625,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:911.5px;top:283.5px" width="61" height="61" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 40 C 50 -10 -10 50 0 0 " transform="translate(10.5,10.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#316b31" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M13.55903999999999,26.37184000000001 L31.950657715430147,15.848133532719789 L22.73126656242089,15.79496617605654 L21.37378389148668,6.675906970298892 L13.55903999999999,26.37184000000001" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,10.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M13.55903999999999,26.37184000000001 L31.950657715430147,15.848133532719789 L22.73126656242089,15.79496617605654 L21.37378389148668,6.675906970298892 L13.55903999999999,26.37184000000001" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,10.5)"></path></svg><div style="position: absolute; transform: translate(-50%, -50%); left: 936.5px; top: 313.5px;" class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_30">loop</div><div style="position: absolute; height: 20px; width: 20px; left: 823px; top: 196px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 822px; top: 324px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 822px; top: 284px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 912px; top: 284px;" class="_jsPlumb_endpoint source-endpoint loop-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 912px; top: 324px;" class="_jsPlumb_endpoint target-endpoint loop-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
