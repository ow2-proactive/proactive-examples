<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="MLOps_Model_Server_Service_Start" tags="MLOps,Dashboard,Model Management,Model Deployment,Model Monitoring,Triton,Service,Service Automation" onTaskError="continueJobExecution" priority="normal" projectName="4. MLOps Lifecycle Workflows" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="true" description="Name of the node on which the service will be deployed." group="Resource Management" name="NODE_NAME" value=""/>
    <variable advanced="true" description="Service instance name." group="Proactive Service Parameters" hidden="false" name="INSTANCE_NAME" value="maas-server-$PA_JOB_ID"/>
    <variable advanced="true" description="The endpoint_id that will be used if PROXYFIED is set to True." group="Proactive Service Parameters" hidden="false" name="ENDPOINT_ID" value="maas-server-gui-$PA_JOB_ID"/>
    <variable advanced="false" description="If True, container will run with NVIDIA GPU support." group="MLOps_Model_Server Service Configuration" hidden="false" model="PA:Boolean" name="GPU_ENABLED" value="false"/>
    <variable advanced="false" description="The index of the GPU to be used by Triton. If [all], all GPUs available are used." group="MLOps_Model_Server Service Configuration" hidden="false" model="PA:LIST(all,0,1,2,3,4,5,6,7)" name="GPU_INDEX" value="all"/>
    <variable advanced="true" description="True if a proxy is needed to protect the access to the service endpoint." group="Proactive Service Parameters" hidden="false" model="PA:Boolean" name="PROXYFIED" value="True"/>
    <variable advanced="true" description="True if an https endpoint will be exposed as the service endpoint." group="Proactive Service Parameters" hidden="false" model="PA:Boolean" name="HTTPS_ENABLED" value="False"/>
    <variable advanced="false" description="Path to the model repository." group="MLOps_Model_Server Service Configuration" hidden="false" name="MODEL_REGISTRY_PATH" value="/tmp/models"/>
    <variable advanced="false" description="The model control mode determines how changes to the model repository are handled by Triton. Triton operates in one of three model control modes: NONE, EXPLICIT or POLL." group="MLOps_Model_Server Service Configuration" hidden="false" model="PA:LIST(none,explicit,poll)" name="MODEL_CONTROL_MODE" value="explicit"/>
    <variable advanced="true" description="If specified, it specifies the port number for the HTTP inference." group="MLOps_Model_Server Service Configuration" hidden="false" name="HTTP_INFERENCE_SERVICE_PORT" value="-1"/>
    <variable advanced="true" description="If specified, it specifies the port number for the GRPC inference." group="MLOps_Model_Server Service Configuration" hidden="false" name="GRPC_INFERENCE_SERVICE_PORT" value="-1"/>
    <variable advanced="true" description="If specified, it specifies the port number for the HTTP metrics." group="MLOps_Model_Server Service Configuration" hidden="false" name="METRICS_PORT" value="-1"/>
    <variable advanced="true" description="Docker image used to start the NVIDIA Triton Inference Server." group="MLOps_Model_Server Service Configuration" hidden="false" name="DOCKER_IMAGE" value="nvcr.io/nvidia/tritonserver:22.10-py3"/>
    <variable advanced="false" hidden="true" model="PA:SPEL(variables['GPU_ENABLED'].toLowerCase() == 'true' ? showVar('GPU_INDEX') : hideVar('GPU_INDEX'))" name="GPU_ENABLED_HANDLER" value=""/>
  </variables>
  <description>
    <![CDATA[ Start a server to deploy NVIDIA Triton models. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-mlops-dashboard"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
<info name="NODESOURCENAME" value=""/>
<info name="NS" value=""/>
<info name="PYTHON_COMMAND" value="python3"/>
<info name="Documentation" value="PAIO/PAIOUserGuide.html#_start_a_generic_service_instance"/>
<info name="NODE_ACCESS_TOKEN" value=""/>
<info name="NS_BATCH" value=""/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="MLOps_Model_Server_Service_Start" onTaskError="cancelJob">
      <description>
        <![CDATA[ Start NVIDIA Triton model server to deploy AI models. ]]>
      </description>
      <variables>
        <variable advanced="false" description="The service activation workflow. Please keep the default value for this variable." group="Service Parameters" hidden="false" inherited="false" model="PA:CATALOG_OBJECT(Workflow/psa,,,MLOps_Model_Server%)" name="SERVICE_ACTIVATION_WORKFLOW" value="ai-mlops-dashboard/MLOps_Model_Server"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html#_start_a_generic_service_instance"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Service_Start/raw">
            <arguments>
              <argument value="true"/>
              <argument value="NATIVE_SCHEDULER"/>
              <argument value="NATIVE_SCHEDULER_PARAMS"/>
              <argument value="NODE_NAME"/>
              <argument value="NODE_SOURCE_NAME"/>
              <argument value="NODE_ACCESS_TOKEN"/>
              <argument value="PROXYFIED"/>
              <argument value="HTTPS_ENABLED"/>
              <argument value="GPU_ENABLED"/>
              <argument value="GPU_INDEX"/>
              <argument value="MODEL_REGISTRY_PATH"/>
              <argument value="MODEL_CONTROL_MODE"/>
              <argument value="HTTP_INFERENCE_SERVICE_PORT"/>
              <argument value="GRPC_INFERENCE_SERVICE_PORT"/>
              <argument value="METRICS_PORT"/>
              <argument value="DOCKER_IMAGE"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            232.78334045410156
        </positionTop>
        <positionLeft>
            191.4000244140625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2703px;
            height:4048px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-227.78334045410156px;left:-186.4000244140625px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable active-task" style="top: 232.783px; left: 191.4px;" id="jsPlumb_1_164"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Start NVIDIA Triton model server to deploy AI models."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">MLOps_Model_Server_Service_Start</span></a></div><div style="position: absolute; height: 20px; width: 20px; left: 259.5px; top: 263px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
