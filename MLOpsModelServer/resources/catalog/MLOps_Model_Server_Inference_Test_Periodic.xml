<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="MLOps_Model_Server_Inference_Test_Periodic" onTaskError="continueJobExecution" priority="normal" projectName="3. MLOps Model Server Workflows" tags="MLOps,Dashboard,Model Management,Model Deployment,Model Monitoring,Triton,Service,Service Automation" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="false" description="GRPC inference url of the model server (e.g. localhost:8001)." group="Model Server" hidden="false" model="" name="GRPC_INFERENCE_URL" value=""/>
    <variable advanced="false" description="Name of the model to be tested." group="Model Server" hidden="false" model="PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)" name="MODEL_NAME" value="simple"/>
    <variable advanced="false" description="Handler for image path" group="Model Server" hidden="true" model="PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'densenet_onnx' || variables['MODEL_NAME'].toLowerCase() == 'inception_graphdef' ? showVar('IMAGE_PATH') : hideVar('IMAGE_PATH'))" name="IMAGE_PATH_HANDLER" value=""/>
    <variable advanced="false" description="Path of the image to be used for inference." group="Model Server" hidden="false" model="PA:URL" name="IMAGE_PATH" value="https://activeeon-public.s3.eu-west-2.amazonaws.com/images/bee.jpg"/>
    <variable advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(,nvcr.io/nvidia/tritonserver:22.10-py3-sdk)" name="CONTAINER_IMAGE" value="nvcr.io/nvidia/tritonserver:22.10-py3-sdk"/>
    <variable advanced="false" description="The frequency (in ms) to send a new inference request for the selected model." group="Model Server" hidden="false" model="PA:Integer" name="INFERENCE_FREQUENCY" value="30000"/>
  </variables>
  <description>
    <![CDATA[ Simple workflow for MLOps Model Server inference test periodically. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-mlops-dashboard"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
<info name="Documentation" value="PAIO/PAIOUserGuide.html"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="MLOps_Model_Server_Inference_Test" runAsMe="true">
      <description>
        <![CDATA[ Simple task for MLOps Model Server inference test. ]]>
      </description>
      <variables>
        <variable advanced="false" hidden="false" inherited="false" name="TASK_FILE_PATH" value="main.py"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html"/>
        <info name="PRE_SCRIPT_AS_FILE" value="$TASK_FILE_PATH"/>
      </genericInformation>
      <depends>
        <task ref="Start"/>
      </depends>
      <selection>
        <script type="static">
          <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/check_node_source_name/raw"/>
        </script>
      </selection>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[
import os
import sys
import argparse
import uuid
import queue
import gevent.ssl
import numpy as np

from PIL import Image
from PIL.Image import Resampling

import tritonclient.http as httpclient
import tritonclient.grpc as grpcclient
import grpc

import tritonclient.grpc.model_config_pb2 as mc

from tritonclient.grpc import service_pb2, service_pb2_grpc
from tritonclient.utils import InferenceServerException
from tritonclient.utils import triton_to_np_dtype

from tritonclient import utils


def test_model_simple(args, triton_client):
    # Infer
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "INT32"))
    inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "INT32"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    input0_data = np.arange(start=0, stop=16, dtype=np.int32)
    input0_data = np.expand_dims(input0_data, axis=0)
    input1_data = np.ones(shape=(1, 16), dtype=np.int32)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Test with outputs
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    # print("response:\n", results.get_response())
    statistics = triton_client.get_inference_statistics(model_name=args.model_name)
    # print("statistics:\n", statistics)
    if len(statistics.model_stats) != 1:
        print("FAILED: Inference Statistics")
        sys.exit(1)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    print("model-outputs:")
    for i in range(16):
        print(str(input0_data[0][i]) + " + " + str(input1_data[0][i]) + " = " + str(output0_data[0][i]))
        print(str(input0_data[0][i]) + " - " + str(input1_data[0][i]) + " = " + str(output1_data[0][i]))
        if (input0_data[0][i] + input1_data[0][i]) != output0_data[0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[0][i] - input1_data[0][i]) != output1_data[0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple')


def test_model_simple_identity(args, triton_client):
    # Example using BYTES input tensor with utf-8 encoded string that
    # has an embedded null character.
    # null_chars_array = np.array(["he\x00llo".encode('utf-8') for i in range(16)], dtype=np.object_)
    # null_char_data = null_chars_array.reshape([1, 16])
    null_chars_array = np.array(["he\x00llo".encode('utf-8') for i in range(1)], dtype=np.object_)
    null_char_data = null_chars_array.reshape([1, 1])
    # Example using BYTES input tensor with 16 elements, where each
    # element is a 4-byte binary blob with value 0x00010203. Can use dtype=np.bytes_ in this case.
    # bytes_data = [b'\x00\x01\x02\x03' for i in range(16)]
    # np_bytes_data = np.array(bytes_data, dtype=np.bytes_)
    # np_bytes_data = np_bytes_data.reshape([1, 16])
    np_array = null_char_data
    # np_array = np_bytes_data
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', np_array.shape, "BYTES"))
    inputs[0].set_data_from_numpy(np_array)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    if (np_array.dtype == np.object_):
        print(results.as_numpy('OUTPUT0'))
        if not np.array_equal(np_array, results.as_numpy('OUTPUT0')):
            print(results.as_numpy('OUTPUT0'))
            print("error: incorrect output")
            sys.exit(1)
    else:
        encoded_results = np.char.encode(results.as_numpy('OUTPUT0').astype(str))
        print(encoded_results)
        if not np.array_equal(np_array, encoded_results):
            print("error: incorrect output")
            sys.exit(1)
    print('PASS: simple_identity')


def test_model_simple_int8(args, triton_client):
    # We use a simple model that takes 2 input tensors of 16 integers
    # each and returns 2 output tensors of 16 integers each. One
    # output tensor is the element-wise sum of the inputs and one
    # output is the element-wise difference.
    # model_name = "simple_int8"
    model_version = ""
    batch_size = 1
    # Create gRPC stub for communicating with the server
    channel = grpc.insecure_channel(args.model_server)
    grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)
    # Generate the request
    request = service_pb2.ModelInferRequest()
    request.model_name = args.model_name
    request.model_version = model_version
    # Input data
    input0_data = [i for i in range(16)]
    input1_data = [1 for i in range(16)]
    # Populate the inputs in inference request
    input0 = service_pb2.ModelInferRequest().InferInputTensor()
    input0.name = "INPUT0"
    input0.datatype = "INT8"
    input0.shape.extend([1, 16])
    input0.contents.int_contents[:] = input0_data
    input1 = service_pb2.ModelInferRequest().InferInputTensor()
    input1.name = "INPUT1"
    input1.datatype = "INT8"
    input1.shape.extend([1, 16])
    input1.contents.int_contents[:] = input1_data
    request.inputs.extend([input0, input1])
    # Populate the outputs in the inference request
    output0 = service_pb2.ModelInferRequest().InferRequestedOutputTensor()
    output0.name = "OUTPUT0"
    output1 = service_pb2.ModelInferRequest().InferRequestedOutputTensor()
    output1.name = "OUTPUT1"
    request.outputs.extend([output0, output1])
    response = grpc_stub.ModelInfer(request)
    output_results = []
    index = 0
    for output in response.outputs:
        shape = []
        for value in output.shape:
            shape.append(value)
        output_results.append(
            np.frombuffer(response.raw_output_contents[index], dtype=np.int8))
        output_results[-1] = np.resize(output_results[-1], shape)
        index += 1
    if len(output_results) != 2:
        print("expected two output results")
        sys.exit(1)
    for i in range(16):
        print(str(input0_data[i]) + " + " + str(input1_data[i]) + " = " + str(output_results[0][0][i]))
        print(str(input0_data[i]) + " - " + str(input1_data[i]) + " = " + str(output_results[1][0][i]))
        if (input0_data[i] + input1_data[i]) != output_results[0][0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[i] - input1_data[i]) != output_results[1][0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple_int8')


def test_model_simple_sequence(args, triton_client):
    # UserData class
    class UserData:
        def __init__(self):
            self._completed_requests = queue.Queue()
    # sync_send function
    def sync_send(triton_client, result_list, values, batch_size, sequence_id, model_name, model_version):
        count = 1
        for value in values:
            # Create the tensor for INPUT
            value_data = np.full(shape=[batch_size, 1], fill_value=value, dtype=np.int32)
            inputs = []
            inputs.append(grpcclient.InferInput('INPUT', value_data.shape, "INT32"))
            # Initialize the data
            inputs[0].set_data_from_numpy(value_data)
            outputs = []
            outputs.append(grpcclient.InferRequestedOutput('OUTPUT'))
            # Issue the synchronous sequence inference.
            result = triton_client.infer(model_name=model_name,
                                        inputs=inputs,
                                        outputs=outputs,
                                        sequence_id=sequence_id,
                                        sequence_start=(count == 1),
                                        sequence_end=(count == len(values)))
            result_list.append(result.as_numpy('OUTPUT'))
            count = count + 1
    # We use custom "sequence" models which take 1 input
    # value. The output is the accumulated value of the inputs. See
    # src/custom/sequence.
    offset = 0
    dyna = False
    int_sequence_model_name = "simple_dyna_sequence" if dyna else "simple_sequence"
    string_sequence_model_name = "simple_string_dyna_sequence" if dyna else "simple_sequence"
    model_version = ""
    batch_size = 1
    # values = [11, 7, 5, 3, 2, 0, 1]
    values = [11] # 6 inferences
    # Will use two sequences and send them synchronously. Note the
    # sequence IDs should be non-zero because zero is reserved for
    # non-sequence requests.
    int_sequence_id0 = 1000 + offset * 2
    int_sequence_id1 = 1001 + offset * 2
    # For string sequence IDs, the dyna backend requires that the
    # sequence id be decodable into an integer, otherwise we'll use
    # a UUID4 sequence id and a model that doesn't require corrid
    # control.
    string_sequence_id0 = str(1002) if dyna else str(uuid.uuid4())
    int_result0_list = []
    int_result1_list = []
    string_result0_list = []
    user_data = UserData()
    try:
        sync_send(triton_client, int_result0_list, 
                  values, batch_size,
                  int_sequence_id0, int_sequence_model_name, model_version)
        # sync_send(triton_client, int_result0_list, 
        #           [0] + values, batch_size,
        #           int_sequence_id0, int_sequence_model_name, model_version)
        # sync_send(triton_client, int_result1_list,
        #           [100] + [-1 * val for val in values], batch_size,
        #           int_sequence_id1, int_sequence_model_name, model_version)
        # sync_send(triton_client, string_result0_list,
        #           [20] + [-1 * val for val in values], batch_size,
        #           string_sequence_id0, string_sequence_model_name,
        #           model_version)
    except InferenceServerException as error:
        print(error)
        sys.exit(1)
    for i in range(len(int_result0_list)):
        int_seq0_expected = 1 if (i == 0) else values[i - 1]
        int_seq1_expected = 101 if (i == 0) else values[i - 1] * -1
        # For string sequence ID we are testing two different backends
        if i == 0 and dyna:
            string_seq0_expected = 20
        elif i == 0 and not dyna:
            string_seq0_expected = 21
        elif i != 0 and dyna:
            string_seq0_expected = values[i - 1] * -1 + int(
                string_result0_list[i - 1][0][0])
        else:
            string_seq0_expected = values[i - 1] * -1
        # The dyna_sequence custom backend adds the correlation ID
        # to the last request in a sequence.
        if dyna and (i != 0) and (values[i - 1] == 1):
            int_seq0_expected += int_sequence_id0
            # int_seq1_expected += int_sequence_id1
            # string_seq0_expected += int(string_sequence_id0)
        # print("[" + str(i) + "] " + str(int_result0_list[i][0][0]) + " : " +
        #       str(int_result1_list[i][0][0]) + " : " +
        #       str(string_result0_list[i][0][0]))
        print("[" + str(i) + "] " + str(int_result0_list[i][0][0]))
        # if ((int_seq0_expected != int_result0_list[i][0][0]) or
        #     (int_seq1_expected != int_result1_list[i][0][0]) or
        #     (string_seq0_expected != string_result0_list[i][0][0])):
        # if ((int_seq0_expected != int_result0_list[i][0][0])):
        #     print("[ expected ] " + str(int_seq0_expected) + " : " +
        #           str(int_seq1_expected) + " : " + str(string_seq0_expected))
        #     sys.exit(1)
    print("PASS: simple_sequence")


def test_model_simple_string(args, triton_client):
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "BYTES"))
    inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "BYTES"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    in0 = np.arange(start=0, stop=16, dtype=np.int32)
    in0 = np.expand_dims(in0, axis=0)
    in1 = np.ones(shape=(1, 16), dtype=np.int32)
    expected_sum = np.add(in0, in1)
    expected_diff = np.subtract(in0, in1)
    # The 'simple_string' model expects 2 BYTES tensors where each
    # element in those tensors is the utf-8 string representation of
    # an integer. The BYTES tensors must be represented by a numpy
    # array with dtype=np.object_.
    in0n = np.array([str(x).encode('utf-8') for x in in0.reshape(in0.size)], dtype=np.object_)
    input0_data = in0n.reshape(in0.shape)
    in1n = np.array([str(x).encode('utf-8') for x in in1.reshape(in1.size)], dtype=np.object_)
    input1_data = in1n.reshape(in1.shape)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Make inference
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    for i in range(16):
        print(str(input0_data[0][i]) + " + " + str(input1_data[0][i]) + " = " + str(output0_data[0][i]))
        print(str(input0_data[0][i]) + " - " + str(input1_data[0][i]) + " = " + str(output1_data[0][i]))
        # Convert result from string to int to check result
        r0 = int(output0_data[0][i])
        r1 = int(output1_data[0][i])
        if expected_sum[0][i] != r0:
            print("error: incorrect sum")
            sys.exit(1)
        if expected_diff[0][i] != r1:
            print("error: incorrect difference")
            sys.exit(1)
    print("PASS: simple_string")


def test_model_image_classification(args, triton_client):
    def parse_model(model_metadata, model_config):
        """
        Check the configuration of a model to make sure it meets the
        requirements for an image classification network (as expected by
        this client)
        """
        if len(model_metadata.inputs) != 1:
            raise Exception("expecting 1 input, got {}".format(
                len(model_metadata.inputs)))
        if len(model_metadata.outputs) != 1:
            raise Exception("expecting 1 output, got {}".format(
                len(model_metadata.outputs)))
        if len(model_config.input) != 1:
            raise Exception(
                "expecting 1 input in model configuration, got {}".format(
                    len(model_config.input)))
        input_metadata = model_metadata.inputs[0]
        input_config = model_config.input[0]
        output_metadata = model_metadata.outputs[0]
        if output_metadata.datatype != "FP32":
            raise Exception("expecting output datatype to be FP32, model '" +
                            model_metadata.name + "' output type is " +
                            output_metadata.datatype)
        # Output is expected to be a vector. But allow any number of
        # dimensions as long as all but 1 is size 1 (e.g. { 10 }, { 1, 10
        # }, { 10, 1, 1 } are all ok). Ignore the batch dimension if there
        # is one.
        output_batch_dim = (model_config.max_batch_size > 0)
        non_one_cnt = 0
        for dim in output_metadata.shape:
            if output_batch_dim:
                output_batch_dim = False
            elif dim > 1:
                non_one_cnt += 1
                if non_one_cnt > 1:
                    raise Exception("expecting model output to be a vector")
        # Model input must have 3 dims, either CHW or HWC (not counting
        # the batch dimension), either CHW or HWC
        input_batch_dim = (model_config.max_batch_size > 0)
        expected_input_dims = 3 + (1 if input_batch_dim else 0)
        if len(input_metadata.shape) != expected_input_dims:
            raise Exception(
                "expecting input to have {} dimensions, model '{}' input has {}".
                format(expected_input_dims, model_metadata.name,
                    len(input_metadata.shape)))
        if type(input_config.format) == str:
            FORMAT_ENUM_TO_INT = dict(mc.ModelInput.Format.items())
            input_config.format = FORMAT_ENUM_TO_INT[input_config.format]
        if ((input_config.format != mc.ModelInput.FORMAT_NCHW) and
            (input_config.format != mc.ModelInput.FORMAT_NHWC)):
            raise Exception("unexpected input format " +
                            mc.ModelInput.Format.Name(input_config.format) +
                            ", expecting " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NCHW) +
                            " or " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NHWC))
        if input_config.format == mc.ModelInput.FORMAT_NHWC:
            h = input_metadata.shape[1 if input_batch_dim else 0]
            w = input_metadata.shape[2 if input_batch_dim else 1]
            c = input_metadata.shape[3 if input_batch_dim else 2]
        else:
            c = input_metadata.shape[1 if input_batch_dim else 0]
            h = input_metadata.shape[2 if input_batch_dim else 1]
            w = input_metadata.shape[3 if input_batch_dim else 2]
        return (model_config.max_batch_size, input_metadata.name,
                output_metadata.name, c, h, w, input_config.format,
                input_metadata.datatype)
    def preprocess(img, format, dtype, c, h, w, scaling):
        """
        Pre-process an image to meet the size, type and format
        requirements specified by the parameters.
        """
        # np.set_printoptions(threshold='nan')
        if c == 1:
            sample_img = img.convert('L')
        else:
            sample_img = img.convert('RGB')
        resized_img = sample_img.resize((w, h), Resampling.BILINEAR) # Image.BILINEAR will be removed in Pillow 10 (2023-07-01)
        resized = np.array(resized_img)
        if resized.ndim == 2:
            resized = resized[:, :, np.newaxis]
        npdtype = triton_to_np_dtype(dtype)
        typed = resized.astype(npdtype)
        if scaling == 'INCEPTION':
            scaled = (typed / 127.5) - 1
        elif scaling == 'VGG':
            if c == 1:
                scaled = typed - np.asarray((128,), dtype=npdtype)
            else:
                scaled = typed - np.asarray((123, 117, 104), dtype=npdtype)
        else:
            scaled = typed
        # Swap to CHW if necessary
        if format == mc.ModelInput.FORMAT_NCHW:
            ordered = np.transpose(scaled, (2, 0, 1))
        else:
            ordered = scaled
        # Channels are in RGB order. Currently model configuration data
        # doesn't provide any information as to other channel orderings
        # (like BGR) so we just assume RGB.
        return ordered
    class UserData:
        def __init__(self):
            self._completed_requests = queue.Queue()
    def requestGenerator(client, batched_image_data, input_name, output_name, dtype, args):
        # Set the input data
        inputs = [grpcclient.InferInput(input_name, batched_image_data.shape, dtype)]
        inputs[0].set_data_from_numpy(batched_image_data)
        outputs = [
            grpcclient.InferRequestedOutput(output_name, class_count=args.classes)
        ]
        yield inputs, outputs, args.model_name, args.model_version
    def postprocess(results, output_name, batch_size, supports_batching):
        """
        Post-process results to show classifications.
        """
        output_array = results.as_numpy(output_name)
        if supports_batching and len(output_array) != batch_size:
            raise Exception("expected {} results, got {}".format(batch_size, len(output_array)))
        # Include special handling for non-batching models
        for results in output_array:
            if not supports_batching:
                results = [results]
            for result in results:
                if output_array.dtype.type == np.object_:
                    cls = "".join(chr(x) for x in result).split(':')
                else:
                    cls = result.split(':')
                print("    {} ({}) = {}".format(cls[0], cls[1], cls[2]))
    # Make sure the model matches our requirements, and get some
    # properties of the model that we need for preprocessing
    try:
        model_metadata = triton_client.get_model_metadata(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the metadata: " + str(e))
        sys.exit(1)
    # print('-'*30, "\nmodel_metadata:\n", model_metadata, '-'*30)
    try:
        model_config = triton_client.get_model_config(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the config: " + str(e))
        sys.exit(1)
    # print('-'*30, "\nmodel_config:\n", model_config, '-'*30)
    model_config = model_config.config
    max_batch_size, input_name, output_name, c, h, w, format, dtype = parse_model(model_metadata, model_config)
    supports_batching = max_batch_size > 0
    if not supports_batching and args.batch_size != 1:
        print("ERROR: This model doesn't support batching.")
        sys.exit(1)
    if args.image_path is None:
        print("ERROR: This model requires an input image.")
        sys.exit(1)
    filenames = []
    if os.path.isdir(args.image_path):
        filenames = [
            os.path.join(args.image_path, f)
            for f in os.listdir(args.image_path)
            if os.path.isfile(os.path.join(args.image_path, f))
        ]
    else:
        filenames = [
            args.image_path,
        ]
    assert(len(filenames) > 0)
    filenames.sort()
    # print("filenames:\n", filenames)
    # Preprocess the images into input data according to model
    # requirements
    image_data = []
    for filename in filenames:
        img = Image.open(filename)
        image_data.append(preprocess(img, format, dtype, c, h, w, args.scaling))
    # Send requests of args.batch_size images. If the number of
    # images isn't an exact multiple of args.batch_size then just
    # start over with the first images until the batch is filled.
    requests = []
    responses = []
    result_filenames = []
    request_ids = []
    image_idx = 0
    last_request = False
    user_data = UserData()
    # Holds the handles to the ongoing HTTP async requests.
    async_requests = []
    sent_count = 0
    # if args.streaming:
    #     triton_client.start_stream(partial(completion_callback, user_data))
    while not last_request:
        input_filenames = []
        repeated_image_data = []
        for idx in range(args.batch_size):
            input_filenames.append(filenames[image_idx])
            repeated_image_data.append(image_data[image_idx])
            image_idx = (image_idx + 1) % len(image_data)
            if image_idx == 0:
                last_request = True
        if supports_batching:
            batched_image_data = np.stack(repeated_image_data, axis=0)
        else:
            batched_image_data = repeated_image_data[0]
        # Send request
        try:
            for inputs, outputs, model_name, model_version in requestGenerator(
                    triton_client, batched_image_data, input_name, output_name, dtype, args):
                sent_count += 1
                responses.append(
                        triton_client.infer(args.model_name,
                                            inputs,
                                            request_id=str(sent_count),
                                            model_version=args.model_version,
                                            outputs=outputs))
                # if args.streaming:
                #     triton_client.async_stream_infer(
                #         args.model_name,
                #         inputs,
                #         request_id=str(sent_count),
                #         model_version=args.model_version,
                #         outputs=outputs)
                # elif args.async_set:
                #     if args.protocol.lower() == "grpc":
                #         triton_client.async_infer(
                #             args.model_name,
                #             inputs,
                #             partial(completion_callback, user_data),
                #             request_id=str(sent_count),
                #             model_version=args.model_version,
                #             outputs=outputs)
                #     else:
                #         async_requests.append(
                #             triton_client.async_infer(
                #                 args.model_name,
                #                 inputs,
                #                 request_id=str(sent_count),
                #                 model_version=args.model_version,
                #                 outputs=outputs))
                # else:
                #     responses.append(
                #         triton_client.infer(args.model_name,
                #                             inputs,
                #                             request_id=str(sent_count),
                #                             model_version=args.model_version,
                #                             outputs=outputs))
        except InferenceServerException as e:
            print("inference failed: " + str(e))
            # if args.streaming:
            #     triton_client.stop_stream()
            sys.exit(1)
    # if args.streaming:
    #     triton_client.stop_stream()
    # if args.protocol.lower() == "grpc":
    #     if args.streaming or args.async_set:
    #         processed_count = 0
    #         while processed_count < sent_count:
    #             (results, error) = user_data._completed_requests.get()
    #             processed_count += 1
    #             if error is not None:
    #                 print("inference failed: " + str(error))
    #                 sys.exit(1)
    #             responses.append(results)
    # else:
    #     if args.async_set:
    #         # Collect results from the ongoing async requests
    #         # for HTTP Async requests.
    #         for async_request in async_requests:
    #             responses.append(async_request.get_result())
    for response in responses:
        this_id = response.get_response().id
        # if args.protocol.lower() == "grpc":
        #     this_id = response.get_response().id
        # else:
        #     this_id = response.get_response()["id"]
        print("Request {}, batch size {}".format(this_id, args.batch_size))
        postprocess(response, output_name, args.batch_size, supports_batching)
    print("PASS: " + args.model_name)


def main(args):
    print("model-server: ", args.model_server)
    print("model-name:   ", args.model_name)
    try:
        # triton_client = httpclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
        triton_client = grpcclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
    except Exception as e:
        print("channel creation failed: " + str(e))
        sys.exit(1)
    if args.model_name == "simple":
        test_model_simple(args, triton_client)
    elif args.model_name == "simple_identity":
        test_model_simple_identity(args, triton_client)
    elif args.model_name == "simple_int8":
        test_model_simple_int8(args, triton_client)
    elif args.model_name == "simple_sequence":
        test_model_simple_sequence(args, triton_client)
    elif args.model_name == "simple_string":
        test_model_simple_string(args, triton_client)
    elif args.model_name == "inception_graphdef" or args.model_name == "densenet_onnx":
        test_model_image_classification(args, triton_client)
    else:
        print("Invalid model name: " + args.model_name)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-s',
                        '--model-server',
                        type=str,
                        required=True,
                        help='Name of the model server.')
    parser.add_argument('-n',
                        '--model-name',
                        type=str,
                        required=True,
                        help='Name of the deployed model to be tested.')
    parser.add_argument('-x',
                        '--model-version',
                        type=str,
                        required=False,
                        default="",
                        help='Version of model. Default is to use latest version.')
    parser.add_argument('-v',
                        '--verbose',
                        action="store_true",
                        required=False,
                        default=False,
                        help='Enable verbose output.')
    parser.add_argument('-b',
                        '--batch-size',
                        type=int,
                        required=False,
                        default=1,
                        help='Batch size. Default is 1.')
    parser.add_argument('-l',
                        '--scaling',
                        type=str,
                        choices=['NONE', 'INCEPTION', 'VGG'],
                        required=False,
                        default='INCEPTION',
                        help='Type of scaling to apply to image pixels. Default is INCEPTION (only for image-based models).')
    parser.add_argument('-i',
                        '--image_path',
                        type=str,
                        nargs='?',
                        default=None,
                        help='Input image / Input folder (mandatory for image-based models).')
    parser.add_argument('-c',
                        '--classes',
                        type=int,
                        required=False,
                        default=3,
                        help='Number of class results to report (only for image-based models). Default is 3.')
    args = parser.parse_args()
    main(args)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="bash">
            <![CDATA[
echo "---"
echo "Debugging info"
python -V
which python
pip -V
echo "---"
if [ $variables_MODEL_NAME = "inception_graphdef" ] || [ $variables_MODEL_NAME = "densenet_onnx" ]; then
	if [ -z "$variables_IMAGE_PATH" ]; then
		echo "IMAGE_PATH is empty!"
		exit -1
    else
		echo "Downloading $variables_IMAGE_PATH as image.png"
		echo "wget -O image.png $variables_IMAGE_PATH"
		wget -O image.png $variables_IMAGE_PATH
		echo "python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME -i image.png"
		python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME -i image.png
    fi
else
	echo "python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME"
	python $variables_TASK_FILE_PATH -s $variables_GRPC_INFERENCE_URL -n $variables_MODEL_NAME
fi
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            427.26666259765625
        </positionTop>
        <positionLeft>
            261.683349609375
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Wait_for_Signal" runAsMe="true">
      <description>
        <![CDATA[ A template task that sends a ready notification for all the signals specified in the variable SIGNALS, then loops until one signal among those specified is received by the job. ]]>
      </description>
      <variables>
        <variable advanced="false" description="The list of comma-separated signals expected by this task." group="" hidden="false" inherited="false" name="SIGNALS" value="Terminate_Job,Update_Inference_Parameters"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/signal-wait.png"/>
        <info name="TASK.DOCUMENTATION" value="user/ProActiveUserGuide.html#_task_signal_api"/>
      </genericInformation>
      <depends>
        <task ref="Init"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import com.google.common.base.Splitter;
import org.ow2.proactive.scheduler.common.job.JobVariable;

public static boolean containsIgnoreCase(String str, String searchStr) {
    if(str == null || searchStr == null) return false;
    final int length = searchStr.length();
    if (length == 0)
        return true;
    for (int i = str.length() - length; i >= 0; i--) {
        if (str.regionMatches(true, i, searchStr, 0, length))
            return true;
    }
    return false;
}

if (variables.get("PA_TASK_ITERATION_SIGNAL") == null) {
	variables.put("PA_TASK_ITERATION_SIGNAL", 0)
}

if (variables.get("PA_TASK_ITERATION_SIGNAL") == 0) {
    List <JobVariable> signalVariables = new java.util.ArrayList<JobVariable>()
    signalVariables.add(new JobVariable("IMAGE_PATH", "https://activeeon-public.s3.eu-west-2.amazonaws.com/images/bee.jpg", "PA:URL", "Path of the image to be used for inference.", "", false, false))
    signalVariables.add(new JobVariable("IMAGE_PATH_HANDLER", "", "PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'densenet_onnx' || variables['MODEL_NAME'].toLowerCase() == 'inception_graphdef' ? showVar('IMAGE_PATH') : hideVar('IMAGE_PATH'))", "Handler for IMAGE_PATH variable.", "", false, true))
    signalVariables.add(new JobVariable("MODEL_NAME", "simple", "PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)", "Name of the model to be tested.", "", false, false))

    // Read the variable SIGNALS
    signals = variables.get("SIGNALS")

    // Split the value of the variable SIGNALS and transform it into a list
    Set signalsSet = new HashSet<>(Splitter.on(',').trimResults().omitEmptyStrings().splitToList(signals))

    // Send a ready notification for each signal in the set with updated variables
    println("Ready for signals "+ signalsSet)
    signalsSet.each{ signal ->
        if(signal.equals("Terminate_Job")) {
            signalapi.readyForSignal(signal);
        } else {
            signalapi.readyForSignal(signal, signalVariables)
        }
    }

    // Add the signals set as a variable to be used by next tasks
    variables.put("SIGNALS_SET", signalsSet)
}
variables.put("PA_TASK_ITERATION_SIGNAL", variables.get("PA_TASK_ITERATION_SIGNAL")+1)

//Read the variable SIGNALS_SET
Set signalsSet = variables.get("SIGNALS_SET")

// Check whether one signal among those specified as input is received
println("Checking whether one signal in the set "+ signalsSet +" is received")
receivedSignals = signalapi.checkForSignals(signalsSet)

// If a signal is received, remove ready signals and break the loop
if (receivedSignals != null && !receivedSignals.isEmpty()) {
    // remove ready signals
    signalapi.removeManySignals(new HashSet<>(signalsSet.collect { signal -> "ready_"+signal }))
    
    // Print the received signal
    println("Received signals: " + receivedSignals.toString())
    terminate = false
    for (String s : Arrays.asList(receivedSignals.keySet())) {
    	if (containsIgnoreCase(s, "Terminate_Job")) {
            result = "Terminate_Job"
            terminate = true
        }
        if (containsIgnoreCase(s, "Update_Inference_Parameters") && !terminate) {
            updatedVariables = receivedSignals.get("Update_Inference_Parameters")
            synchronizationapi.put(variables.get("PA_JOB_ID"), "MODEL_NAME", updatedVariables.get("MODEL_NAME"))
            synchronizationapi.put(variables.get("PA_JOB_ID"), "IMAGE_PATH", updatedVariables.get("IMAGE_PATH"))
            result = "Update_Inference_Parameters"
        }
    }
} else {
    result = "continue"
}

println("Wait_For_Signal.result: " + result)
synchronizationapi.put(variables.get("PA_JOB_ID"), "RECEIVED_SIGNAL", result)
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow>
        <loop target="Wait_for_Signal">
          <script>
            <code language="groovy">
              <![CDATA[
public static boolean containsIgnoreCase(String str, String searchStr) {
    if(str == null || searchStr == null) return false;
    final int length = searchStr.length();
    if (length == 0)
        return true;
    for (int i = str.length() - length; i >= 0; i--) {
        if (str.regionMatches(true, i, searchStr, 0, length))
            return true;
    }
    return false;
}

println("Internal_Loop.result: " + result)

if (containsIgnoreCase(result, "Terminate_Job")){
    loop = false
} else {
    if (containsIgnoreCase(result, "Update_Inference_Parameters")){
        variables.remove("PA_TASK_ITERATION_SIGNAL")
    }
    loop = "* * * * *"
}

]]>
            </code>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
            236.63333129882812
        </positionTop>
        <positionLeft>
            129.4000244140625
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Start" runAsMe="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_loop"/>
      </genericInformation>
      <depends>
        <task ref="Init"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="javascript">
            <![CDATA[
print('Loop block start ' + variables.get('PA_TASK_ITERATION'))
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="start"/>
      <metadata>
        <positionTop>
            240.61666870117188
        </positionTop>
        <positionLeft>
            374.66668701171875
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Loop" runAsMe="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
      </genericInformation>
      <depends>
        <task ref="MLOps_Model_Server_Inference_Test"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="javascript">
            <![CDATA[
// variables.remove("PA_TASK_ITERATION_SIGNAL")
print('Loop block end ' + variables.get('PA_TASK_ITERATION'))
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="end">
        <loop target="Start">
          <script>
            <code language="groovy">
              <![CDATA[
public static boolean containsIgnoreCase(String str, String searchStr) {
    if(str == null || searchStr == null) return false;
    final int length = searchStr.length();
    if (length == 0)
        return true;
    for (int i = str.length() - length; i >= 0; i--) {
        if (str.regionMatches(true, i, searchStr, 0, length))
            return true;
    }
    return false;
}

signal = synchronizationapi.get(variables.get("PA_JOB_ID"), "RECEIVED_SIGNAL")
println("External_Loop.signal: " + signal)

if (containsIgnoreCase(signal, "Update_Inference_Parameters")){
    MODEL_NAME = synchronizationapi.get(variables.get("PA_JOB_ID"), "MODEL_NAME")
    IMAGE_PATH = synchronizationapi.get(variables.get("PA_JOB_ID"), "IMAGE_PATH")
    variables.put("MODEL_NAME", MODEL_NAME)
	variables.put("IMAGE_PATH", IMAGE_PATH)
    println("External_Loop - Updating inference parameters requested")
    println("External_Loop.MODEL_NAME: " + MODEL_NAME)
    println("External_Loop.IMAGE_PATH: " + IMAGE_PATH)
}

if (containsIgnoreCase(signal, "Terminate_Job")){
    loop = false
} else {
    frequence = variables.get('INFERENCE_FREQUENCY') as int
    sleep(frequence)
    loop = true
}
]]>
            </code>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
            604.4333190917969
        </positionTop>
        <positionLeft>
            403.4000244140625
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="Init">
      <description>
        <![CDATA[ The simplest task, ran by a Groovy engine. ]]>
      </description>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
jobid = variables.get("PA_JOB_ID")
synchronizationapi.createChannel(jobid, false)
synchronizationapi.put(jobid, "RECEIVED_SIGNAL", "")
println "Channel " + jobid + " created."
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            127.44999694824219
        </positionTop>
        <positionLeft>
            276.58331298828125
        </positionLeft>
      </metadata>
    </task>
    <task fork="true" name="End">
      <description>
        <![CDATA[ The simplest task, ran by a Groovy engine. ]]>
      </description>
      <depends>
        <task ref="Loop"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
jobid = variables.get("PA_JOB_ID")
synchronizationapi.deleteChannel(jobid )
println "Channel " + jobid + " deleted."
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            706.6499938964844
        </positionTop>
        <positionLeft>
            414.5
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2638px;
            height:3488px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-122.44999694824219px;left:-124.4000244140625px"><div class="task ui-draggable _jsPlumb_endpoint_anchor_" style="top: 427.273px; left: 261.683px;" id="jsPlumb_1_46"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Simple task for MLOps Model Server inference test."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">MLOps_Model_Server_Inference_Test</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_ active-task" style="top: 236.633px; left: 129.4px; z-index: 24;" id="jsPlumb_1_49"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A template task that sends a ready notification for all the signals specified in the variable SIGNALS, then loops until one signal among those specified is received by the job."><img src="/automation-dashboard/styles/patterns/img/wf-icons/signal-wait.png" width="20px">&nbsp;<span class="name">Wait_for_Signal</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><div class="task block-start ui-draggable _jsPlumb_endpoint_anchor_" style="top: 240.618px; left: 374.666px;" id="jsPlumb_1_52"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Start</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><div class="task block-end ui-draggable _jsPlumb_endpoint_anchor_" style="top: 604.433px; left: 403.4px; z-index: 24;" id="jsPlumb_1_55"><a class="task-name" data-toggle="tooltip" data-placement="right" title=""><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Loop</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" style="top: 127.45px; left: 276.583px; z-index: 24;" id="jsPlumb_1_58"><a class="task-name" data-toggle="tooltip" data-placement="right" title="The simplest task, ran by a Groovy engine."><img src="/studio/images/Groovy.png" width="20px">&nbsp;<span class="name">Init</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" style="top: 706.648px; left: 414.496px;" id="jsPlumb_1_61"><a class="task-name" data-toggle="tooltip" data-placement="right" title="The simplest task, ran by a Groovy engine."><img src="/studio/images/Groovy.png" width="20px">&nbsp;<span class="name">End</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i id="called-icon"></i></a></div><svg style="position:absolute;left:357.5px;top:280.5px" width="78" height="147" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 146 C -10 96 67 50 57 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M5.105574749999999,111.30887700000001 L20.265679614732484,96.50444453906742 L11.328280720943656,98.76782718751691 L7.724629802249392,90.28173856812377 L5.105574749999999,111.30887700000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M5.105574749999999,111.30887700000001 L20.265679614732484,96.50444453906742 L11.328280720943656,98.76782718751691 L7.724629802249392,90.28173856812377 L5.105574749999999,111.30887700000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:174.4000244140625px;top:166.9499969482422px" width="162.68328857421875" height="70.18333435058594" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 69.18333435058594 C -10 19.183334350585938 151.68328857421875 50 141.68328857421875 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M18.516276385635376,44.6093954431572 L39.6923893429915,45.365860919761495 L31.88697848979336,40.459175540324914 L35.542169440159206,31.99515881560351 L18.516276385635376,44.6093954431572" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M18.516276385635376,44.6093954431572 L39.6923893429915,45.365860919761495 L31.88697848979336,40.459175540324914 L35.542169440159206,31.99515881560351 L18.516276385635376,44.6093954431572" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:229.9000244140625px;top:226.13333129882812px" width="61" height="61" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 40 C 50 -10 -10 50 0 0 " transform="translate(10.5,10.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#316b31" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M13.55903999999999,26.37184000000001 L31.950657715430147,15.848133532719789 L22.73126656242089,15.79496617605654 L21.37378389148668,6.675906970298892 L13.55903999999999,26.37184000000001" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,10.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M13.55903999999999,26.37184000000001 L31.950657715430147,15.848133532719789 L22.73126656242089,15.79496617605654 L21.37378389148668,6.675906970298892 L13.55903999999999,26.37184000000001" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,10.5)"></path></svg><div style="position: absolute; transform: translate(-50%, -50%); left: 254.9px; top: 256.133px;" class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_75">loop</div><svg style="position:absolute;left:316.08331298828125px;top:166.9499969482422px" width="119.41668701171875" height="74.55000305175781" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 98.41668701171875 73.55000305175781 C 108.41668701171875 23.550003051757812 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M88.58889532279969,49.66921984529496 L74.03680174993019,34.266733921053685 L76.152303293893,43.24028690844341 L67.60786881307865,46.70332594996037 L88.58889532279969,49.66921984529496" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M88.58889532279969,49.66921984529496 L74.03680174993019,34.266733921053685 L76.152303293893,43.24028690844341 L67.60786881307865,46.70332594996037 L88.58889532279969,49.66921984529496" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:357.5px;top:466.5px" width="106.4000244140625" height="138.43331909179688" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 85.4000244140625 137.43331909179688 C 95.4000244140625 87.43331909179688 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M75.8494800703125,103.25739795898438 L68.99120579632003,83.20835623668022 L67.19664513600583,92.2515609803788 L57.98536881771445,91.86119117098688 L75.8494800703125,103.25739795898438" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M75.8494800703125,103.25739795898438 L68.99120579632003,83.20835623668022 L67.19664513600583,92.2515609803788 L57.98536881771445,91.86119117098688 L75.8494800703125,103.25739795898438" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:474.5px;top:230.5px" width="51" height="424.4333190917969" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 0 0 C 50 -50 18.4000244140625 373.4333190917969 28.4000244140625 323.4333190917969 " transform="translate(0.5,50.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#316b31" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M25.29917952346802,46.468317152976994 L34.10597791614773,65.74110372032875 L26.58463713226501,60.409177913202164 L20.16511715592256,67.02656132912573 L25.29917952346802,46.468317152976994" class="" stroke="#316b31" fill="#316b31" transform="translate(0.5,50.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M25.29917952346802,46.468317152976994 L34.10597791614773,65.74110372032875 L26.58463713226501,60.409177913202164 L20.16511715592256,67.02656132912573 L25.29917952346802,46.468317152976994" class="" stroke="#316b31" fill="#316b31" transform="translate(0.5,50.5)"></path></svg><div style="position: absolute; transform: translate(-50%, -50%); left: 503.7px; top: 442.217px;" class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_87">loop</div><svg style="position:absolute;left:442.9000244140625px;top:643.9333190917969px" width="32.5999755859375" height="63.566680908203125" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector"><path d="M 11.5999755859375 62.566680908203125 C 21.5999755859375 12.566680908203125 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M13.582197687499999,46.22792601562501 L16.497757103617776,25.239845748695235 L10.819446619245788,32.50323270266314 L2.7730637906559075,28.002596816949445 L13.582197687499999,46.22792601562501" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M13.582197687499999,46.22792601562501 L16.497757103617776,25.239845748695235 L10.819446619245788,32.50323270266314 L2.7730637906559075,28.002596816949445 L13.582197687499999,46.22792601562501" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div style="position: absolute; height: 20px; width: 20px; left: 358px; top: 457px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 358px; top: 417px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 174.9px; top: 266.633px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 174.9px; top: 226.633px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 230.4px; top: 226.633px;" class="_jsPlumb_endpoint source-endpoint loop-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 230.4px; top: 266.633px;" class="_jsPlumb_endpoint target-endpoint loop-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 415px; top: 271px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 415px; top: 231px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 465px; top: 271px;" class="_jsPlumb_endpoint target-endpoint loop-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 443.4px; top: 634.433px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 443.4px; top: 594.433px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 493.4px; top: 594.433px;" class="_jsPlumb_endpoint source-endpoint loop-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 316.583px; top: 157.45px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 455px; top: 737px;" class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div style="position: absolute; height: 20px; width: 20px; left: 455px; top: 697px;" class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
