<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="MLOps_Model_Server_Application_Inference" projectName="3. MLOps Model Server Workflows" tags="MLOps,Model Inference,Triton" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="GRPC_INFERENCE_URL" value=""  description="GRPC inference url of the model server (e.g. localhost:8001)." group="Model Server" advanced="true" hidden="false"/>
    <variable name="MODEL_NAME" value="simple" model="PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)" description="Name of the model to be tested." group="Model Server" advanced="false" hidden="false"/>
    <variable name="IMAGE_PATH_HANDLER" value="" model="PA:SPEL(variables[&#x27;MODEL_NAME&#x27;].toLowerCase() == &#x27;densenet_onnx&#x27; || variables[&#x27;MODEL_NAME&#x27;].toLowerCase() == &#x27;inception_graphdef&#x27; ? showVar(&#x27;IMAGE_PATH&#x27;) : hideVar(&#x27;IMAGE_PATH&#x27;))" description="Handler for the image path" group="Model Server" advanced="false" hidden="true"/>
    <variable name="IMAGE_PATH" value="https://activeeon-public.s3.eu-west-2.amazonaws.com/images/bee.jpg" model="PA:URL" description="Handler for image path" group="Model Server" advanced="false" hidden="false"/>
    <variable name="CONTAINER_PLATFORM" value="docker" model="PA:LIST(no-container,docker,podman,singularity)" description="Container platform used for executing the workflow tasks." group="Container Parameters" advanced="true" hidden="false"/>
    <variable name="CONTAINER_IMAGE" value="nvcr.io/nvidia/tritonserver:22.10-py3-sdk" model="PA:LIST(,nvcr.io/nvidia/tritonserver:22.10-py3-sdk)" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" advanced="true" hidden="false"/>
    <variable name="INFERENCE_FREQUENCY" value="30000" model="PA:Integer" description="The frequency (in ms) to send a new inference request for the selected model." group="Model Server" advanced="false" hidden="false"/>
    <variable name="SIGNAL_FREQUENCY_CHECK" value="60000" model="PA:Integer" description="Time in milliseconds to check if there are any signal updates." group="Model Server" advanced="true" hidden="false"/>
    <variable name="MODEL_SERVER_ID" value=""  description="ID of the model server where the model that need to be consumed is deployed" group="Model Server" advanced="false" hidden="false"/>
    <variable name="USE_GRPC" value="False" model="PA:Boolean" description="True, if you want to use a specific GRPC_INFERENCE_URL to send inferences instead of the MODEL_SERVER_ID" group="Model Server" advanced="true" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ Inference workflow that periodically sends inferences consuming a specific deployed model. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="ai-mlops-dashboard"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
    <info name="Documentation" value="PAIO/PAIOUserGuide.html"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Model_Inference_Periodic" 
    
    
    
    preciousResult="true" 
    fork="true">
      <description>
        <![CDATA[ Inference workflow that periodically sends inferences consuming a specific deployed model. ]]>
      </description>
      <variables>
        <variable name="SIGNALS" value="Terminate_Job, Update_Frequency_Parameter, Update_Inference_Parameters" inherited="false" model="PA:REGEXP(((\w|-|_)+,?\s?)+)" description="List of comma-separated signals expected by this task."  advanced="false" hidden="false"/>
        <variable name="TASK_FILE_PATH" value="main.py" inherited="false"    advanced="false" hidden="false"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
        <info name="PRE_SCRIPT_AS_FILE" value="$TASK_FILE_PATH"/>
      </genericInformation>
      <forkEnvironment >
        <envScript>
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw" language="groovy"></file>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[
import os
import sys
import argparse
import uuid
import queue
import gevent.ssl
import numpy as np

from PIL import Image
from PIL.Image import Resampling

import tritonclient.http as httpclient
import tritonclient.grpc as grpcclient
import grpc

import tritonclient.grpc.model_config_pb2 as mc

from tritonclient.grpc import service_pb2, service_pb2_grpc
from tritonclient.utils import InferenceServerException
from tritonclient.utils import triton_to_np_dtype

from tritonclient import utils


def test_model_simple(args, triton_client):
    # Infer
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "INT32"))
    inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "INT32"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    input0_data = np.arange(start=0, stop=16, dtype=np.int32)
    input0_data = np.expand_dims(input0_data, axis=0)
    input1_data = np.ones(shape=(1, 16), dtype=np.int32)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Test with outputs
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    # print("response:\n", results.get_response())
    statistics = triton_client.get_inference_statistics(model_name=args.model_name)
    # print("statistics:\n", statistics)
    if len(statistics.model_stats) != 1:
        print("FAILED: Inference Statistics")
        sys.exit(1)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    print("model-outputs:")
    for i in range(16):
        print(str(input0_data[0][i]) + " + " + str(input1_data[0][i]) + " = " + str(output0_data[0][i]))
        print(str(input0_data[0][i]) + " - " + str(input1_data[0][i]) + " = " + str(output1_data[0][i]))
        if (input0_data[0][i] + input1_data[0][i]) != output0_data[0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[0][i] - input1_data[0][i]) != output1_data[0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple')


def test_model_simple_identity(args, triton_client):
    # Example using BYTES input tensor with utf-8 encoded string that
    # has an embedded null character.
    # null_chars_array = np.array(["he\x00llo".encode('utf-8') for i in range(16)], dtype=np.object_)
    # null_char_data = null_chars_array.reshape([1, 16])
    null_chars_array = np.array(["he\x00llo".encode('utf-8') for i in range(1)], dtype=np.object_)
    null_char_data = null_chars_array.reshape([1, 1])
    # Example using BYTES input tensor with 16 elements, where each
    # element is a 4-byte binary blob with value 0x00010203. Can use dtype=np.bytes_ in this case.
    # bytes_data = [b'\x00\x01\x02\x03' for i in range(16)]
    # np_bytes_data = np.array(bytes_data, dtype=np.bytes_)
    # np_bytes_data = np_bytes_data.reshape([1, 16])
    np_array = null_char_data
    # np_array = np_bytes_data
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', np_array.shape, "BYTES"))
    inputs[0].set_data_from_numpy(np_array)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    if (np_array.dtype == np.object_):
        print(results.as_numpy('OUTPUT0'))
        if not np.array_equal(np_array, results.as_numpy('OUTPUT0')):
            print(results.as_numpy('OUTPUT0'))
            print("error: incorrect output")
            sys.exit(1)
    else:
        encoded_results = np.char.encode(results.as_numpy('OUTPUT0').astype(str))
        print(encoded_results)
        if not np.array_equal(np_array, encoded_results):
            print("error: incorrect output")
            sys.exit(1)
    print('PASS: simple_identity')


def test_model_simple_int8(args, triton_client):
    # We use a simple model that takes 2 input tensors of 16 integers
    # each and returns 2 output tensors of 16 integers each. One
    # output tensor is the element-wise sum of the inputs and one
    # output is the element-wise difference.
    # model_name = "simple_int8"
    model_version = ""
    batch_size = 1
    # Create gRPC stub for communicating with the server
    channel = grpc.insecure_channel(args.model_server)
    grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)
    # Generate the request
    request = service_pb2.ModelInferRequest()
    request.model_name = args.model_name
    request.model_version = model_version
    # Input data
    input0_data = [i for i in range(16)]
    input1_data = [1 for i in range(16)]
    # Populate the inputs in inference request
    input0 = service_pb2.ModelInferRequest().InferInputTensor()
    input0.name = "INPUT0"
    input0.datatype = "INT8"
    input0.shape.extend([1, 16])
    input0.contents.int_contents[:] = input0_data
    input1 = service_pb2.ModelInferRequest().InferInputTensor()
    input1.name = "INPUT1"
    input1.datatype = "INT8"
    input1.shape.extend([1, 16])
    input1.contents.int_contents[:] = input1_data
    request.inputs.extend([input0, input1])
    # Populate the outputs in the inference request
    output0 = service_pb2.ModelInferRequest().InferRequestedOutputTensor()
    output0.name = "OUTPUT0"
    output1 = service_pb2.ModelInferRequest().InferRequestedOutputTensor()
    output1.name = "OUTPUT1"
    request.outputs.extend([output0, output1])
    response = grpc_stub.ModelInfer(request)
    output_results = []
    index = 0
    for output in response.outputs:
        shape = []
        for value in output.shape:
            shape.append(value)
        output_results.append(
            np.frombuffer(response.raw_output_contents[index], dtype=np.int8))
        output_results[-1] = np.resize(output_results[-1], shape)
        index += 1
    if len(output_results) != 2:
        print("expected two output results")
        sys.exit(1)
    for i in range(16):
        print(str(input0_data[i]) + " + " + str(input1_data[i]) + " = " + str(output_results[0][0][i]))
        print(str(input0_data[i]) + " - " + str(input1_data[i]) + " = " + str(output_results[1][0][i]))
        if (input0_data[i] + input1_data[i]) != output_results[0][0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[i] - input1_data[i]) != output_results[1][0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple_int8')


def test_model_simple_sequence(args, triton_client):
    # UserData class
    class UserData:
        def __init__(self):
            self._completed_requests = queue.Queue()
    # sync_send function
    def sync_send(triton_client, result_list, values, batch_size, sequence_id, model_name, model_version):
        count = 1
        for value in values:
            # Create the tensor for INPUT
            value_data = np.full(shape=[batch_size, 1], fill_value=value, dtype=np.int32)
            inputs = []
            inputs.append(grpcclient.InferInput('INPUT', value_data.shape, "INT32"))
            # Initialize the data
            inputs[0].set_data_from_numpy(value_data)
            outputs = []
            outputs.append(grpcclient.InferRequestedOutput('OUTPUT'))
            # Issue the synchronous sequence inference.
            result = triton_client.infer(model_name=model_name,
                                        inputs=inputs,
                                        outputs=outputs,
                                        sequence_id=sequence_id,
                                        sequence_start=(count == 1),
                                        sequence_end=(count == len(values)))
            result_list.append(result.as_numpy('OUTPUT'))
            count = count + 1
    # We use custom "sequence" models which take 1 input
    # value. The output is the accumulated value of the inputs. See
    # src/custom/sequence.
    offset = 0
    dyna = False
    int_sequence_model_name = "simple_dyna_sequence" if dyna else "simple_sequence"
    string_sequence_model_name = "simple_string_dyna_sequence" if dyna else "simple_sequence"
    model_version = ""
    batch_size = 1
    # values = [11, 7, 5, 3, 2, 0, 1]
    values = [11] # 6 inferences
    # Will use two sequences and send them synchronously. Note the
    # sequence IDs should be non-zero because zero is reserved for
    # non-sequence requests.
    int_sequence_id0 = 1000 + offset * 2
    int_sequence_id1 = 1001 + offset * 2
    # For string sequence IDs, the dyna backend requires that the
    # sequence id be decodable into an integer, otherwise we'll use
    # a UUID4 sequence id and a model that doesn't require corrid
    # control.
    string_sequence_id0 = str(1002) if dyna else str(uuid.uuid4())
    int_result0_list = []
    int_result1_list = []
    string_result0_list = []
    user_data = UserData()
    try:
        sync_send(triton_client, int_result0_list, 
                  values, batch_size,
                  int_sequence_id0, int_sequence_model_name, model_version)
        # sync_send(triton_client, int_result0_list, 
        #           [0] + values, batch_size,
        #           int_sequence_id0, int_sequence_model_name, model_version)
        # sync_send(triton_client, int_result1_list,
        #           [100] + [-1 * val for val in values], batch_size,
        #           int_sequence_id1, int_sequence_model_name, model_version)
        # sync_send(triton_client, string_result0_list,
        #           [20] + [-1 * val for val in values], batch_size,
        #           string_sequence_id0, string_sequence_model_name,
        #           model_version)
    except InferenceServerException as error:
        print(error)
        sys.exit(1)
    for i in range(len(int_result0_list)):
        int_seq0_expected = 1 if (i == 0) else values[i - 1]
        int_seq1_expected = 101 if (i == 0) else values[i - 1] * -1
        # For string sequence ID we are testing two different backends
        if i == 0 and dyna:
            string_seq0_expected = 20
        elif i == 0 and not dyna:
            string_seq0_expected = 21
        elif i != 0 and dyna:
            string_seq0_expected = values[i - 1] * -1 + int(
                string_result0_list[i - 1][0][0])
        else:
            string_seq0_expected = values[i - 1] * -1
        # The dyna_sequence custom backend adds the correlation ID
        # to the last request in a sequence.
        if dyna and (i != 0) and (values[i - 1] == 1):
            int_seq0_expected += int_sequence_id0
            # int_seq1_expected += int_sequence_id1
            # string_seq0_expected += int(string_sequence_id0)
        # print("[" + str(i) + "] " + str(int_result0_list[i][0][0]) + " : " +
        #       str(int_result1_list[i][0][0]) + " : " +
        #       str(string_result0_list[i][0][0]))
        print("[" + str(i) + "] " + str(int_result0_list[i][0][0]))
        # if ((int_seq0_expected != int_result0_list[i][0][0]) or
        #     (int_seq1_expected != int_result1_list[i][0][0]) or
        #     (string_seq0_expected != string_result0_list[i][0][0])):
        # if ((int_seq0_expected != int_result0_list[i][0][0])):
        #     print("[ expected ] " + str(int_seq0_expected) + " : " +
        #           str(int_seq1_expected) + " : " + str(string_seq0_expected))
        #     sys.exit(1)
    print("PASS: simple_sequence")


def test_model_simple_string(args, triton_client):
    inputs = []
    outputs = []
    inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "BYTES"))
    inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "BYTES"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    in0 = np.arange(start=0, stop=16, dtype=np.int32)
    in0 = np.expand_dims(in0, axis=0)
    in1 = np.ones(shape=(1, 16), dtype=np.int32)
    expected_sum = np.add(in0, in1)
    expected_diff = np.subtract(in0, in1)
    # The 'simple_string' model expects 2 BYTES tensors where each
    # element in those tensors is the utf-8 string representation of
    # an integer. The BYTES tensors must be represented by a numpy
    # array with dtype=np.object_.
    in0n = np.array([str(x).encode('utf-8') for x in in0.reshape(in0.size)], dtype=np.object_)
    input0_data = in0n.reshape(in0.shape)
    in1n = np.array([str(x).encode('utf-8') for x in in1.reshape(in1.size)], dtype=np.object_)
    input1_data = in1n.reshape(in1.shape)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Make inference
    results = triton_client.infer(model_name=args.model_name, inputs=inputs, outputs=outputs)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    for i in range(16):
        print(str(input0_data[0][i]) + " + " + str(input1_data[0][i]) + " = " + str(output0_data[0][i]))
        print(str(input0_data[0][i]) + " - " + str(input1_data[0][i]) + " = " + str(output1_data[0][i]))
        # Convert result from string to int to check result
        r0 = int(output0_data[0][i])
        r1 = int(output1_data[0][i])
        if expected_sum[0][i] != r0:
            print("error: incorrect sum")
            sys.exit(1)
        if expected_diff[0][i] != r1:
            print("error: incorrect difference")
            sys.exit(1)
    print("PASS: simple_string")


def test_model_image_classification(args, triton_client):
    def parse_model(model_metadata, model_config):
        """
        Check the configuration of a model to make sure it meets the
        requirements for an image classification network (as expected by
        this client)
        """
        if len(model_metadata.inputs) != 1:
            raise Exception("expecting 1 input, got {}".format(
                len(model_metadata.inputs)))
        if len(model_metadata.outputs) != 1:
            raise Exception("expecting 1 output, got {}".format(
                len(model_metadata.outputs)))
        if len(model_config.input) != 1:
            raise Exception(
                "expecting 1 input in model configuration, got {}".format(
                    len(model_config.input)))
        input_metadata = model_metadata.inputs[0]
        input_config = model_config.input[0]
        output_metadata = model_metadata.outputs[0]
        if output_metadata.datatype != "FP32":
            raise Exception("expecting output datatype to be FP32, model '" +
                            model_metadata.name + "' output type is " +
                            output_metadata.datatype)
        # Output is expected to be a vector. But allow any number of
        # dimensions as long as all but 1 is size 1 (e.g. { 10 }, { 1, 10
        # }, { 10, 1, 1 } are all ok). Ignore the batch dimension if there
        # is one.
        output_batch_dim = (model_config.max_batch_size > 0)
        non_one_cnt = 0
        for dim in output_metadata.shape:
            if output_batch_dim:
                output_batch_dim = False
            elif dim > 1:
                non_one_cnt += 1
                if non_one_cnt > 1:
                    raise Exception("expecting model output to be a vector")
        # Model input must have 3 dims, either CHW or HWC (not counting
        # the batch dimension), either CHW or HWC
        input_batch_dim = (model_config.max_batch_size > 0)
        expected_input_dims = 3 + (1 if input_batch_dim else 0)
        if len(input_metadata.shape) != expected_input_dims:
            raise Exception(
                "expecting input to have {} dimensions, model '{}' input has {}".
                format(expected_input_dims, model_metadata.name,
                    len(input_metadata.shape)))
        if type(input_config.format) == str:
            FORMAT_ENUM_TO_INT = dict(mc.ModelInput.Format.items())
            input_config.format = FORMAT_ENUM_TO_INT[input_config.format]
        if ((input_config.format != mc.ModelInput.FORMAT_NCHW) and
            (input_config.format != mc.ModelInput.FORMAT_NHWC)):
            raise Exception("unexpected input format " +
                            mc.ModelInput.Format.Name(input_config.format) +
                            ", expecting " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NCHW) +
                            " or " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NHWC))
        if input_config.format == mc.ModelInput.FORMAT_NHWC:
            h = input_metadata.shape[1 if input_batch_dim else 0]
            w = input_metadata.shape[2 if input_batch_dim else 1]
            c = input_metadata.shape[3 if input_batch_dim else 2]
        else:
            c = input_metadata.shape[1 if input_batch_dim else 0]
            h = input_metadata.shape[2 if input_batch_dim else 1]
            w = input_metadata.shape[3 if input_batch_dim else 2]
        return (model_config.max_batch_size, input_metadata.name,
                output_metadata.name, c, h, w, input_config.format,
                input_metadata.datatype)
    def preprocess(img, format, dtype, c, h, w, scaling):
        """
        Pre-process an image to meet the size, type and format
        requirements specified by the parameters.
        """
        # np.set_printoptions(threshold='nan')
        if c == 1:
            sample_img = img.convert('L')
        else:
            sample_img = img.convert('RGB')
        resized_img = sample_img.resize((w, h), Resampling.BILINEAR) # Image.BILINEAR will be removed in Pillow 10 (2023-07-01)
        resized = np.array(resized_img)
        if resized.ndim == 2:
            resized = resized[:, :, np.newaxis]
        npdtype = triton_to_np_dtype(dtype)
        typed = resized.astype(npdtype)
        if scaling == 'INCEPTION':
            scaled = (typed / 127.5) - 1
        elif scaling == 'VGG':
            if c == 1:
                scaled = typed - np.asarray((128,), dtype=npdtype)
            else:
                scaled = typed - np.asarray((123, 117, 104), dtype=npdtype)
        else:
            scaled = typed
        # Swap to CHW if necessary
        if format == mc.ModelInput.FORMAT_NCHW:
            ordered = np.transpose(scaled, (2, 0, 1))
        else:
            ordered = scaled
        # Channels are in RGB order. Currently model configuration data
        # doesn't provide any information as to other channel orderings
        # (like BGR) so we just assume RGB.
        return ordered
    class UserData:
        def __init__(self):
            self._completed_requests = queue.Queue()
    def requestGenerator(client, batched_image_data, input_name, output_name, dtype, args):
        # Set the input data
        inputs = [grpcclient.InferInput(input_name, batched_image_data.shape, dtype)]
        inputs[0].set_data_from_numpy(batched_image_data)
        outputs = [
            grpcclient.InferRequestedOutput(output_name, class_count=args.classes)
        ]
        yield inputs, outputs, args.model_name, args.model_version
    def postprocess(results, output_name, batch_size, supports_batching):
        """
        Post-process results to show classifications.
        """
        output_array = results.as_numpy(output_name)
        if supports_batching and len(output_array) != batch_size:
            raise Exception("expected {} results, got {}".format(batch_size, len(output_array)))
        # Include special handling for non-batching models
        for results in output_array:
            if not supports_batching:
                results = [results]
            for result in results:
                if output_array.dtype.type == np.object_:
                    cls = "".join(chr(x) for x in result).split(':')
                else:
                    cls = result.split(':')
                print("    {} ({}) = {}".format(cls[0], cls[1], cls[2]))
    # Make sure the model matches our requirements, and get some
    # properties of the model that we need for preprocessing
    try:
        model_metadata = triton_client.get_model_metadata(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the metadata: " + str(e))
        sys.exit(1)
    # print('-'*30, "\nmodel_metadata:\n", model_metadata, '-'*30)
    try:
        model_config = triton_client.get_model_config(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the config: " + str(e))
        sys.exit(1)
    # print('-'*30, "\nmodel_config:\n", model_config, '-'*30)
    model_config = model_config.config
    max_batch_size, input_name, output_name, c, h, w, format, dtype = parse_model(model_metadata, model_config)
    supports_batching = max_batch_size > 0
    if not supports_batching and args.batch_size != 1:
        print("ERROR: This model doesn't support batching.")
        sys.exit(1)
    if args.image_path is None:
        print("ERROR: This model requires an input image.")
        sys.exit(1)
    filenames = []
    if os.path.isdir(args.image_path):
        filenames = [
            os.path.join(args.image_path, f)
            for f in os.listdir(args.image_path)
            if os.path.isfile(os.path.join(args.image_path, f))
        ]
    else:
        filenames = [
            args.image_path,
        ]
    assert(len(filenames) > 0)
    filenames.sort()
    # print("filenames:\n", filenames)
    # Preprocess the images into input data according to model
    # requirements
    image_data = []
    for filename in filenames:
        img = Image.open(filename)
        image_data.append(preprocess(img, format, dtype, c, h, w, args.scaling))
    # Send requests of args.batch_size images. If the number of
    # images isn't an exact multiple of args.batch_size then just
    # start over with the first images until the batch is filled.
    requests = []
    responses = []
    result_filenames = []
    request_ids = []
    image_idx = 0
    last_request = False
    user_data = UserData()
    # Holds the handles to the ongoing HTTP async requests.
    async_requests = []
    sent_count = 0
    # if args.streaming:
    #     triton_client.start_stream(partial(completion_callback, user_data))
    while not last_request:
        input_filenames = []
        repeated_image_data = []
        for idx in range(args.batch_size):
            input_filenames.append(filenames[image_idx])
            repeated_image_data.append(image_data[image_idx])
            image_idx = (image_idx + 1) % len(image_data)
            if image_idx == 0:
                last_request = True
        if supports_batching:
            batched_image_data = np.stack(repeated_image_data, axis=0)
        else:
            batched_image_data = repeated_image_data[0]
        # Send request
        try:
            for inputs, outputs, model_name, model_version in requestGenerator(
                    triton_client, batched_image_data, input_name, output_name, dtype, args):
                sent_count += 1
                responses.append(
                        triton_client.infer(args.model_name,
                                            inputs,
                                            request_id=str(sent_count),
                                            model_version=args.model_version,
                                            outputs=outputs))
                # if args.streaming:
                #     triton_client.async_stream_infer(
                #         args.model_name,
                #         inputs,
                #         request_id=str(sent_count),
                #         model_version=args.model_version,
                #         outputs=outputs)
                # elif args.async_set:
                #     if args.protocol.lower() == "grpc":
                #         triton_client.async_infer(
                #             args.model_name,
                #             inputs,
                #             partial(completion_callback, user_data),
                #             request_id=str(sent_count),
                #             model_version=args.model_version,
                #             outputs=outputs)
                #     else:
                #         async_requests.append(
                #             triton_client.async_infer(
                #                 args.model_name,
                #                 inputs,
                #                 request_id=str(sent_count),
                #                 model_version=args.model_version,
                #                 outputs=outputs))
                # else:
                #     responses.append(
                #         triton_client.infer(args.model_name,
                #                             inputs,
                #                             request_id=str(sent_count),
                #                             model_version=args.model_version,
                #                             outputs=outputs))
        except InferenceServerException as e:
            print("inference failed: " + str(e))
            # if args.streaming:
            #     triton_client.stop_stream()
            sys.exit(1)
    # if args.streaming:
    #     triton_client.stop_stream()
    # if args.protocol.lower() == "grpc":
    #     if args.streaming or args.async_set:
    #         processed_count = 0
    #         while processed_count < sent_count:
    #             (results, error) = user_data._completed_requests.get()
    #             processed_count += 1
    #             if error is not None:
    #                 print("inference failed: " + str(error))
    #                 sys.exit(1)
    #             responses.append(results)
    # else:
    #     if args.async_set:
    #         # Collect results from the ongoing async requests
    #         # for HTTP Async requests.
    #         for async_request in async_requests:
    #             responses.append(async_request.get_result())
    for response in responses:
        this_id = response.get_response().id
        # if args.protocol.lower() == "grpc":
        #     this_id = response.get_response().id
        # else:
        #     this_id = response.get_response()["id"]
        print("Request {}, batch size {}".format(this_id, args.batch_size))
        postprocess(response, output_name, args.batch_size, supports_batching)
    print("PASS: " + args.model_name)


def main(args):
    print("model-server: ", args.model_server)
    print("model-name:   ", args.model_name)
    try:
        # triton_client = httpclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
        triton_client = grpcclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
    except Exception as e:
        print("channel creation failed: " + str(e))
        sys.exit(1)
    if args.model_name == "simple":
        test_model_simple(args, triton_client)
    elif args.model_name == "simple_identity":
        test_model_simple_identity(args, triton_client)
    elif args.model_name == "simple_int8":
        test_model_simple_int8(args, triton_client)
    elif args.model_name == "simple_sequence":
        test_model_simple_sequence(args, triton_client)
    elif args.model_name == "simple_string":
        test_model_simple_string(args, triton_client)
    elif args.model_name == "inception_graphdef" or args.model_name == "densenet_onnx":
        test_model_image_classification(args, triton_client)
    else:
        print("Invalid model name: " + args.model_name)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-s',
                        '--model-server',
                        type=str,
                        required=True,
                        help='Name of the model server.')
    parser.add_argument('-n',
                        '--model-name',
                        type=str,
                        required=True,
                        help='Name of the deployed model to be tested.')
    parser.add_argument('-x',
                        '--model-version',
                        type=str,
                        required=False,
                        default="",
                        help='Version of model. Default is to use latest version.')
    parser.add_argument('-v',
                        '--verbose',
                        action="store_true",
                        required=False,
                        default=False,
                        help='Enable verbose output.')
    parser.add_argument('-b',
                        '--batch-size',
                        type=int,
                        required=False,
                        default=1,
                        help='Batch size. Default is 1.')
    parser.add_argument('-l',
                        '--scaling',
                        type=str,
                        choices=['NONE', 'INCEPTION', 'VGG'],
                        required=False,
                        default='INCEPTION',
                        help='Type of scaling to apply to image pixels. Default is INCEPTION (only for image-based models).')
    parser.add_argument('-i',
                        '--image_path',
                        type=str,
                        nargs='?',
                        default=None,
                        help='Input image / Input folder (mandatory for image-based models).')
    parser.add_argument('-c',
                        '--classes',
                        type=int,
                        required=False,
                        default=3,
                        help='Number of class results to report (only for image-based models). Default is 3.')
    args = parser.parse_args()
    main(args)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import com.google.common.base.Splitter;
import org.ow2.proactive.scheduler.common.job.JobVariable;
import groovy.json.JsonSlurper

// -------------------------------------------------------------
// Extract the GRPC_INFERENCE_URL using the MODEL_SERVER_ID
String getGrpcEndpointUrl(String sessionid, String proactiveUrl, String instanceId) {
    // Define headers
    def headers = ['sessionid': sessionid]

    // Define base URL
    def baseUrl = "$proactiveUrl/cloud-automation-service/serviceInstances/$instanceId"

    // Create URL object
    def url = new URL(baseUrl)

    // Open connection
    def connection = url.openConnection()

    // Set headers
    headers.each { key, value ->
        connection.setRequestProperty(key, value)
    }

    // Get response
    def response = connection.getInputStream()

    // Parse JSON response
    def json = new JsonSlurper().parseText(response.text)

    // Fetch GRPC_ENDPOINT_ID
    def grpcEndpointId = json.deployments.endpoint.find { it.id.contains("-grpc") }?.id

    // Fetch GRPC_ENDPOINT_URL
    def grpcEndpointUrl = json.deployments.endpoint.find { it.id == grpcEndpointId }?.url

    return grpcEndpointUrl
}

// Usage example
schedulerapi.connect()
connectionInfo = schedulerapi.getConnectionInfo()
ciLogin = connectionInfo.getLogin()
ciPasswd = connectionInfo.getPassword()
ciUrl = connectionInfo.getUrl()
def sessionId = schedulerapi.getSession()

user_credentials = [
  sessionId: sessionId,
  ciLogin: ciLogin,
  ciPasswd: ciPasswd,
  ciUrl: ciUrl
]
// def sessionId = 'your_session_id'
def url = new URL(ciUrl)
def proactiveUrl = url.getProtocol() + "://" + url.getHost() + ":" + url.getPort()
def instanceId = variables.get("MODEL_SERVER_ID")
def USE_GRPC = variables.get("USE_GRPC")
println "USE_GRPC: $USE_GRPC"

String GRPC_INFERENCE_URL = ""  // Declare and initialize the variable outside the conditional blocks

if (USE_GRPC == "false") {
    GRPC_INFERENCE_URL = getGrpcEndpointUrl(sessionId, proactiveUrl, instanceId)
    println "Using the defined variable MODEL_SERVER_ID to get the GRPC_INFERENCE_URL associated with it."
} else {
    GRPC_INFERENCE_URL = variables.get("GRPC_INFERENCE_URL")
    println "Using the defined variable GRPC_INFERENCE_URL to send inferences."
}

println("GRPC_INFERENCE_URL: " + GRPC_INFERENCE_URL)

// -------------------------------------------------------------
// Define the containsIgnoreCase function
public static boolean containsIgnoreCase(String str, String searchStr) {
    if(str == null || searchStr == null) return false;
    final int length = searchStr.length();
    if (length == 0)
        return true;
    for (int i = str.length() - length; i >= 0; i--) {
        if (str.regionMatches(true, i, searchStr, 0, length))
            return true;
    }
    return false;
}

// Get workflow variables
int INFERENCE_FREQUENCY = variables.get("INFERENCE_FREQUENCY") as Integer
int SIGNAL_FREQUENCY_CHECK = variables.get("SIGNAL_FREQUENCY_CHECK") as Integer
String MODEL_NAME = variables.get("MODEL_NAME")
String IMAGE_PATH = variables.get("IMAGE_PATH")
String TASK_FILE_PATH = variables.get("TASK_FILE_PATH")
println("INFERENCE_FREQUENCY: " + INFERENCE_FREQUENCY)
println("SIGNAL_FREQUENCY_CHECK : " + SIGNAL_FREQUENCY_CHECK)
println("INFERENCE_FREQUENCY: " + INFERENCE_FREQUENCY)
println("MODEL_NAME: " + MODEL_NAME)
println("IMAGE_PATH: " + IMAGE_PATH)
println("TASK_FILE_PATH: " + TASK_FILE_PATH)

// Create the inference script file
def create_inference_script(TASK_FILE_PATH, GRPC_INFERENCE_URL, MODEL_NAME, IMAGE_PATH) {
    def file = new File("inference.sh")
    try {
        if (file.exists()) {
            println "inference.sh file exists"
            file.delete()
            println "inference.sh file deleted successfully"
        }
        println "Creating a new inference.sh file"
        def bash_command = """
            #!/bin/bash
            if [ $MODEL_NAME = "inception_graphdef" ] || [ $MODEL_NAME = "densenet_onnx" ]; then
                if [ -z $IMAGE_PATH ]; then
                    echo "$IMAGE_PATH is empty!"
                    exit -1
                else
                    echo "Downloading $IMAGE_PATH as image.png"
                    wget -O image.png "$IMAGE_PATH"
                    echo "python $TASK_FILE_PATH -s $GRPC_INFERENCE_URL -n $MODEL_NAME -i image.png"
                    python $TASK_FILE_PATH -s $GRPC_INFERENCE_URL -n $MODEL_NAME -i image.png
                fi
            else
                echo "python $TASK_FILE_PATH -s $GRPC_INFERENCE_URL -n $MODEL_NAME"
                python $TASK_FILE_PATH -s $GRPC_INFERENCE_URL -n $MODEL_NAME
            fi
        """
        file << bash_command
        file.setExecutable(true)
    } catch (Exception e) {
        println("Failed to create inference.sh file: ${e.getMessage()}")
    }
}

// Create the inference script file using the initial workflow variables values
create_inference_script(TASK_FILE_PATH, GRPC_INFERENCE_URL, MODEL_NAME, IMAGE_PATH)
result = null
int timeWithoutSignalCheck = 0
int timeSpent = 0
int sleepTime = 0
int firstStart = 1
int signalCheckStartTime = System.currentTimeMillis()

// Function to copy a file from one location to another
def copyFile(String sourceFilePath, String destinationFilePath) {
    try {
        def sourceFile = new File(sourceFilePath)
        def destinationFile = new File(destinationFilePath)
        destinationFile.parentFile.mkdirs()
        destinationFile.createNewFile()
        destinationFile << sourceFile.text
        println("File copied successfully from ${sourceFilePath} to ${destinationFilePath}")
    } catch (Exception e) {
        println("Failed to copy file from ${sourceFilePath} to ${destinationFilePath}: ${e.getMessage()}")
    }
}

// Display current folder
def currentFolder = System.getProperty("user.dir")
println("Current folder: ${currentFolder}")

// Backup files
copyFile("${currentFolder}/main.py", "/home/main.py")
copyFile("${currentFolder}/inference.sh", "/home/inference.sh")

// Loop continuously sending inferences periodically according to the defined frequency value
while (true) {
    int startTime = System.currentTimeMillis()
    // Check if main.py file exists in the current folder, if not, copy it from /home/main.py
    if (!new File("${currentFolder}/main.py").exists()) {
        copyFile("/home/main.py", "${currentFolder}/main.py")
    }
    // Check if inference.sh file exists in the current folder, if not, copy it from /home/inference.sh
    if (!new File("${currentFolder}/inference.sh").exists()) {
        copyFile("/home/inference.sh", "${currentFolder}/inference.sh")
    }
    println("INFERENCE_FREQUENCY: " + INFERENCE_FREQUENCY)
    timeWithoutSignalCheck = System.currentTimeMillis() - signalCheckStartTime
    println("timeWithoutSignalCheck: " + timeWithoutSignalCheck)
    if (firstStart == 1 || timeWithoutSignalCheck >= SIGNAL_FREQUENCY_CHECK) {
        signalCheckStartTime = System.currentTimeMillis()
        println("Checking for signals")
        if (variables.get("READY_SENT") == null || "Update_Frequency_Parameter".equalsIgnoreCase(result) || "Update_Inference_Parameters".equalsIgnoreCase(result)) {
            // Define the Update_Inference_Parameters signal
            List <JobVariable> signalVariables = new java.util.ArrayList<JobVariable>()
            signalVariables.add(new JobVariable("GRPC_INFERENCE_URL", GRPC_INFERENCE_URL, "PA:NOT_EMPTY_STRING", "GRPC inference url of the model server (e.g. localhost:8001).", "", false, false))
            signalVariables.add(new JobVariable("MODEL_NAME", MODEL_NAME, "PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)", "Name of the model to be tested.", "", false, false))
            signalVariables.add(new JobVariable("IMAGE_PATH", IMAGE_PATH, "PA:URL", "Path of the image to be used for inference.", "", false, false))
            signalVariables.add(new JobVariable("IMAGE_PATH_HANDLER", "", "PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'densenet_onnx' || variables['MODEL_NAME'].toLowerCase() == 'inception_graphdef' ? showVar('IMAGE_PATH') : hideVar('IMAGE_PATH'))", "Handler for IMAGE_PATH variable.", "", false, true))

            // Define the Update_Frequency_Parameter signal
            List <JobVariable> signalVariables_frequency = new java.util.ArrayList<JobVariable>()
            signalVariables_frequency.add(new JobVariable("INFERENCE_FREQUENCY", ""+INFERENCE_FREQUENCY, "PA:Integer", "The frequency (in ms) to send a new inference request for the selected model.", "", false, false))

            // Read the variable SIGNALS
            signals = variables.get("SIGNALS")

            // Split the value of the variable SIGNALS and transform it into a list
            Set signalsSet = new HashSet<>(Splitter.on(',').trimResults().omitEmptyStrings().splitToList(signals))

            // Send a ready notification for each signal in the set with updated variables
            println("Ready for signals "+ signalsSet)
            signalsSet.each{ signal ->
                if(signal.equals("Terminate_Job")) {
                    signalapi.readyForSignal(signal);
                } else if (signal.equals("Update_Frequency_Parameter")) {
                    signalapi.readyForSignal(signal, signalVariables_frequency)
                } else if (signal.equals("Update_Inference_Parameters")) {
                    signalapi.readyForSignal(signal, signalVariables)
                }
            }

            // Add a variable to avoid sending ready notifications again
            variables.put("READY_SENT", true)

            // Add the signals set as a variable to be used by next tasks
            variables.put("SIGNALS_SET", signalsSet)
        }

        //Read the variable SIGNALS_SET
        Set signalsSet =  variables.get("SIGNALS_SET")

        // Check whether one signal among those specified as input is received
        println("Checking whether one signal in the set "+ signalsSet +" is received")
        receivedSignals = signalapi.checkForSignals(signalsSet)

        // Check if a signal was received
        if (receivedSignals != null && !receivedSignals.isEmpty()) {
            // Print the received signal
            println("Received signals: " + receivedSignals.toString())
            for (String s : Arrays.asList(receivedSignals.keySet())) {
                if (containsIgnoreCase(s, "Terminate_Job")) {
                    result = "Terminate_Job"
                    break
                }
                if (containsIgnoreCase(s, "Update_Frequency_Parameter")) {
                    updatedVariables = receivedSignals.get("Update_Frequency_Parameter")
                    INFERENCE_FREQUENCY = updatedVariables.get("INFERENCE_FREQUENCY") as Integer
                    result = "Update_Frequency_Parameter"
                }
                if (containsIgnoreCase(s, "Update_Inference_Parameters")) {
                    updatedVariables = receivedSignals.get("Update_Inference_Parameters")
                    GRPC_INFERENCE_URL = updatedVariables.get("GRPC_INFERENCE_URL")
                    MODEL_NAME = updatedVariables.get("MODEL_NAME")
                    IMAGE_PATH = updatedVariables.get("IMAGE_PATH")
                    result = "Update_Inference_Parameters"
                    // Re-generate the inference script using the new variable values
                    create_inference_script(TASK_FILE_PATH, GRPC_INFERENCE_URL, MODEL_NAME, IMAGE_PATH)
                }
            }
        } else {
            result = null
        }
        println("result: " + result)
        if ("Terminate_Job".equalsIgnoreCase(result)){
            break
        }
        firstStart = 0
    }
    def command = "sh inference.sh"
    def output = command.execute().text
    println(output)
    int endTime = System.currentTimeMillis()
    timeSpent = endTime - startTime
    println("timeSpent: " + timeSpent)
    sleepTime = INFERENCE_FREQUENCY - timeSpent
    if (sleepTime < 0) {
        sleepTime = 0
    }
    println("sleepTime: " + sleepTime)
    sleep(sleepTime)
}
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <metadata>
        <positionTop>
            121.1632080078125
        </positionTop>
        <positionLeft>
            350.3993225097656
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2339px;
            height:2711px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-116.1632080078125px;left:-345.3993225097656px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable active-task" id="jsPlumb_1_80" style="top: 121.164px; left: 350.399px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Inference workflow that periodically sends inferences consuming a specific deployed model."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">Model_Inference_Periodic</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i title="Workflows being Called by this Task" id="called-icon"></i></a><a title="Scripts being Called by this Task" id="reference-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: -7px; right: 3px;"><i id="reference-icon" class="glyphicon glyphicon-list-alt"></i></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 418px; top: 151px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>