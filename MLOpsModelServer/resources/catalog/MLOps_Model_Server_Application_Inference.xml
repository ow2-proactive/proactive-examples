<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="MLOps_Model_Server_Application_Inference" onTaskError="continueJobExecution" priority="normal" projectName="3. MLOps Model Server Workflows" tags="MLOps,Model Inference,Triton" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="false" description="Protocol to use for inference (http or grpc)." group="Model Server" hidden="false" model="PA:LIST(http,grpc)" name="INFERENCE_PROTOCOL" value="http"/>
    <variable advanced="false" description="Name of the model to be tested." group="Model Server" hidden="false" model="PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef,iris-classification-model)" name="MODEL_NAME" value="simple"/>
    <variable advanced="false" description="Handler for the image path" group="Model Server" hidden="true" model="PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'densenet_onnx' || variables['MODEL_NAME'].toLowerCase() == 'inception_graphdef' ? showVar('IMAGE_PATH') : hideVar('IMAGE_PATH'))" name="IMAGE_PATH_HANDLER" value=""/>
    <variable advanced="false" description="Handler for image path" group="Model Server" hidden="false" model="PA:URL" name="IMAGE_PATH" value="https://activeeon-public.s3.eu-west-2.amazonaws.com/images/bee.jpg"/>
    <variable advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(,nvcr.io/nvidia/tritonserver:22.10-py3-sdk)" name="CONTAINER_IMAGE" value="nvcr.io/nvidia/tritonserver:22.10-py3-sdk"/>
    <variable advanced="false" description="The frequency (in ms) to send a new inference request for the selected model." group="Model Server" hidden="false" model="PA:Integer" name="INFERENCE_FREQUENCY" value="30000"/>
    <variable advanced="true" description="Time in milliseconds to check if there are any signal updates." group="Model Server" hidden="false" model="PA:Integer" name="SIGNAL_FREQUENCY_CHECK" value="60000"/>
    <variable advanced="false" description="ID of the model server where the model that need to be consumed is deployed" group="Model Server" hidden="false" model="PA:Integer" name="MODEL_SERVER_ID" value="-1"/>
    <variable advanced="false" hidden="true" model="PA:BOOLEAN" name="DRIFT_DATA" value="false"/>
    <variable advanced="false" hidden="true" model="PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'iris-classification-model' ? showVar('DRIFT_DATA') : hideVar('DRIFT_DATA'))" name="DRIFT_DATA_HANDLER" value=""/>
    <variable advanced="false" description="Handler for USE_PROXY" group="Model Server" hidden="true" model="PA:SPEL(variables['INFERENCE_PROTOCOL'].toLowerCase() == 'http' ? showVar('USE_PROXY') : hideVar('USE_PROXY'))" name="USE_PROXY_HANDLER" value=""/>
    <variable advanced="false" description="Enable data drift detection (only for HTTP protocol)" group="Model Server" hidden="false" model="PA:BOOLEAN" name="USE_PROXY" value="true"/>
    <variable advanced="true" description="If True, it will activate the use of GPU on the selected container platform. Boolean (default=True)" group="Container Parameters" hidden="false" model="PA:BOOLEAN" name="CONTAINER_GPU_ENABLED" value="true"/>
  </variables>
  <description>
    <![CDATA[ Inference workflow that periodically sends inferences consuming a specific deployed model. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-mlops-dashboard"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
<info name="Documentation" value="PAIO/PAIOUserGuide.html"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="Model_Inference_Periodic" preciousResult="true">
      <description>
        <![CDATA[ Inference workflow that periodically sends inferences consuming a specific deployed model. ]]>
      </description>
      <variables>
        <variable advanced="false" description="List of comma-separated signals expected by this task." hidden="false" inherited="false" model="PA:REGEXP(((\w|-|_)+,?\s?)+)" name="SIGNALS" value="Terminate_Job, Update_Frequency_Parameter, Update_Inference_Parameters"/>
        <variable advanced="false" hidden="false" inherited="false" name="TASK_FILE_PATH" value="main.py"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
        <info name="PRE_SCRIPT_AS_FILE" value="$TASK_FILE_PATH"/>
      </genericInformation>
      <forkEnvironment>
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[
#!/usr/bin/env python3

"""
Triton Inference Server Client Test Script

This script provides a comprehensive test suite for various models deployed on the Triton Inference Server.
It supports testing different model types including iris classification, simple arithmetic, image classification,
and sequence models. The script can communicate with the server using either HTTP or gRPC protocols.

Key Features:
- Supports multiple model types: iris classification, simple arithmetic, image classification, etc.
- Handles both HTTP and gRPC communication methods
- Provides data drift simulation for testing model robustness
- Supports batch processing for applicable models
- Includes preprocessing and postprocessing functions for image-based models

Usage:
    python3 triton_client.py -s <server_url> -n <model_name> [options]

For detailed usage instructions, run:
    python3 triton_client.py --help

Required Arguments:
    -s, --model-server : URL of the Triton Inference Server
    -n, --model-name   : Name of the deployed model to be tested

Optional Arguments:
    -x, --model-version : Version of the model (default: latest)
    -b, --batch-size    : Batch size for inference (default: 1)
    -l, --scaling       : Type of scaling for image pixels (NONE, INCEPTION, VGG; default: INCEPTION)
    -i, --image-path    : Path to input image or folder (required for image-based models)
    -c, --classes       : Number of class results to report for image classification (default: 3)
    -m, --method        : Communication method (http, grpc; default: http)
    -d, --drift-data    : Flag to use drift data for testing (default: False)
    -v, --verbose       : Enable verbose output

Dependencies:
    - numpy
    - Pillow
    - tritonclient
    - sklearn
    - gevent

Note: This script requires the Triton Inference Server to be running and accessible at the specified URL.
"""

import os
import sys
import argparse
import numpy as np
from PIL import Image
from PIL.Image import Resampling

import tritonclient.http as httpclient
import tritonclient.grpc as grpcclient
import tritonclient.grpc.model_config_pb2 as mc

from tritonclient.utils import InferenceServerException
from tritonclient.utils import triton_to_np_dtype

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


def test_iris_classification_model(args, triton_client):
    """
    Test the iris classification model deployed on the Triton Inference Server.

    This function loads the iris dataset, optionally applies data drift,
    prepares input tensors, performs inference, and evaluates the model's accuracy.

    Args:
        args (argparse.Namespace): Command-line arguments.
        triton_client (tritonclient.http.InferenceServerClient or tritonclient.grpc.InferenceServerClient):
            Client object for communicating with the Triton server.

    Raises:
        InferenceServerException: If inference fails.
        SystemExit: If there's an error in processing.

    Example:
        test_iris_classification_model(args, triton_client)
    """
    # Load and prepare data
    iris = load_iris()
    X, y = iris.data, iris.target
    _, X_test, _, y_test = train_test_split(X, y, test_size=0.1, random_state=None)
    input_data = X_test.astype(np.float32)
    print("Input data:\n", input_data)
    # Simulate data drift if specified
    drift_data = args.drift_data and args.drift_data.lower() == "true"
    if drift_data:
        print("Drift Data")
        noise_std = 10.0
        noise = np.random.normal(0, noise_std, input_data.shape)
        input_data += noise
        print("Drifted input data:\n", input_data)
    # Prepare input tensor
    if args.method == 'http':
        inputs = [httpclient.InferInput('float_input', input_data.shape, "FP32")]
    else:  # grpc
        inputs = [grpcclient.InferInput('float_input', input_data.shape, "FP32")]
    inputs[0].set_data_from_numpy(input_data)
    print("input0_data:\n", inputs[0])
    # Prepare output tensors
    if args.method == 'http':
        outputs = [
            httpclient.InferRequestedOutput('label'),
            httpclient.InferRequestedOutput('probabilities')
        ]
    else:  # grpc
        outputs = [
            grpcclient.InferRequestedOutput('label'),
            grpcclient.InferRequestedOutput('probabilities')
        ]
    # Perform inference
    try:
        results = triton_client.infer(model_name=args.model_name, model_version=args.model_version, inputs=inputs, outputs=outputs)
    except InferenceServerException as e:
        print(f"Inference failed: {str(e)}")
        sys.exit(1)
    # Get and print output data
    output0_data = results.as_numpy('label')
    output1_data = results.as_numpy('probabilities')
    print("output0_data (label):\n", output0_data)
    print("output1_data (probabilities):\n", output1_data)
    # Calculate and print accuracy
    accuracy = accuracy_score(y_test, output0_data)
    print(f"Model accuracy: {accuracy * 100:.2f}%")
    print('PASS: iris-classification-model')


def test_model_simple(args, triton_client):
    """
    Test a simple arithmetic model that adds and subtracts integers.

    This function prepares input tensors with integer data, sends an inference request
    to the server, and verifies the output for correct addition and subtraction operations.

    Args:
        args (argparse.Namespace): Command-line arguments.
        triton_client (tritonclient.http.InferenceServerClient or tritonclient.grpc.InferenceServerClient):
            Client object for communicating with the Triton server.

    Raises:
        InferenceServerException: If inference fails.
        SystemExit: If there's an error in processing or verification.

    Example:
        test_model_simple(args, triton_client)
    """
    # Infer
    inputs = []
    outputs = []
    # Prepare input tensors
    if args.method == 'http':
        inputs.append(httpclient.InferInput('INPUT0', [1, 16], "INT32"))
        inputs.append(httpclient.InferInput('INPUT1', [1, 16], "INT32"))
    else:  # grpc
        inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "INT32"))
        inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "INT32"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    input0_data = np.arange(start=0, stop=16, dtype=np.int32)
    input0_data = np.expand_dims(input0_data, axis=0)
    input1_data = np.ones(shape=(1, 16), dtype=np.int32)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    # Prepare output tensors
    if args.method == 'http':
        outputs.append(httpclient.InferRequestedOutput('OUTPUT0'))
        outputs.append(httpclient.InferRequestedOutput('OUTPUT1'))
    else:  # grpc
        outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
        outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Test with outputs
    try:
        results = triton_client.infer(model_name=args.model_name, model_version=args.model_version, inputs=inputs, outputs=outputs)
    except InferenceServerException as e:
        print(f"Inference failed: {str(e)}")
        sys.exit(1)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    print("model-outputs:")
    for i in range(16):
        print(f"{input0_data[0][i]} + {input1_data[0][i]} = {output0_data[0][i]}")
        print(f"{input0_data[0][i]} - {input1_data[0][i]} = {output1_data[0][i]}")
        if (input0_data[0][i] + input1_data[0][i]) != output0_data[0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[0][i] - input1_data[0][i]) != output1_data[0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple')


def test_model_simple_identity(args, triton_client):
    # Example using BYTES input tensor with utf-8 encoded string that
    # has an embedded null character.
    null_chars_array = np.array(["he\x00llo".encode('utf-8') for i in range(1)], dtype=np.object_)
    null_char_data = null_chars_array.reshape([1, 1])
    # Example using BYTES input tensor with 16 elements, where each
    # element is a 4-byte binary blob with value 0x00010203. Can use dtype=np.bytes_ in this case.
    # bytes_data = [b'\x00\x01\x02\x03' for i in range(16)]
    # np_bytes_data = np.array(bytes_data, dtype=np.bytes_)
    # np_bytes_data = np_bytes_data.reshape([1, 16])
    np_array = null_char_data
    # np_array = np_bytes_data
    inputs = []
    outputs = []
    # Prepare input tensor
    if args.method == 'http':
        inputs.append(httpclient.InferInput('INPUT0', np_array.shape, "BYTES"))
    else:  # grpc
        inputs.append(grpcclient.InferInput('INPUT0', np_array.shape, "BYTES"))
    inputs[0].set_data_from_numpy(np_array)
    # Prepare output tensor
    if args.method == 'http':
        outputs.append(httpclient.InferRequestedOutput('OUTPUT0'))
    else:  # grpc
        outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
    # Perform inference
    try:
        results = triton_client.infer(model_name=args.model_name, model_version=args.model_version, inputs=inputs, outputs=outputs)
    except InferenceServerException as e:
        print(f"Inference failed: {str(e)}")
        sys.exit(1)
    # Process results
    output_data = results.as_numpy('OUTPUT0')
    print("Output data:", output_data)
    if np_array.dtype == np.object_:
        if not np.array_equal(np_array, output_data):
            print("Error: incorrect output")
            print("Expected:", np_array)
            print("Got:", output_data)
            sys.exit(1)
    else:
        encoded_results = np.char.encode(output_data.astype(str))
        if not np.array_equal(np_array, encoded_results):
            print("Error: incorrect output")
            print("Expected:", np_array)
            print("Got:", encoded_results)
            sys.exit(1)
    print('PASS: simple_identity')


def test_model_simple_int8(args, triton_client):
    batch_size = 1
    # Input data
    input0_data = np.array([i for i in range(16)], dtype=np.int8)
    input1_data = np.array([1 for _ in range(16)], dtype=np.int8)
    # Reshape data to match expected input shape
    input0_data = np.expand_dims(input0_data, axis=0)
    input1_data = np.expand_dims(input1_data, axis=0)
    # Initialize the inputs
    inputs = []
    if args.method == 'http':
        inputs.append(httpclient.InferInput('INPUT0', input0_data.shape, "INT8"))
        inputs.append(httpclient.InferInput('INPUT1', input1_data.shape, "INT8"))
    else:  # grpc
        inputs.append(grpcclient.InferInput('INPUT0', input0_data.shape, "INT8"))
        inputs.append(grpcclient.InferInput('INPUT1', input1_data.shape, "INT8"))
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    # Initialize the outputs
    outputs = []
    if args.method == 'http':
        outputs.append(httpclient.InferRequestedOutput('OUTPUT0'))
        outputs.append(httpclient.InferRequestedOutput('OUTPUT1'))
    else:  # grpc
        outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
        outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Perform inference
    try:
        results = triton_client.infer(
            model_name=args.model_name,
            model_version=args.model_version,
            inputs=inputs,
            outputs=outputs,
            client_timeout=None)
    except InferenceServerException as e:
        print(f"Inference failed: {str(e)}")
        sys.exit(1)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    # Verify results
    for i in range(16):
        print(f"{input0_data[0][i]} + {input1_data[0][i]} = {output0_data[0][i]}")
        print(f"{input0_data[0][i]} - {input1_data[0][i]} = {output1_data[0][i]}")
        if (input0_data[0][i] + input1_data[0][i]) != output0_data[0][i]:
            print("sync infer error: incorrect sum")
            sys.exit(1)
        if (input0_data[0][i] - input1_data[0][i]) != output1_data[0][i]:
            print("sync infer error: incorrect difference")
            sys.exit(1)
    print('PASS: simple_int8')


def test_model_simple_sequence(args, triton_client):
    batch_size = 1
    dtype = "INT32"
    # Prepare sequence input
    values = [11]  # Single value for simplicity, you can extend this as needed
    sequence_id = 1000
    result_list = []
    for value in values:
        # Create the tensor for INPUT
        value_data = np.full(shape=[batch_size, 1], fill_value=value, dtype=np.int32)
        # Initialize the input
        if args.method == 'http':
            inputs = [httpclient.InferInput('INPUT', value_data.shape, dtype)]
        else:  # grpc
            inputs = [grpcclient.InferInput('INPUT', value_data.shape, dtype)]
        
        inputs[0].set_data_from_numpy(value_data)
        # Initialize the output
        if args.method == 'http':
            outputs = [httpclient.InferRequestedOutput('OUTPUT')]
        else:  # grpc
            outputs = [grpcclient.InferRequestedOutput('OUTPUT')]
        # Issue the synchronous sequence inference.
        try:
            result = triton_client.infer(
                model_name=args.model_name,
                model_version=args.model_version,
                inputs=inputs,
                outputs=outputs,
                sequence_id=sequence_id,
                sequence_start=(value == values[0]),
                sequence_end=(value == values[-1])
            )
            result_list.append(result.as_numpy('OUTPUT'))
        except InferenceServerException as error:
            print(f"Inference failed: {error}")
            sys.exit(1)
    # Process and verify results
    for i, result in enumerate(result_list):
        print(f"[{i}] {result[0][0]}")
        # You can add more specific result verification here if needed
        # For example:
        # expected = ... # Calculate expected value
        # if result[0][0] != expected:
        #     print(f"Error: Expected {expected}, got {result[0][0]}")
        #     sys.exit(1)
    print("PASS: simple_sequence")


def test_model_simple_string(args, triton_client):
    inputs = []
    outputs = []
    # Prepare input tensors
    if args.method == 'http':
        inputs.append(httpclient.InferInput('INPUT0', [1, 16], "BYTES"))
        inputs.append(httpclient.InferInput('INPUT1', [1, 16], "BYTES"))
    else:  # grpc
        inputs.append(grpcclient.InferInput('INPUT0', [1, 16], "BYTES"))
        inputs.append(grpcclient.InferInput('INPUT1', [1, 16], "BYTES"))
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    in0 = np.arange(start=0, stop=16, dtype=np.int32)
    in0 = np.expand_dims(in0, axis=0)
    in1 = np.ones(shape=(1, 16), dtype=np.int32)
    expected_sum = np.add(in0, in1)
    expected_diff = np.subtract(in0, in1)
    # The 'simple_string' model expects 2 BYTES tensors where each
    # element in those tensors is the utf-8 string representation of
    # an integer. The BYTES tensors must be represented by a numpy
    # array with dtype=np.object_.
    in0n = np.array([str(x).encode('utf-8') for x in in0.reshape(in0.size)], dtype=np.object_)
    input0_data = in0n.reshape(in0.shape)
    in1n = np.array([str(x).encode('utf-8') for x in in1.reshape(in1.size)], dtype=np.object_)
    input1_data = in1n.reshape(in1.shape)
    # Initialize the data
    inputs[0].set_data_from_numpy(input0_data)
    inputs[1].set_data_from_numpy(input1_data)
    # Prepare output tensors
    if args.method == 'http':
        outputs.append(httpclient.InferRequestedOutput('OUTPUT0'))
        outputs.append(httpclient.InferRequestedOutput('OUTPUT1'))
    else:  # grpc
        outputs.append(grpcclient.InferRequestedOutput('OUTPUT0'))
        outputs.append(grpcclient.InferRequestedOutput('OUTPUT1'))
    # Perform inference
    try:
        results = triton_client.infer(
            model_name=args.model_name,
            model_version=args.model_version,
            inputs=inputs,
            outputs=outputs)
    except InferenceServerException as e:
        print(f"Inference failed: {str(e)}")
        sys.exit(1)
    # Get the output arrays from the results
    output0_data = results.as_numpy('OUTPUT0')
    output1_data = results.as_numpy('OUTPUT1')
    for i in range(16):
        print(f"{input0_data[0][i].decode()} + {input1_data[0][i].decode()} = {output0_data[0][i].decode()}")
        print(f"{input0_data[0][i].decode()} - {input1_data[0][i].decode()} = {output1_data[0][i].decode()}")
        # Convert result from string to int to check result
        r0 = int(output0_data[0][i])
        r1 = int(output1_data[0][i])
        if expected_sum[0][i] != r0:
            print(f"Error: incorrect sum at index {i}. Expected {expected_sum[0][i]}, got {r0}")
            sys.exit(1)
        if expected_diff[0][i] != r1:
            print(f"Error: incorrect difference at index {i}. Expected {expected_diff[0][i]}, got {r1}")
            sys.exit(1)
    print("PASS: simple_string")


def test_model_image_classification(args, triton_client):
    def parse_model(model_metadata, model_config, is_http):
        """
        Check the configuration of a model to make sure it meets the
        requirements for an image classification network (as expected by
        this client)
        """
        if is_http:
            inputs = model_metadata['inputs']
            outputs = model_metadata['outputs']
            input_config = model_config['input'][0]
            output_metadata = outputs[0]
            max_batch_size = model_config['max_batch_size']
        else:
            inputs = model_metadata.inputs
            outputs = model_metadata.outputs
            input_config = model_config.config.input[0]
            output_metadata = outputs[0]
            max_batch_size = model_config.config.max_batch_size
        if len(inputs) != 1:
            raise Exception(f"expecting 1 input, got {len(inputs)}")
        if len(outputs) != 1:
            raise Exception(f"expecting 1 output, got {len(outputs)}")
        if is_http:
            if len(model_config['input']) != 1:
                raise Exception(f"expecting 1 input in model configuration, got {len(model_config['input'])}")
        else:
            if len(model_config.config.input) != 1:
                raise Exception(f"expecting 1 input in model configuration, got {len(model_config.config.input)}")
        input_metadata = inputs[0]
        if is_http:
            if output_metadata['datatype'] != "FP32":
                raise Exception(f"expecting output datatype to be FP32, model '{model_metadata['name']}' output type is {output_metadata['datatype']}")
        else:
            if output_metadata.datatype != "FP32":
                raise Exception(f"expecting output datatype to be FP32, model '{model_metadata.name}' output type is {output_metadata.datatype}")
        # Output is expected to be a vector. But allow any number of
        # dimensions as long as all but 1 is size 1 (e.g. { 10 }, { 1, 10
        # }, { 10, 1, 1 } are all ok). Ignore the batch dimension if there
        # is one.
        output_batch_dim = (max_batch_size > 0)
        non_one_cnt = 0
        output_shape = output_metadata['shape'] if is_http else output_metadata.shape
        for dim in output_shape:
            if output_batch_dim:
                output_batch_dim = False
            elif dim > 1:
                non_one_cnt += 1
                if non_one_cnt > 1:
                    raise Exception("expecting model output to be a vector")
        # Model input must have 3 dims, either CHW or HWC (not counting
        # the batch dimension), either CHW or HWC
        input_batch_dim = (max_batch_size > 0)
        expected_input_dims = 3 + (1 if input_batch_dim else 0)
        input_shape = input_metadata['shape'] if is_http else input_metadata.shape
        if len(input_shape) != expected_input_dims:
            raise Exception(f"expecting input to have {expected_input_dims} dimensions, model '{model_metadata['name'] if is_http else model_metadata.name}' input has {len(input_shape)}")
        input_format = input_config['format'] if is_http else input_config.format
        if isinstance(input_format, str):
            FORMAT_ENUM_TO_INT = dict(mc.ModelInput.Format.items())
            input_format = FORMAT_ENUM_TO_INT[input_format]
        if ((input_format != mc.ModelInput.FORMAT_NCHW) and
            (input_format != mc.ModelInput.FORMAT_NHWC)):
            raise Exception("unexpected input format " +
                            mc.ModelInput.Format.Name(input_format) +
                            ", expecting " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NCHW) +
                            " or " +
                            mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NHWC))
        if input_format == mc.ModelInput.FORMAT_NHWC:
            h = input_shape[1 if input_batch_dim else 0]
            w = input_shape[2 if input_batch_dim else 1]
            c = input_shape[3 if input_batch_dim else 2]
        else:
            c = input_shape[1 if input_batch_dim else 0]
            h = input_shape[2 if input_batch_dim else 1]
            w = input_shape[3 if input_batch_dim else 2]
        return (max_batch_size, 
                input_metadata['name'] if is_http else input_metadata.name,
                output_metadata['name'] if is_http else output_metadata.name, 
                c, h, w, input_format,
                input_metadata['datatype'] if is_http else input_metadata.datatype)
    def preprocess(img, format, dtype, c, h, w, scaling):
        """
        Pre-process an image to meet the size, type and format
        requirements specified by the parameters.
        """
        # np.set_printoptions(threshold='nan')
        if c == 1:
            sample_img = img.convert('L')
        else:
            sample_img = img.convert('RGB')
        resized_img = sample_img.resize((w, h), Resampling.BILINEAR) # Image.BILINEAR will be removed in Pillow 10 (2023-07-01)
        resized = np.array(resized_img)
        if resized.ndim == 2:
            resized = resized[:, :, np.newaxis]
        npdtype = triton_to_np_dtype(dtype)
        typed = resized.astype(npdtype)
        if scaling == 'INCEPTION':
            scaled = (typed / 127.5) - 1
        elif scaling == 'VGG':
            if c == 1:
                scaled = typed - np.asarray((128,), dtype=npdtype)
            else:
                scaled = typed - np.asarray((123, 117, 104), dtype=npdtype)
        else:
            scaled = typed
        # Swap to CHW if necessary
        if format == mc.ModelInput.FORMAT_NCHW:
            ordered = np.transpose(scaled, (2, 0, 1))
        else:
            ordered = scaled
        # Channels are in RGB order. Currently model configuration data
        # doesn't provide any information as to other channel orderings
        # (like BGR) so we just assume RGB.
        return ordered
    def requestGenerator(client, batched_image_data, input_name, output_name, dtype, args):
        # Set the input data
        if args.method == 'http':
            inputs = [httpclient.InferInput(input_name, batched_image_data.shape, dtype)]
        else:  # grpc
            inputs = [grpcclient.InferInput(input_name, batched_image_data.shape, dtype)]
        inputs[0].set_data_from_numpy(batched_image_data)
        if args.method == 'http':
            outputs = [httpclient.InferRequestedOutput(output_name, class_count=args.classes)]
        else:  # grpc
            outputs = [grpcclient.InferRequestedOutput(output_name, class_count=args.classes)]
        yield inputs, outputs, args.model_name, args.model_version
    def postprocess(results, output_name, batch_size, supports_batching):
        """
        Post-process results to show classifications.
        """
        output_array = results.as_numpy(output_name)
        if supports_batching and len(output_array) != batch_size:
            raise Exception("expected {} results, got {}".format(batch_size, len(output_array)))
        # Include special handling for non-batching models
        for results in output_array:
            if not supports_batching:
                results = [results]
            for result in results:
                if output_array.dtype.type == np.object_:
                    cls = "".join(chr(x) for x in result).split(':')
                else:
                    cls = result.split(':')
                print("    {} ({}) = {}".format(cls[0], cls[1], cls[2]))
    # Make sure the model matches our requirements, and get some
    # properties of the model that we need for preprocessing
    try:
        model_metadata = triton_client.get_model_metadata(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the metadata: " + str(e))
        sys.exit(1)
    try:
        model_config = triton_client.get_model_config(model_name=args.model_name, model_version=args.model_version)
    except InferenceServerException as e:
        print("failed to retrieve the config: " + str(e))
        sys.exit(1)
    is_http = args.method == 'http'
    max_batch_size, input_name, output_name, c, h, w, format, dtype = parse_model(model_metadata, model_config, is_http)
    supports_batching = max_batch_size > 0
    if not supports_batching and args.batch_size != 1:
        print("ERROR: This model doesn't support batching.")
        sys.exit(1)
    if args.image_path is None:
        print("ERROR: This model requires an input image.")
        sys.exit(1)
    filenames = []
    if os.path.isdir(args.image_path):
        filenames = [
            os.path.join(args.image_path, f)
            for f in os.listdir(args.image_path)
            if os.path.isfile(os.path.join(args.image_path, f))
        ]
    else:
        filenames = [args.image_path]
    assert(len(filenames) > 0)
    filenames.sort()
    # Preprocess the images into input data according to model requirements
    image_data = []
    for filename in filenames:
        img = Image.open(filename)
        image_data.append(preprocess(img, format, dtype, c, h, w, args.scaling))
    # Send requests of args.batch_size images. If the number of
    # images isn't an exact multiple of args.batch_size then just
    # start over with the first images until the batch is filled.
    responses = []
    image_idx = 0
    last_request = False
    sent_count = 0
    while not last_request:
        input_filenames = []
        repeated_image_data = []
        for idx in range(args.batch_size):
            input_filenames.append(filenames[image_idx])
            repeated_image_data.append(image_data[image_idx])
            image_idx = (image_idx + 1) % len(image_data)
            if image_idx == 0:
                last_request = True
        if supports_batching:
            batched_image_data = np.stack(repeated_image_data, axis=0)
        else:
            batched_image_data = repeated_image_data[0]
        # Send request
        try:
            for inputs, outputs, model_name, model_version in requestGenerator(
                    triton_client, batched_image_data, input_name, output_name, dtype, args):
                sent_count += 1
                responses.append(
                    triton_client.infer(model_name,
                                        inputs,
                                        request_id=str(sent_count),
                                        model_version=model_version,
                                        outputs=outputs))
        except InferenceServerException as e:
            print("inference failed: " + str(e))
            sys.exit(1)
    for response in responses:
        if args.method == 'http':
            this_id = response.get_response()["id"]
        else:  # grpc
            this_id = response.get_response().id
        print("Request {}, batch size {}".format(this_id, args.batch_size))
        postprocess(response, output_name, args.batch_size, supports_batching)
    print("PASS: " + args.model_name)


def main(args):
    """
    Main function that orchestrates the testing of different model types.

    This function creates a Triton client based on the specified method (HTTP or gRPC)
    and calls the appropriate test function based on the model name.

    Args:
        args (argparse.Namespace): Command-line arguments.

    Raises:
        SystemExit: If there's an error in client creation or an invalid model name is provided.

    Example:
        main(args)
    """
    print("model-server: ", args.model_server)
    print("model-name:   ", args.model_name)
    print("drift-data:   ", args.drift_data)
    print("method:       ", args.method)
    try:
        if args.method == 'http':
            triton_client = httpclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
        else:  # grpc
            triton_client = grpcclient.InferenceServerClient(url=args.model_server, verbose=args.verbose)
    except Exception as e:
        print("channel creation failed: " + str(e))
        sys.exit(1)
    if "iris-classification-model" in args.model_name:
        test_iris_classification_model(args, triton_client)
    elif args.model_name == "simple":
        test_model_simple(args, triton_client)
    elif args.model_name == "simple_identity":
        test_model_simple_identity(args, triton_client)
    elif args.model_name == "simple_int8":
        test_model_simple_int8(args, triton_client)
    elif args.model_name == "simple_sequence":
        test_model_simple_sequence(args, triton_client)
    elif args.model_name == "simple_string":
        test_model_simple_string(args, triton_client)
    elif args.model_name == "inception_graphdef" or args.model_name == "densenet_onnx":
        test_model_image_classification(args, triton_client)
    else:
        print("Invalid model name: " + args.model_name)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-s',
                        '--model-server',
                        type=str,
                        required=True,
                        help='Name of the model server.')
    parser.add_argument('-n',
                        '--model-name',
                        type=str,
                        required=True,
                        help='Name of the deployed model to be tested.')
    parser.add_argument('-x',
                        '--model-version',
                        type=str,
                        required=False,
                        default="",
                        help='Version of model. Default is to use latest version.')
    parser.add_argument('-b',
                        '--batch-size',
                        type=int,
                        required=False,
                        default=1,
                        help='Batch size. Default is 1.')
    parser.add_argument('-l',
                        '--scaling',
                        type=str,
                        choices=['NONE', 'INCEPTION', 'VGG'],
                        required=False,
                        default='INCEPTION',
                        help='Type of scaling to apply to image pixels. Default is INCEPTION (only for image-based models).')
    parser.add_argument('-i',
                        '--image-path',
                        type=str,
                        nargs='?',
                        default=None,
                        help='Input image / Input folder (mandatory for image-based models).')
    parser.add_argument('-c',
                        '--classes',
                        type=int,
                        required=False,
                        default=3,
                        help='Number of class results to report (only for image-based models). Default is 3.')
    parser.add_argument('-m',
                        '--method',
                        type=str,
                        choices=['http', 'grpc'],
                        default='http',
                        help='Communication method with server: "http" or "grpc". Default is "http".')
    parser.add_argument('-d',
                        '--drift-data',
                        type=str,
                        default="False",
                        help='Flag to indicate whether drift data should be used.')
    parser.add_argument('-v',
                        '--verbose',
                        action="store_true",
                        required=False,
                        default=False,
                        help='Enable verbose output.')
    args = parser.parse_args()
    main(args)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import com.google.common.base.Splitter;
import org.ow2.proactive.scheduler.common.job.JobVariable;
import groovy.json.JsonSlurper

// -------------------------------------------------------------
// Extract the INFERENCE_URL using the MODEL_SERVER_ID
String getInferenceEndpointUrl(String sessionid, String proactiveUrl, Integer modelServerId, String protocol, boolean use_proxy) {
    // Define headers
    def headers = ['sessionid': sessionid]
    // Define base URL
    def baseUrl = "$proactiveUrl/cloud-automation-service/serviceInstances/$modelServerId"
    // Create URL object
    def url = new URL(baseUrl)
    // Open connection
    def connection = url.openConnection()
    // Set headers
    headers.each { key, value ->
        connection.setRequestProperty(key, value)
    }
    // Get response
    def response = connection.getInputStream()
    // Parse JSON response
    def json = new JsonSlurper().parseText(response.text)
    // Fetch ENDPOINT_ID based on protocol
    def endpointId
    if (protocol == "http" && use_proxy) {
        endpointId = json.deployments.endpoint.find { it.id.contains("-proxy") }?.id
    } else {
        endpointId = json.deployments.endpoint.find { it.id.contains("-${protocol}") }?.id
    }
    // Fetch ENDPOINT_URL
    def endpointUrl = json.deployments.endpoint.find { it.id == endpointId }?.url
    return endpointUrl
}

Integer MODEL_SERVER_ID = variables.get("MODEL_SERVER_ID") as Integer
String INFERENCE_PROTOCOL = variables.get("INFERENCE_PROTOCOL")
boolean USE_PROXY = variables.get("USE_PROXY") as Boolean
println("MODEL_SERVER_ID:    " + MODEL_SERVER_ID)
println("INFERENCE_PROTOCOL: " + INFERENCE_PROTOCOL)
println("USE_PROXY:          " + USE_PROXY)

def getInferenceUrl(Integer modelServerId, String inferenceProtocol, boolean useProxy) {
    schedulerapi.connect()
    connectionInfo = schedulerapi.getConnectionInfo()
    def sessionId = schedulerapi.getSession()
    def url = new URL(connectionInfo.getUrl())
    def proactiveUrl = url.getProtocol() + "://" + url.getHost() + ":" + url.getPort()
    String inferenceUrl = ""
    if (modelServerId != -1) {
        inferenceUrl = getInferenceEndpointUrl(sessionId, proactiveUrl, modelServerId, inferenceProtocol, useProxy)
        println "Using the defined variable MODEL_SERVER_ID to get the ${inferenceProtocol.toUpperCase()}_INFERENCE_URL associated with it."
    } else {
        println "ERROR: No valid MODEL_SERVER_ID provided."
        System.exit(1)
    }
    return inferenceUrl
}
String INFERENCE_URL = getInferenceUrl(MODEL_SERVER_ID, INFERENCE_PROTOCOL, USE_PROXY)
println("INFERENCE_URL: " + INFERENCE_URL)

// -------------------------------------------------------------
// Define the containsIgnoreCase function
public static boolean containsIgnoreCase(String str, String searchStr) {
    if(str == null || searchStr == null) return false;
    final int length = searchStr.length();
    if (length == 0)
        return true;
    for (int i = str.length() - length; i >= 0; i--) {
        if (str.regionMatches(true, i, searchStr, 0, length))
            return true;
    }
    return false;
}

// Get workflow variables
int INFERENCE_FREQUENCY = variables.get("INFERENCE_FREQUENCY") as Integer
int SIGNAL_FREQUENCY_CHECK = variables.get("SIGNAL_FREQUENCY_CHECK") as Integer
String MODEL_NAME = variables.get("MODEL_NAME")
String IMAGE_PATH = variables.get("IMAGE_PATH")
String TASK_FILE_PATH = variables.get("TASK_FILE_PATH")
String DRIFT_DATA = variables.get("DRIFT_DATA")

println("INFERENCE_FREQUENCY:    " + INFERENCE_FREQUENCY)
println("SIGNAL_FREQUENCY_CHECK: " + SIGNAL_FREQUENCY_CHECK)
println("INFERENCE_FREQUENCY:    " + INFERENCE_FREQUENCY)
println("MODEL_NAME:             " + MODEL_NAME)
println("IMAGE_PATH:             " + IMAGE_PATH)
println("TASK_FILE_PATH:         " + TASK_FILE_PATH)
println("DRIFT_DATA:             " + DRIFT_DATA)

// Create the inference script file
def create_inference_script(TASK_FILE_PATH, INFERENCE_URL, MODEL_NAME, IMAGE_PATH, DRIFT_DATA, INFERENCE_PROTOCOL) {
    def file = new File("inference.sh")
    try {
        if (file.exists()) {
            println "inference.sh file exists"
            file.delete()
            println "inference.sh file deleted successfully"
        }
        println "Creating a new inference.sh file"
        def bash_command = """
            #!/bin/bash
            python -m pip install scikit-learn
            if [ $MODEL_NAME = "inception_graphdef" ] || [ $MODEL_NAME = "densenet_onnx" ]; then
                if [ -z $IMAGE_PATH ]; then
                    echo "$IMAGE_PATH is empty!"
                    exit -1
                else
                    echo "Downloading $IMAGE_PATH as image.png"
                    wget -O image.png "$IMAGE_PATH"
                    echo "python $TASK_FILE_PATH -s $INFERENCE_URL -n $MODEL_NAME -i image.png -d $DRIFT_DATA -m $INFERENCE_PROTOCOL"
                    python $TASK_FILE_PATH -s $INFERENCE_URL -n $MODEL_NAME -i image.png -d $DRIFT_DATA -m $INFERENCE_PROTOCOL
                fi
            else
                echo "python $TASK_FILE_PATH -s $INFERENCE_URL -n $MODEL_NAME -d $DRIFT_DATA -m $INFERENCE_PROTOCOL"
                python $TASK_FILE_PATH -s $INFERENCE_URL -n $MODEL_NAME -d $DRIFT_DATA -m $INFERENCE_PROTOCOL
            fi
        """
        file << bash_command
        file.setExecutable(true)
    } catch (Exception e) {
        println("Failed to create inference.sh file: ${e.getMessage()}")
    }
}

// Create the inference script file using the initial workflow variables values
create_inference_script(TASK_FILE_PATH, INFERENCE_URL, MODEL_NAME, IMAGE_PATH, DRIFT_DATA, INFERENCE_PROTOCOL)
result = null
int timeWithoutSignalCheck = 0
int timeSpent = 0
int sleepTime = 0
int firstStart = 1
int signalCheckStartTime = System.currentTimeMillis()

// Function to copy a file from one location to another
def copyFile(String sourceFilePath, String destinationFilePath) {
    try {
        def sourceFile = new File(sourceFilePath)
        def destinationFile = new File(destinationFilePath)
        destinationFile.parentFile.mkdirs()
        destinationFile.createNewFile()
        destinationFile << sourceFile.text
        println("File copied successfully from ${sourceFilePath} to ${destinationFilePath}")
    } catch (Exception e) {
        println("Failed to copy file from ${sourceFilePath} to ${destinationFilePath}: ${e.getMessage()}")
    }
}

// Display current folder
def currentFolder = System.getProperty("user.dir")
println("Current folder: ${currentFolder}")

// Backup files
copyFile("${currentFolder}/main.py", "/home/main.py")
copyFile("${currentFolder}/inference.sh", "/home/inference.sh")

// Loop continuously sending inferences periodically according to the defined frequency value
while (true) {
    int startTime = System.currentTimeMillis()
    // Check if main.py file exists in the current folder, if not, copy it from /home/main.py
    if (!new File("${currentFolder}/main.py").exists()) {
        copyFile("/home/main.py", "${currentFolder}/main.py")
    }
    // Check if inference.sh file exists in the current folder, if not, copy it from /home/inference.sh
    if (!new File("${currentFolder}/inference.sh").exists()) {
        copyFile("/home/inference.sh", "${currentFolder}/inference.sh")
    }
    println("INFERENCE_FREQUENCY: " + INFERENCE_FREQUENCY)
    timeWithoutSignalCheck = System.currentTimeMillis() - signalCheckStartTime
    println("timeWithoutSignalCheck: " + timeWithoutSignalCheck)
    if (firstStart == 1 || timeWithoutSignalCheck >= SIGNAL_FREQUENCY_CHECK) {
        signalCheckStartTime = System.currentTimeMillis()
        println("Checking for signals")
        if (variables.get("READY_SENT") == null || "Update_Frequency_Parameter".equalsIgnoreCase(result) || "Update_Inference_Parameters".equalsIgnoreCase(result)) {
            // Define the Update_Inference_Parameters signal
            // JobVariable(String name, String value, String model, String description, String group, boolean advanced, boolean hidden)
            List <JobVariable> signalVariables = new java.util.ArrayList<JobVariable>()
            signalVariables.add(new JobVariable("MODEL_SERVER_ID", ""+MODEL_SERVER_ID, "PA:Integer", "ID of the model server where the model that need to be consumed is deployed", "", false, false))
            signalVariables.add(new JobVariable("MODEL_NAME", MODEL_NAME, "PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef,iris-classification-model)", "Name of the model to be tested.", "", false, false))
            signalVariables.add(new JobVariable("IMAGE_PATH", IMAGE_PATH, "PA:URL", "Path of the image to be used for inference.", "", false, false))
            signalVariables.add(new JobVariable("IMAGE_PATH_HANDLER", "", "PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'densenet_onnx' || variables['MODEL_NAME'].toLowerCase() == 'inception_graphdef' ? showVar('IMAGE_PATH') : hideVar('IMAGE_PATH'))", "Handler for IMAGE_PATH variable.", "", false, true))
            signalVariables.add(new JobVariable("DRIFT_DATA_HANDLER", "", "PA:SPEL(variables['MODEL_NAME'].toLowerCase() == 'iris-classification-model' ? showVar('DRIFT_DATA') : hideVar('DRIFT_DATA'))", "Handler for DRIFT_DATA variable.", "", false, true))
            signalVariables.add(new JobVariable("DRIFT_DATA", ""+DRIFT_DATA, "PA:Boolean", "If True, input data will be drifted", "", false, false))
            signalVariables.add(new JobVariable("INFERENCE_PROTOCOL", INFERENCE_PROTOCOL, "PA:LIST(http,grpc)", "Protocol to use for inference (http or grpc).", "", false, false))
            signalVariables.add(new JobVariable("USE_PROXY_HANDLER", "", "PA:SPEL(variables['INFERENCE_PROTOCOL'].toLowerCase() == 'http' ? showVar('USE_PROXY') : hideVar('USE_PROXY'))", "Handler for USE_PROXY variable.", "", false, true))
            signalVariables.add(new JobVariable("USE_PROXY", ""+USE_PROXY, "PA:Boolean", "Enable data drift detection (only for HTTP protocol)", "", false, false))

            // Define the Update_Frequency_Parameter signal
            List <JobVariable> signalVariables_frequency = new java.util.ArrayList<JobVariable>()
            signalVariables_frequency.add(new JobVariable("INFERENCE_FREQUENCY", ""+INFERENCE_FREQUENCY, "PA:Integer", "The frequency (in ms) to send a new inference request for the selected model.", "", false, false))

            // Read the variable SIGNALS
            signals = variables.get("SIGNALS")

            // Split the value of the variable SIGNALS and transform it into a list
            Set signalsSet = new HashSet<>(Splitter.on(',').trimResults().omitEmptyStrings().splitToList(signals))

            // Send a ready notification for each signal in the set with updated variables
            println("Ready for signals "+ signalsSet)
            signalsSet.each{ signal ->
                if(signal.equals("Terminate_Job")) {
                    signalapi.readyForSignal(signal);
                } else if (signal.equals("Update_Frequency_Parameter")) {
                    signalapi.readyForSignal(signal, signalVariables_frequency)
                } else if (signal.equals("Update_Inference_Parameters")) {
                    signalapi.readyForSignal(signal, signalVariables)
                }
            }

            // Add a variable to avoid sending ready notifications again
            variables.put("READY_SENT", true)

            // Add the signals set as a variable to be used by next tasks
            variables.put("SIGNALS_SET", signalsSet)
        }

        //Read the variable SIGNALS_SET
        Set signalsSet =  variables.get("SIGNALS_SET")

        // Check whether one signal among those specified as input is received
        println("Checking whether one signal in the set "+ signalsSet +" is received")
        receivedSignals = signalapi.checkForSignals(signalsSet)

        // Check if a signal was received
        if (receivedSignals != null && !receivedSignals.isEmpty()) {
            // Print the received signal
            println("Received signals: " + receivedSignals.toString())
            for (String s : Arrays.asList(receivedSignals.keySet())) {
                if (containsIgnoreCase(s, "Terminate_Job")) {
                    result = "Terminate_Job"
                    break
                }
                if (containsIgnoreCase(s, "Update_Frequency_Parameter")) {
                    updatedVariables = receivedSignals.get("Update_Frequency_Parameter")
                    INFERENCE_FREQUENCY = updatedVariables.get("INFERENCE_FREQUENCY") as Integer
                    result = "Update_Frequency_Parameter"
                }
                if (containsIgnoreCase(s, "Update_Inference_Parameters")) {
                    updatedVariables = receivedSignals.get("Update_Inference_Parameters")
                    MODEL_SERVER_ID = updatedVariables.get("MODEL_SERVER_ID") as Integer
                    MODEL_NAME = updatedVariables.get("MODEL_NAME")
                    IMAGE_PATH = updatedVariables.get("IMAGE_PATH")
                    DRIFT_DATA = updatedVariables.get("DRIFT_DATA")
                    INFERENCE_PROTOCOL = updatedVariables.get("INFERENCE_PROTOCOL")
                    USE_PROXY = updatedVariables.get("USE_PROXY") as Boolean
                    INFERENCE_URL = getInferenceUrl(MODEL_SERVER_ID, INFERENCE_PROTOCOL, USE_PROXY)
                    println("MODEL_SERVER_ID:    " + MODEL_SERVER_ID)
                    println("MODEL_NAME:         " + MODEL_NAME)
                    println("IMAGE_PATH:         " + IMAGE_PATH)
                    println("DRIFT_DATA:         " + DRIFT_DATA)
                    println("INFERENCE_PROTOCOL: " + INFERENCE_PROTOCOL)
                    println("USE_PROXY:          " + USE_PROXY)
                    println("INFERENCE_URL:      " + INFERENCE_URL)
                    println("TASK_FILE_PATH:     " + TASK_FILE_PATH)
                    result = "Update_Inference_Parameters"
                    // Re-generate the inference script using the new variable values
                    create_inference_script(TASK_FILE_PATH, INFERENCE_URL, MODEL_NAME, IMAGE_PATH, DRIFT_DATA, INFERENCE_PROTOCOL)
                }
            }
        } else {
            result = null
        }
        println("result: " + result)
        if ("Terminate_Job".equalsIgnoreCase(result)){
            break
        }
        firstStart = 0
    }
    def command = "sh inference.sh"
    def output = command.execute().text
    println(output)
    int endTime = System.currentTimeMillis()
    timeSpent = endTime - startTime
    println("timeSpent: " + timeSpent)
    sleepTime = INFERENCE_FREQUENCY - timeSpent
    if (sleepTime < 0) {
        sleepTime = 0
    }
    println("sleepTime: " + sleepTime)
    sleep(sleepTime)
}
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            153.578125
        </positionTop>
        <positionLeft>
            389.671875
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2728px;
            height:3492px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-148.578125px;left:-384.671875px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable active-task" id="jsPlumb_1_39" style="top: 153.578px; left: 389.672px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="" data-original-title="Inference workflow that periodically sends inferences consuming a specific deployed model."><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">Model_Inference_Periodic</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i title="Workflows being Called by this Task" id="called-icon"></i></a><a title="Scripts being Called by this Task" id="reference-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: -7px; right: 3px;"><i id="reference-icon" class="glyphicon glyphicon-list-alt"></i></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 458.5px; top: 184px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
