<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.14" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="MLOps_Model_Server_Application_Performance_Analyzer" onTaskError="continueJobExecution" priority="normal" projectName="3. MLOps Model Server Workflows" tags="MLOps,Model Inference,Triton" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd">
  <variables>
    <variable advanced="false" description="Id of the model server where the model that need to be consumed is deployed" group="Model Server" hidden="false" name="MODEL_SERVER_ID" value=""/>
    <variable advanced="true" description="GRPC inference url of the model server (e.g. localhost:8001)." group="Model Server" hidden="false" name="GRPC_INFERENCE_URL" value=""/>
    <variable advanced="false" description="Name of the model to be tested." group="Model Server" hidden="false" model="PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)" name="MODEL_NAME" value="simple"/>
    <variable advanced="true" description="Container platform used for executing the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(no-container,docker,podman,singularity)" name="CONTAINER_PLATFORM" value="docker"/>
    <variable advanced="true" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" hidden="false" model="PA:LIST(,nvcr.io/nvidia/tritonserver:22.10-py3-sdk,nvcr.io/nvidia/tritonserver:23.05-py3-sdk)" name="CONTAINER_IMAGE" value="nvcr.io/nvidia/tritonserver:23.05-py3-sdk"/>
    <variable advanced="false" description="The concurrency-range option is used to define the range of concurrency levels that will be evaluated to measure the performance of a deployed model. The format for defining this range is start:end:step, where start is the starting concurrency level, end is the ending concurrency level, and step is the increment between each measurement. The performance analysis will begin at the concurrency level specified by start and will continue until end in strides of step." group="Model Server" hidden="false" model="PA:REGEXP(^(\d+):(\d+):(\d+)$)?" name="CONCURRENCY_RANGE" value="1:4:1"/>
    <variable advanced="true" description="True, if you want to use a specific GRPC_INFERENCE_URL to send inferences instead of the MODEL_SERVER_ID" group="Model Server" hidden="false" model="PA:Boolean" name="USE_GRPC" value="False"/>
  </variables>
  <description>
    <![CDATA[ This workflow helps to measure the inference performance of the deployed models on the Model Server. It measures the changes in performance by testing different concurrency ranges.

To analyze the performance of your deployed model, you need to specify your MODEL_NAME, the GRPC_INFERENCE_URI of the Model Server where the model is deployed, and the concurrency range.

In concurrency mode, the workflow attempts to send inference requests to the server such that N requests are always outstanding during the test. For example, when using concurrency range = 4, it will attempt to have 4 outgoing inference requests at all times during the test.

The performance report can be accessed at the end of the execution via the job results tab. The model latency is broken down into the following components:

Queue: The average time spent in the inference schedule queue by a request waiting for an instance of the model to become available.
Compute: The average time spent performing the actual inference, including any time needed to copy data to/from the GPU/CPU.
Overhead: The average time spent in the endpoint that cannot be correctly captured in the send/receive time. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="ai-mlops-dashboard"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
<info name="Documentation" value="PAIO/PAIOUserGuide.html"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task fork="true" name="Model_Performance_Analyzer" preciousResult="true">
      <description>
        <![CDATA[ Analyze the performance of a deployed model by sending multiple inferences concurrently ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html"/>
        <info name="PRE_SCRIPT_AS_FILE" value="main.py"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[
import os
import sys
import json
import argparse
import subprocess
import numpy as np


def generate_simple_input():
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    input0_data = np.arange(start=0, stop=16, dtype=np.int32)
    input0_data = np.expand_dims(input0_data, axis=0)
    input1_data = np.ones(shape=(1, 16), dtype=np.int32)
    # Create a dictionary to hold the input data
    input_dict = {
        "data": [
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            },
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            },
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            },
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            }
        ]
    }
    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple was successfully generated and saved')


def generate_simple_identity_input():
    # Create the data for simple_identity model
    input_dict = {
        "data": [
            {
                "INPUT0":
                {
                    "content": ["1"],
                    "shape": [1]
                },
                "INPUT0":
                {
                    "content": ["1"],
                    "shape": [1]
                }
            }
        ]
    }
    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple_identity was successfully generated and saved')


def generate_simple_int8_input():
    # We use a simple model that takes 2 input tensors of 16 integers
    # each and returns 2 output tensors of 16 integers each. One
    # Input data
    input0_data = [i for i in range(16)]
    input1_data = [1 for i in range(16)]
    # Create a dictionary to hold the input data
    input_dict = {
        "data": [
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            },
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            },
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            },
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            }
        ]
    }
    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple_int8 was successfully generated and saved')


def generate_simple_string_input():
    # Create a dictionary to hold the input data
    input_dict = {
        "data": [
            {
                "INPUT0": ["1","2","3","4","5","6","7","8","9","1","2","3","4","2","9","3"],
                "INPUT1": ["1","2","3","4","5","6","7","8","9","1","2","3","4","2","9","3"]
            }
        ]
    }
    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple_string was successfully generated and saved')


if __name__ == "__main__":
    # Define script args
    parser = argparse.ArgumentParser()
    parser.add_argument('-m',
                        '--model_name',
                        type=str,
                        required=True,
                        help='Name of the deployed model to be tested.')
    args, unknowns = parser.parse_known_args()
    # Create a json file containing the right data expected by each model
    if args.model_name == "simple":
        generate_simple_input()
    elif args.model_name == "simple_identity":
        generate_simple_identity_input()
    elif args.model_name == "simple_int8":
        generate_simple_int8_input()
    elif args.model_name == "simple_string":
        generate_simple_string_input()
    else :
        print("Please define a valid model name as an argument")
    if unknowns:
        print("Unknown arguments: {}", unknowns)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import groovy.io.FileType
import com.google.common.base.Splitter;
import org.ow2.proactive.scheduler.common.job.JobVariable;
import groovy.json.JsonSlurper

// -------------------------------------------------------------
// Extract the GRPC_INFERENCE_URL using the MODEL_SERVER_ID
String getGrpcEndpointUrl(String sessionid, String proactiveUrl, String instanceId) {
    // Define headers
    def headers = ['sessionid': sessionid]
    // Define base URL
    def baseUrl = "$proactiveUrl/cloud-automation-service/serviceInstances/$instanceId"
    // Create URL object
    def url = new URL(baseUrl)
    // Open connection
    def connection = url.openConnection()
    // Set headers
    headers.each { key, value ->
        connection.setRequestProperty(key, value)
    }
    // Get response
    def response = connection.getInputStream()
    // Parse JSON response
    def json = new JsonSlurper().parseText(response.text)
    // Fetch GRPC_ENDPOINT_ID
    def grpcEndpointId = json.deployments.endpoint.find { it.id.contains("-grpc") }?.id
    // Fetch GRPC_ENDPOINT_URL
    def grpcEndpointUrl = json.deployments.endpoint.find { it.id == grpcEndpointId }?.url
    return grpcEndpointUrl
}

// Usage example
schedulerapi.connect()
connectionInfo = schedulerapi.getConnectionInfo()
ciLogin = connectionInfo.getLogin()
ciPasswd = connectionInfo.getPassword()
ciUrl = connectionInfo.getUrl()
def sessionId = schedulerapi.getSession()

user_credentials = [
    sessionId: sessionId,
    ciLogin: ciLogin,
    ciPasswd: ciPasswd,
    ciUrl: ciUrl
]
// def sessionId = 'your_session_id'
def url = new URL(ciUrl)
def proactiveUrl = url.getProtocol() + "://" + url.getHost() + ":" + url.getPort()
def instanceId = variables.get("MODEL_SERVER_ID")
def USE_GRPC = variables.get("USE_GRPC")
println "USE_GRPC: $USE_GRPC"

String GRPC_INFERENCE_URL = ""  // Declare and initialize the variable outside the conditional blocks
if (USE_GRPC == "false") {
    GRPC_INFERENCE_URL = getGrpcEndpointUrl(sessionId, proactiveUrl, instanceId)
    println "Using the defined variable MODEL_SERVER_ID to get the GRPC_INFERENCE_URL associated with it."
} else {
    GRPC_INFERENCE_URL = variables.get("GRPC_INFERENCE_URL")
    println "Using the defined variable GRPC_INFERENCE_URL to send inferences."
}
println("GRPC_INFERENCE_URL: " + GRPC_INFERENCE_URL)
// -------------------------------------------------------------

String MODEL_NAME = variables.get("MODEL_NAME")
String CONCURRENCY_RANGE = variables.get("CONCURRENCY_RANGE")

// -------------------------------------------------------------
// Execute the performance analyzer on the selected model
def file = new File("inference.sh")
try {
    if (file.exists()) {
        println "inference.sh file exists"
        file.delete()
        println "inference.sh file deleted successfully"
    }
    println "Creating a new inference.sh file"
    def bash_command = """
        #!/bin/bash
        if [ $MODEL_NAME = "inception_graphdef" ] || [ $MODEL_NAME = "densenet_onnx" ] || [ $MODEL_NAME = "simple_sequence" ] ; then
            echo perf_analyzer -i grpc -u $GRPC_INFERENCE_URL -m $MODEL_NAME --concurrency-range $CONCURRENCY_RANGE -f perf.csv
            perf_analyzer -i grpc -u $GRPC_INFERENCE_URL -m $MODEL_NAME --concurrency-range $CONCURRENCY_RANGE -f perf.csv
        else
            echo python main.py --model_name $MODEL_NAME
            python main.py --model_name $MODEL_NAME
            echo perf_analyzer -i grpc -u $GRPC_INFERENCE_URL -m $MODEL_NAME --input-data model_input.json --concurrency-range $CONCURRENCY_RANGE -f perf.csv
            perf_analyzer -i grpc -u $GRPC_INFERENCE_URL -m $MODEL_NAME --input-data model_input.json --concurrency-range $CONCURRENCY_RANGE -f perf.csv
        fi
        cp perf.csv perf.bkp
        sort -k1 -n -t, perf.bkp > perf.csv
    """
    file << bash_command
    file.setExecutable(true)
    command = "sh inference.sh"
    println "Running " + command
    // def output = command.execute().text
    // println(output)
    def process = command.execute()
    process.consumeProcessOutput(System.out, System.err)
    process.waitFor()
} catch (Exception e) {
    println("Failed to create inference.sh file: ${e.getMessage()}")
}
// -------------------------------------------------------------

// Read the CSV output file
def inputFile = new File('perf.csv')
if (inputFile.exists()) {
    println "Parsing the results from the perf.csv file"
    def lines = inputFile.readLines()
    def headers = lines[0].split(',')
    // Generate the HTML output
    def html = new StringBuilder()
    html << """
    <!DOCTYPE html>
    <html>
    <head>
    <style>
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background-color: rgb(51, 122, 183);
        color: white; /* Optional - this will set the text color to white */
        font-weight: bold;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    </style>
    </head>
    <body>
    <table>
    """
    // Add table headers
    html << '<tr>'
    headers.each { header ->
        html << "<th>${header}</th>"
    }
    html << '</tr>'
    // Add table data
    lines.drop(1).each { line ->
        def values = line.split(',')
        html << '<tr>'
        values.each { value ->
            html << "<td>${value}</td>"
        }
        html << '</tr>'
    }
    // Add the description of the different fields
    html << """
    </table>
    <p><strong>The Description of the Performance Report Fields:</strong></p>
    <ul>
    <li><strong>Concurrency:</strong> number of requests sent at the same time</li>
    <li><strong>Inferences/Second:</strong> number of inferences made per second. It indicates the computational efficiency and speed of the model in processing input data and providing results.</li>
    <li><strong>Client Send:</strong> total amount of time it takes the client to send a request, plus the amount of time it takes for the client to receive the response.</li>
    <li><strong>Network+Server Send/Recv:</strong> combined time taken for both network communication and server processing during the execution of the inference.</li>
    <li><strong>Server Queue:</strong> average time spent in the inference schedule queue by a request waiting for an instance of the model to become available.</li>
    <li><strong>Server Compute Input:</strong> Time needed to copy data to the GPU/CPU from input buffers.</li>
    <li><strong>Server Compute Infer:</strong> Average time spent performing the actual inference.</li>
    <li><strong>Server Compute Output:</strong> Time needed to copy data from the GPU/CPU to output buffers.</li>
    <li><strong>Client Recv:</strong> the amount of time it takes for the client to receive the response.</li>
    <li><strong>p50 latency:</strong> the 50th percentile of latency (inference time)</li>
    <ul>
    <li><em>50th percentile:</em> it means that a certain value separates all inference latencies in such a way that 50% of the values are below that point, and only 50% are higher.</li>
    </ul>
    <li><strong>p90 latency:</strong> the 90th percentile of latency (inference time)</li>
    <ul>
    <li><em>90th percentile:</em> it means that a certain value separates all inference
    </ul>
    <li><strong>p95 latency:</strong> the 95th percentile of latency</li>
    <li><strong>p99 latency:</strong> the 99th percentile of latency</li>
    </body>
    </html>
    """
    byte[] htmlBytes = html.toString().getBytes("UTF-8")
    result = htmlBytes
    println "Done"
} else {
    println "File perf.csv not found"
    result = ""
}
resultMetadata.put("file.extension", ".html")
resultMetadata.put("file.name", "output.html")
resultMetadata.put("content.type", "text/html")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
      <metadata>
        <positionTop>
            130.515625
        </positionTop>
        <positionLeft>
            326.390625
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2506px;
            height:3516px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-125.515625px;left:-321.390625px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable active-task" id="jsPlumb_1_4" style="top: 130.521px; left: 326.398px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Analyze the performance of a deployed model by sending multiple inferences concurrently"><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">Model_Performance_Analyzer</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i title="Workflows being Called by this Task" id="called-icon"></i></a><a title="Scripts being Called by this Task" id="reference-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: -7px; right: 3px;"><i id="reference-icon" class="glyphicon glyphicon-list-alt"></i></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 403.5px; top: 161px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style="--darkreader-inline-fill: #a8a095; --darkreader-inline-stroke: none;" data-darkreader-inline-fill="" data-darkreader-inline-stroke=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
