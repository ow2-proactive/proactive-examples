<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="MLOps_Model_Server_Application_Performance_Analyzer" projectName="3. MLOps Model Server Workflows" tags="MLOps,Model Inference,Triton" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="GRPC_INFERENCE_URL" value="" model="" description="GRPC inference url of the model server (e.g. localhost:8001)." group="Model Server" advanced="false" hidden="false"/>
    <variable name="MODEL_NAME" value="simple" model="PA:LIST(simple,simple_identity,simple_int8,simple_sequence,simple_string,densenet_onnx,inception_graphdef)" description="Name of the model to be tested." group="Model Server" advanced="false" hidden="false"/>
    <variable name="CONTAINER_PLATFORM" value="docker" model="PA:LIST(no-container,docker,podman,singularity)" description="Container platform used for executing the workflow tasks." group="Container Parameters" advanced="true" hidden="false"/>
    <variable name="CONTAINER_IMAGE" value="nvcr.io/nvidia/tritonserver:22.10-py3-sdk" model="PA:LIST(,nvcr.io/nvidia/tritonserver:22.10-py3-sdk)" description="Name of the container image being used to run the workflow tasks." group="Container Parameters" advanced="true" hidden="false"/>
    <variable name="CONCURRENCY_RANGE" value="1:4:1" model="PA:REGEXP(^(\d+):(\d+):(\d+)$)?" description="The concurrency-range option is used to define the range of concurrency levels that will be evaluated to measure the performance of a deployed model. The format for defining this range is start:end:step, where start is the starting concurrency level, end is the ending concurrency level, and step is the increment between each measurement. The performance analysis will begin at the concurrency level specified by start and will continue until end in strides of step." group="Model Server" advanced="false" hidden="false"/>
  </variables>
  <description>
    <![CDATA[ This workflow helps to measure the inference performance of the deployed models on the Model Server. It measures the changes in performance by testing different concurrency ranges.

To analyze the performance of your deployed model, you need to specify your MODEL_NAME, the GRPC_INFERENCE_URI of the Model Server where the model is deployed, and the concurrency range.

In concurrency mode, the workflow attempts to send inference requests to the server such that N requests are always outstanding during the test. For example, when using concurrency range = 4, it will attempt to have 4 outgoing inference requests at all times during the test.

The performance report can be accessed at the end of the execution via the job results tab. The model latency is broken down into the following components:

Queue: The average time spent in the inference schedule queue by a request waiting for an instance of the model to become available.
Compute: The average time spent performing the actual inference, including any time needed to copy data to/from the GPU/CPU.
Overhead: The average time spent in the endpoint that cannot be correctly captured in the send/receive time. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="ai-mlops-dashboard"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
    <info name="Documentation" value="PAIO/PAIOUserGuide.html"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Model_Performance_Analyzer" 
    
    
    
    preciousResult="true" 
    fork="true">
      <description>
        <![CDATA[ Analyze the performance of a deployed model by sending multiple inferences concurrently ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png"/>
        <info name="task.documentation" value="PAIO/PAIOUserGuide.html"/>
        <info name="PRE_SCRIPT_AS_FILE" value="main.py"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_ai/raw" language="groovy"></file>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="cpython">
            <![CDATA[

import json
import argparse
import sys
import numpy as np
import subprocess
import os

def generate_simple_input():
    # Create the data for the two input tensors. Initialize the first
    # to unique integers and the second to all ones.
    input0_data = np.arange(start=0, stop=16, dtype=np.int32)
    input0_data = np.expand_dims(input0_data, axis=0)
    input1_data = np.ones(shape=(1, 16), dtype=np.int32)

    # Create a dictionary to hold the input data
    input_dict = {
        "data": [
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            },
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            },
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            },
            {
                "INPUT0": input0_data[0].tolist(),
                "INPUT1": input1_data[0].tolist()
            }
        ]
    }


    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple was successfully generated and saved')


def generate_simple_identity_input():
    # Create the data for simple_identity model
    input_dict = {
    "data":
      [
        {
            "INPUT0":
            {
                "content": ["1"],
                "shape": [1]
            },
            "INPUT0":
            {
                "content": ["1"],
                "shape": [1]
            }
        }
      ]
    }

    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple_identity was successfully generated and saved')


def generate_simple_int8_input():
    # We use a simple model that takes 2 input tensors of 16 integers
    # each and returns 2 output tensors of 16 integers each. One
    # Input data
    input0_data = [i for i in range(16)]
    input1_data = [1 for i in range(16)]

        # Create a dictionary to hold the input data
    input_dict = {
        "data": [
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            },
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            },
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            },
            {
                "INPUT0": input0_data,
                "INPUT1": input1_data
            }
        ]
    }

    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple_int8 was successfully generated and saved')


def generate_simple_string_input():
    # Create a dictionary to hold the input data
    input_dict = {
        "data":
            [
            {
                "INPUT0": ["1","2","3","4","5","6","7","8","9","1","2","3","4","2","9","3"],
                "INPUT1": ["1","2","3","4","5","6","7","8","9","1","2","3","4","2","9","3"]
            }
            ]
        }

    # Write the dictionary to a JSON file
    with open('model_input.json', 'w') as f:
        json.dump(input_dict, f)
    print('Input Data for model : simple_string was successfully generated and saved')


if __name__ == "__main__":

    # Define script args
    parser = argparse.ArgumentParser()
    parser.add_argument('-m',
                        '--model_name',
                        type=str,
                        required=True,
                        help='Name of the deployed model to be tested.')
    args, unknowns = parser.parse_known_args()

    # Create a json file containing the right data expected by each model
    if args.model_name == "simple":
        generate_simple_input()
    elif args.model_name == "simple_identity":
        generate_simple_identity_input()
    elif args.model_name == "simple_int8":
        generate_simple_int8_input()
    elif args.model_name == "simple_string":
        generate_simple_string_input()
    else :
        print("Please define a valid model name as an argument")
    if unknowns:
        print("Unknown arguments: {}", unknowns)
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import groovy.io.FileType

String GRPC_INFERENCE_URL = variables.get("GRPC_INFERENCE_URL")
String MODEL_NAME = variables.get("MODEL_NAME")
String CONCURRENCY_RANGE = variables.get("CONCURRENCY_RANGE")

// -------------------------------------------------------------
// Execute the performance analyzer on the selected model
def file = new File("inference.sh")
try {
    if (file.exists()) {
        println "inference.sh file exists"
        file.delete()
        println "inference.sh file deleted successfully"
    }
    println "Creating a new inference.sh file"
    def bash_command = """
            #!/bin/bash
            if [ $MODEL_NAME = "inception_graphdef" ] || [ $MODEL_NAME = "densenet_onnx" ] || [ $MODEL_NAME = "simple_sequence" ] ; then
                perf_analyzer -i grpc -u $GRPC_INFERENCE_URL -m $MODEL_NAME --concurrency-range $CONCURRENCY_RANGE -f perf.csv
            else
            	python main.py --model_name $MODEL_NAME
                perf_analyzer -i grpc -u $GRPC_INFERENCE_URL -m $MODEL_NAME --input-data model_input.json --concurrency-range $CONCURRENCY_RANGE -f perf.csv
            fi
        """
    // def command = "perf_analyzer -i grpc -u $GRPC_INFERENCE_URL -m $MODEL_NAME  --input-data model_input.json --concurrency-range $CONCURRENCY_RANGE -f perf.csv"
    
    file << bash_command
    file.setExecutable(true)
    command = "sh inference.sh"
    println "Running " + command
    def output = command.execute().text
    println(output)
} catch (Exception e) {
        println("Failed to create inference.sh file: ${e.getMessage()}")
}
// -------------------------------------------------------------

// Read the CSV output file
def inputFile = new File('perf.csv')
if (inputFile.exists()) {
    println "Parsing the results from the perf.csv file"
    def lines = inputFile.readLines()
    def headers = lines[0].split(',')
    // Generate the HTML output
    def html = new StringBuilder()
    html << """
    <!DOCTYPE html>
    <html>
    <head>
    <style>
      table {
        width: 100%;
        border-collapse: collapse;
      }
      th, td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #ddd;
      }
      th {
        background-color: rgb(51, 122, 183);
        color: white; /* Optional - this will set the text color to white */
        font-weight: bold;
      }
      tr:nth-child(even) {
        background-color: #f2f2f2;
      }
    </style>
    </head>
    <body>
    <table>
    """
    // Add table headers
    html << '<tr>'
    headers.each { header ->
        html << "<th>${header}</th>"
    }
    html << '</tr>'
    // Add table data
    lines.drop(1).each { line ->
        def values = line.split(',')
        html << '<tr>'
        values.each { value ->
            html << "<td>${value}</td>"
        }
        html << '</tr>'
    }
    byte[] htmlBytes = html.toString().getBytes("UTF-8")
    result = htmlBytes
    println "Done"
} else {
  println "File perf.csv not found"
  result = ""
}
resultMetadata.put("file.extension", ".html")
resultMetadata.put("file.name", "output.html")
resultMetadata.put("content.type", "text/html")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
      <metadata>
        <positionTop>
            181.96182250976562
        </positionTop>
        <positionLeft>
            325.4340515136719
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2388px;
            height:3031px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-176.96182250976562px;left:-320.4340515136719px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_43" style="top: 181.962px; left: 325.442px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Analyze the performance of a deployed model by sending multiple inferences concurrently"><img src="/automation-dashboard/styles/patterns/img/wf-icons/model_server.png" width="20px">&nbsp;<span class="name">Model_Performance_Analyzer</span></a>&nbsp;&nbsp;<a id="called-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: 17px; right: 3px;"><i title="Workflows being Called by this Task" id="called-icon"></i></a><a title="Scripts being Called by this Task" id="reference-icon-a" href="javascript:void(0)" class="pointer" style=" position: inherit; top: -7px; right: 3px;"><i id="reference-icon" class="glyphicon glyphicon-list-alt"></i></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 402px; top: 212px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
