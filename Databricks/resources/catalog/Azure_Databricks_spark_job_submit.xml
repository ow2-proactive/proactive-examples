<?xml version="1.0" encoding="UTF-8"?>
<job
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns="urn:proactive:jobdescriptor:3.10"
        xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
        name="Azure_Databricks_spark_job_submit" projectName="Azure Big Data"
        priority="normal"
        onTaskError="continueJobExecution"
        maxNumberOfExecution="2"
>
  <variables>
    <variable name="domain" value="westeurope.azuredatabricks.net"/>
    <variable name="token" value="my_databricks_workspace_token"/>
    <variable name="existing_cluster_id" value="my_spark_cluster_id" />
    <variable name="jar_path_from_dataspace" value="my_jar_to_put_to_the_dbfs" />
    <variable name="main_class_name" value="my.example.class" />
    <variable name="parameters" value="" />
    <variable name="run_name" value="my_first_spark_job" />
    <variable name="timeout_seconds" value="3600" model="PA:Integer"/>
    <variable name="max_retries" value="1" model="PA:Integer"/>
  </variables>
  <description>
    <![CDATA[ Creates a new Spark job with the provided settings and runs it now. ]]>
  </description>
  <genericInformation>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/databricks.png"/>
    <info name="Documentation" value="https://docs.azuredatabricks.net/api/latest/index.html"/>
  </genericInformation>
  <taskFlow>

    <task name="Azure_Databricks_spark_job_submit">
      <genericInformation>
        <info name="task.documentation" value="https://docs.azuredatabricks.net/api/latest/index.html"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/databricks.png"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import groovy.json.JsonBuilder
import org.apache.http.entity.StringEntity
import org.apache.http.client.methods.HttpPost
import org.apache.http.entity.mime.MultipartEntityBuilder
import org.apache.http.impl.client.HttpClientBuilder
import org.apache.http.entity.mime.content.FileBody
import org.apache.http.entity.mime.content.StringBody
import org.apache.http.entity.ContentType


// Retrieve variables
def domain = variables.get("domain")
def token = variables.get("token")
def existing_cluster_id = variables.get("existing_cluster_id")
def jar_path_from_dataspace = variables.get("jar_path_from_dataspace")
def main_class_name = variables.get("main_class_name")
def parameters = variables.get("parameters")
def run_name = variables.get("run_name")
def timeout_seconds = variables.get("timeout_seconds")
def max_retries = variables.get("max_retries")


// Build the command params
def json = new JsonBuilder([run_name: run_name, existing_cluster_id: existing_cluster_id, libraries: ["jar": "dbfs:/" + jar_path_from_dataspace], timeout_seconds: timeout_seconds, max_retries: max_retries, spark_jar_task: ["main_class_name": main_class_name, "parameters": parameters] ])
println json.toString()

// Build the command
def submit_query = "https://" + domain + "/api/2.0/jobs/runs/submit"
def submit_post = new HttpPost(submit_query)
submit_post.addHeader("Content-Type", "application/json")
submit_post.addHeader("Authorization", "Bearer " +  token)
submit_post.setEntity(new StringEntity(json.toString()));
println "RUNS/SUBMIT REQUEST=> " + submit_post.getRequestLine()

// Execute the command
def submit_response = HttpClientBuilder.create().build().execute(submit_post)
println "RUNS/SUBMIT RESPONSE=> " + submit_response
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>