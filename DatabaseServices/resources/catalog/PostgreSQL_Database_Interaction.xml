<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="PostgreSQL_Database_Interaction" projectName="PostgreSQL Workflows"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="POSTGRES_DATABASE" value="activeeon" />
    <variable name="POSTGRES_INSTANCE_NAME" value="postgres-server-1" />
    <variable name="POSTGRES_PASSWORD" value="proactive" />
    <variable name="POSTGRES_USER" value="mlos" />
  </variables>
  <description>
    <![CDATA[ This workflow shows how to use PCA to ease deployment of service dependencies.
Here, any PostgreSQL Task needs PostgreSQL service.
First, a bind_or_start_service task (Examples/5. Cloud Automation Task Templates), specifies the service we want to start, and an instance name, which is inherited by a job variable.
Thus, any PostgreSQL Task can use a "endpoint" variable where service is running. Everything is handled by the previous bind_or_start_service task. 
In this example, we use PostgreSQL data connector workflow to show how to export data to and import data from the database. 
You can share PostgreSQL service reusing the same instance name. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="database-services"/>
    <info name="Documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
    <info name="group" value="public-objects"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
  </genericInformation>
  <taskFlow>
    <task name="Parse_Endpoint">
      <description>
        <![CDATA[ The simplest task, ran by a groovy engine. ]]>
      </description>
      <depends>
        <task ref="Start_PostgreSQL_Service"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
def postgres_endpoint = variables.get("postgres_endpoint")
def POSTGRES_HOST
def POSTGRES_USER = variables.get("POSTGRES_USER")
def POSTGRES_PASSWORD = variables.get("POSTGRES_PASSWORD")
def POSTGRES_PORT
def POSTGRES_PASSWORD_KEY

print("POSTGRES_USER="+POSTGRES_USER)

if (POSTGRES_USER.isEmpty()){
    POSTGRES_USER = "postgres"
    variables.put("POSTGRES_USER", POSTGRES_USER)
}

if (postgres_endpoint != null){
  postgres_endpoint = postgres_endpoint.replace("http://", "")
  POSTGRES_HOST = postgres_endpoint.split(":")[0]
  variables.put("POSTGRES_HOSTNAME", POSTGRES_HOST)
  POSTGRES_PORT = postgres_endpoint.split(":")[1]
  variables.put("POSTGRES_PORT", POSTGRES_PORT)
  
  // This key is used for getting the password from 3rd party credentials.
  POSTGRES_PASSWORD_KEY = "postgres://" + POSTGRES_USER + "@" + POSTGRES_HOST + ":" + POSTGRES_PORT
  schedulerapi.connect()
  schedulerapi.putThirdPartyCredential(POSTGRES_PASSWORD_KEY, POSTGRES_PASSWORD)
}
else{
  throw new IOException("PostgreSQL endpoint not found")
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Start_PostgreSQL_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ Start the PostgreSQL server as a service. ]]>
      </description>
      <variables>
        <variable name="POSTGRES_SERVICE_ID" value="PostgreSQL" inherited="false" />
        <variable name="POSTGRES_INSTANCE_NAME" value="postgres-server-1" inherited="true" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Start_PostgreSQL_Service ---")

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription
http://localhost:8080/studio/#
// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

def service_id = variables.get("POSTGRES_SERVICE_ID")
def instance_name = variables.get("POSTGRES_INSTANCE_NAME")
def user = variables.get("POSTGRES_USER")
def password = variables.get("POSTGRES_PASSWORD")
def database = variables.get("POSTGRES_DATABASE")
println("*_service_id:    " + service_id)
println("*_instance_name: " + instance_name)
println("*_user: " + user)
println("*_password: " + password)
println("*_database: " + database)

// Check existing service instances
boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
	if ( (service_instance_data.getServiceId() == service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      if (service_instance_data.getVariables().get("POSTGRES_INSTANCE_NAME") == instance_name) {
        instance_exists = true
        instance_id = service_instance_data.getInstanceId()
  		endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
        println("*_instance_id: " + instance_id)
        println("*_endpoint:    " + endpoint)
        variables.put("POSTGRES_INSTANCE_ID", instance_id)
        variables.put("postgres_endpoint", endpoint)
        break
      }
  	}
}

println("instance_exists: " + instance_exists)

if (!instance_exists){
  // Prepare service description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(service_id) 
  serviceDescription.putVariablesItem("POSTGRES_INSTANCE_NAME_CL", instance_name)
  serviceDescription.putVariablesItem("POSTGRES_USER_CL", user)
  serviceDescription.putVariablesItem("POSTGRES_PASSWORD_CL", password)
  serviceDescription.putVariablesItem("POSTGRES_DATABASE_CL", database)
  
  // Run service
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire service Instance ID
  def service_instance_id = service_instance_data.getInstanceId()
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id
  println("channel: " + channel)
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire service endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  def instance_id = service_instance_data.getInstanceId()
  endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
  
  println("*_instance_id: " + instance_id)
  println("*_endpoint: " + endpoint)
  
  variables.put("POSTGRES_INSTANCE_ID", instance_id)
  variables.put("postgres_endpoint", endpoint)
}

println("--- END Start_PostgreSQL_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Finish_PostgreSQL_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ Finish the PostgreSQL service. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Import_from_PostgreSQL"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Finish_PostgreSQL_Service ---")

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

def instance_id = (int) variables.get("POSTGRES_INSTANCE_ID")
println("*_instance_id: " + instance_id)
assert instance_id != null

// Finish service
ServiceDescription service = new ServiceDescription()
service.setBucketName("cloud-automation") 
service.setWorkflowName("Finish_PostgreSQL")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, instance_id, service)

println("--- END Finish_PostgreSQL_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Import_from_PostgreSQL">
      <description>
        <![CDATA[ This task allows to import data from PostgreSQL database.
It requires the following third-party credentials: POSTGRES_USERNAME and POSTGRES_PASSWORD. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables: 
$LABEL (optional) used when the imported data is labeled. Then, the user can specify the label column name.
$POSTGRES_QUERY (required) is the user's sql query.
$OUTPUT_FILE (optional) is a relative path in the data space used to save the results in a CSV file.
This task uses also the task variable RMDB_DRIVER as a driver to connect to the database. The specified default driver "psycopg2"is already provided for this task. To use another driver, make sure you have it properly installed before. 
The imported data is exported in a JSON format using the variable $DATAFRAME_JSON. ]]>
      </description>
      <variables>
        <variable name="LABEL" value="" inherited="false" />
        <variable name="POSTGRES_QUERY" value="SELECT * FROM diabetes" inherited="false" />
        <variable name="RDBMS_DRIVER" value="psycopg2" inherited="false" />
        <variable name="OUTPUT_FILE" value="imported_data.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
        <info name="task.documentation" value="http://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Export_to_PostgreSQL"/>
      </depends>
      <forkEnvironment javaHome="/usr" ></forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import sys

RDBMS_NAME = 'postgresql'
RDBMS_DRIVER = variables.get("RDBMS_DRIVER")
OUTPUT_FILE = None
POSTGRES_URL_KEY = "postgres://<username>@<hostname>:<port>"

HOSTNAME = variables.get("POSTGRES_HOSTNAME")
PORT = int(variables.get("POSTGRES_PORT"))
DATABASE = variables.get("POSTGRES_DATABASE")
USER = variables.get("POSTGRES_USER")
# This key is used for getting the password from 3rd party credentials.
POSTGRES_PASSWORD_KEY = "postgres://" + USER + "@" + HOSTNAME + ":" + str(PORT)
POSTGRES_PASSWORD=credentials.get(POSTGRES_PASSWORD_KEY)
SQL_QUERY = variables.get("POSTGRES_QUERY")
OUTPUT_FILE = variables.get("OUTPUT_FILE")

if not HOSTNAME:
    print("ERROR: POSTGRES_HOSTNAME not defined by the user.")
    sys.exit(1)
if not PORT:
    PORT = 5432
    print("POSTGRES_PORT not defined by the user. Using the default value:", PORT)
if not DATABASE:
    print("ERROR: POSTGRES_DATABASE not defined by the user.")
    sys.exit(1)
if not USER:
    print("ERROR: POSTGRES_USER not defined by the user.")
    sys.exit(1)
if not POSTGRES_PASSWORD:
    print("ERROR: Please add your PostgreSQL password to 3rd-party credentials in the scheduler-portal under the key :\"" + POSTGRES_URL_KEY + "\"")
    sys.exit(1)
if not SQL_QUERY:
    print("ERROR: POSTGRES_QUERY not defined by the user.")
    sys.exit(1)

IS_LABELED_DATA = 'False'
LABEL = variables.get("LABEL")
if LABEL:
    IS_LABELED_DATA='True'

# Please refer to SQLAlchemy doc for more info about database urls.
# http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls
# Never print this to avoid displaying your credentials in the logs
print("BEGIN Import_Data from " + RDBMS_NAME + " database using " + variables.get("RDBMS_DRIVER") + " connector")
print("EXECUTING QUERY...")
print('POSTGRES_HOSTNAME='+HOSTNAME)
print('POSTGRES_PORT=', PORT)
print('POSTGRES_USER='+USER)
print('POSTGRES_DATABASE='+DATABASE)
print('POSTGRES_QUERY='+SQL_QUERY)
if OUTPUT_FILE:
    print('OUTPUT_FILE='+OUTPUT_FILE)

database_url = '{0}+{1}://{2}:{3}@{4}:{5}/{6}'.format(RDBMS_NAME,RDBMS_DRIVER,USER,POSTGRES_PASSWORD,HOSTNAME,PORT,DATABASE)
engine = create_engine(database_url)

with engine.connect() as conn, conn.begin():
    #pd.read_sql() can take either a SQL query as a parameter or a table name
    dataframe = pd.read_sql(SQL_QUERY, conn)

columns_name = dataframe.columns
columns_number = len(columns_name)

if IS_LABELED_DATA == 'True':
    label_index= dataframe.columns.get_loc(LABEL)
    data_indices=[x for i,x in enumerate(range(columns_number)) if i!=label_index]
    data  = dataframe.values[:,data_indices]
    label = dataframe.values[:,label_index]
    data_df = pd.DataFrame(data=data,columns=columns_name[data_indices])
    label_df = pd.DataFrame(data=label,columns=[columns_name[label_index]])
    LABEL_TRAIN_DF_JSON = label_df.to_json(orient='split')
    LABEL_TEST_DF_JSON = label_df.to_json(orient='split')
elif IS_LABELED_DATA == 'False':
    data = dataframe.values
    data_df = pd.DataFrame(data=data,columns=columns_name)

DATAFRAME_JSON = dataframe.to_json(orient='split')
COLUMNS_NAME_JSON = pd.Series(columns_name).to_json()
DATA_TRAIN_DF_JSON = data_df.to_json(orient='split')
DATA_TEST_DF_JSON = data_df.to_json(orient='split')

if IS_LABELED_DATA == 'True':
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
    dataframe=data_df.join(label_df)

variables.put("DATAFRAME_JSON", DATAFRAME_JSON)
variables.put("COLUMNS_NAME_JSON", COLUMNS_NAME_JSON)
variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
variables.put("IS_LABELED_DATA", IS_LABELED_DATA)

# Write results to the task result in CSV format
result = dataframe.to_csv(index=False).encode('utf-8')
resultMetadata.put("file.extension", ".csv")
resultMetadata.put("file.name", "result.csv")
resultMetadata.put("content.type", "text/csv")

# If an OUTPUT_FILE path in the dataspace is designated, then write to this file.
if OUTPUT_FILE:
    dataframe.to_csv(path_or_buf=OUTPUT_FILE, index=False)

print("END Import_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
    </task>
    <task name="Export_to_PostgreSQL">
      <description>
        <![CDATA[ This task allows to export data to PostgreSQL database.
It requires the following third-party credentials: POSTGRES_USERNAME and POSTGRES_PASSWORD. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables: 
$POSTGRES_TABLE (required) is the table name.
$INSERT_MODE (required) indicates the behavior to follow when the table exists in the database amongst: 
. fail: If table exists, do nothing.
. replace: If table exists, drop it, recreate it, and insert data.
. append: (default) If table exists, insert data. Create if does not exist.
$INPUT_FILE (required) is the relative path in the data space of the CSV file that contains data to be imported. The string could also be a URL. Valid URL schemes include http, ftp, s3, and file. 
This task uses also the task variable RMDB_DRIVER as a driver to connect to the database. The specified default driver "psycopg2" is already provided for this task. To use another driver, make sure you have it properly installed before. ]]>
      </description>
      <variables>
        <variable name="POSTGRES_TABLE" value="diabetes" inherited="false" />
        <variable name="RDBMS_DRIVER" value="psycopg2" inherited="false" />
        <variable name="INSERT_MODE" value="append" inherited="false" model="PA:LIST(fail, replace, append)"/>
        <variable name="INPUT_FILE" value="pima-indians-diabetes.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/postgresql.png"/>
        <info name="task.documentation" value="http://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Parse_Endpoint"/>
      </depends>
      <inputFiles>
        <files  includes="$INPUT_FILE" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment javaHome="/usr" ></forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import sys

RDBMS_NAME = 'postgresql'
RDBMS_DRIVER = variables.get("RDBMS_DRIVER")
POSTGRES_URL_KEY = "postgres://<username>@<hostname>:<port>"

HOSTNAME = variables.get("POSTGRES_HOSTNAME")
PORT = int(variables.get("POSTGRES_PORT"))
DATABASE = variables.get("POSTGRES_DATABASE")
USER = variables.get("POSTGRES_USER")
# This key is used for getting the password from 3rd party credentials.
POSTGRES_PASSWORD_KEY = "postgres://" + USER + "@" + HOSTNAME + ":" + str(PORT)
POSTGRES_PASSWORD=credentials.get(POSTGRES_PASSWORD_KEY)
INPUT_FILE = variables.get("INPUT_FILE")
SQL_TABLE = variables.get("POSTGRES_TABLE")
INSERT_MODE = variables.get("INSERT_MODE")

if not HOSTNAME:
    print("ERROR: POSTGRES_HOSTNAME not defined by the user.")
    sys.exit(1)
if not PORT:
    PORT = 5432
    print("POSTGRES_PORT not defined by the user. Using the default value:", PORT)
if not DATABASE:
    print("ERROR: POSTGRES_DATABASE not defined by the user.")
    sys.exit(1)
if not USER:
    print("ERROR: POSTGRES_USER not defined by the user.")
    sys.exit(1)
if not POSTGRES_PASSWORD:
    print("ERROR: Please add your PostgreSQL password to 3rd-party credentials in the scheduler-portal under the key :\"" + POSTGRES_URL_KEY + "\"")
    sys.exit(1)
if not INPUT_FILE:
    print("ERROR: INPUT_FILE not defined by the user.")
    sys.exit(1)
if not SQL_TABLE:
    print("ERROR: POSTGRES_TABLE not defined by the user.")
    sys.exit(1)
if not INSERT_MODE:
    INSERT_MODE = 'append'

# Please refer to SQLAlchemy doc for more info about database urls.
# http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls
# Never print this to avoid displaying your credentials in the logs

print("BEGIN Export_Data to " + RDBMS_NAME + " database using " + variables.get("RDBMS_DRIVER") + " connector")
print("INSERTING DATA IN POSTGRESQL...")
print('POSTGRES_HOSTNAME='+HOSTNAME)
print('POSTGRES_PORT=', PORT)
print('POSTGRES_USER='+USER)
print('POSTGRES_DATABASE='+DATABASE)
print('POSTGRES_TABLE='+SQL_TABLE)
database_url = '{0}+{1}://{2}:{3}@{4}:{5}/{6}'.format(RDBMS_NAME,RDBMS_DRIVER,USER,POSTGRES_PASSWORD,HOSTNAME,PORT,DATABASE)
engine = create_engine(database_url)
dataframe = pd.read_csv(INPUT_FILE, sep='\s+|;|,',index_col=None, engine='python')
with engine.connect() as conn, conn.begin():
     dataframe.to_sql(SQL_TABLE, conn, flavor=None, schema=None, if_exists=INSERT_MODE, index=True, index_label=None, chunksize=None, dtype=None)
                        
print("END Export_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>