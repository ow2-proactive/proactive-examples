<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="MySQL_Database_Interaction" projectName="MySQL Workflows"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="ACTION" value="Finish_MySQL" model="PA:LIST(Pause_MySQL, Resume_MySQL, Finish_MySQL)"/>
    <variable name="MYSQL_DATABASE" value="activeeon" />
    <variable name="MYSQL_INSTANCE_NAME" value="mysql-server" model="PA:SPEL( (variables[MYSQL_INSTANCE_NAME] = &quot;mysql-server-&quot;+(T(java.lang.Math).random()*1000).intValue()) instanceof T(String) )"/>
    <variable name="MYSQL_PASSWORD" value="proactive" />
    <variable name="MYSQL_USER" value="mlos" />
  </variables>
  <description>
    <![CDATA[ This workflow shows how to use PCA to ease deployment of service dependencies and interact with a MySQL database. It is a complete example putting together the use of
    1) a PCA service to create a MySQL database and,
    2) a mysql connector from data-connectors bucket to interact with this database along with its tow modes: Import and Export. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="database-services"/>
    <info name="Documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
    <info name="group" value="public-objects"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
  </genericInformation>
  <taskFlow>
    <task name="Import_from_MySQL">
      <description>
        <![CDATA[ This task allows to import data from MySQL database.
It requires the following third-party credentials: MYSQL_USERNAME and MYSQL_PASSWORD. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables: 
$LABEL (optional) used when the imported data is labeled. Then, the user can specify the label column name.
$MYSQL_QUERY (required) is the user's sql query.
$OUTPUT_FILE (optional) is a relative path in the data space used to save the results in a CSV file. 
This task uses also the task variable RMDB_DRIVER as a driver to connect to the database. The specified default driver "mysqlconnector" as well as "pymysql" are already provided for this task. To use another driver, make sure you have it properly installed before.
The imported data is exported in a JSON format using the variable $DATAFRAME_JSON. ]]>
      </description>
      <variables>
        <variable name="LABEL" value="" inherited="false" />
        <variable name="MYSQL_QUERY" value="SELECT * FROM diabetes" inherited="false" />
        <variable name="RDBMS_DRIVER" value="mysqlconnector" inherited="false" />
        <variable name="OUTPUT_FILE" value="output-mysql.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="http://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Export_to_MySQL"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3'
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import sys

RDBMS_NAME = 'mysql'
HOSTNAME = variables.get("MYSQL_HOSTNAME")
PORT = int(variables.get("MYSQL_PORT"))
DATABASE = variables.get("MYSQL_DATABASE")
MYSQL_USER = variables.get("MYSQL_USER")
RDBMS_DRIVER = variables.get("RDBMS_DRIVER")
MYSQL_URL_KEY = "mysql://<username>@<hostname>:<port>"
# This key is used for getting the password from 3rd party credentials.
MYSQL_PASSWORD_KEY = "mysql://" + MYSQL_USER + "@" + HOSTNAME + ":" + str(PORT)
MYSQL_PASSWORD=credentials.get(MYSQL_PASSWORD_KEY)
OUTPUT_FILE = variables.get("OUTPUT_FILE")
SQL_QUERY = variables.get("MYSQL_QUERY")

if not HOSTNAME:
    print("ERROR: The MYSQL_HOSTNAME variable is not provided by the user.")
    sys.exit(1)
if not PORT:
    print("ERROR: The MYSQL_PORT variable is not provided by the user. Using the default value:", PORT)
    PORT = 3360
if not DATABASE:
    print("ERROR: The MYSQL_DATABASE variable is not provided by the user.")
    sys.exit(1)
if not MYSQL_USER:
    print("ERROR: The MYSQL_USER is not provided by the user.")
    sys.exit(1)
if not MYSQL_PASSWORD:
    print("ERROR: Please add your mysql password to 3rd-party credentials in the scheduler-portal under the key :\"" + MYSQL_URL_KEY + "\"")
    sys.exit(1)
if not SQL_QUERY:
    print("ERROR: The MYSQL_QUERY is not provided by the user.")
    sys.exit(1)
    
IS_LABELED_DATA = 'False'

try:
    LABEL = variables.get("LABEL")
    if LABEL:
        IS_LABELED_DATA='True'
except NameError:
    pass

# Please refer to SQLAlchemy doc for more info about database urls.
# http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls
# Never print this to avoid displaying your credentials in the logs
print("BEGIN Import_Data from " + RDBMS_NAME + " database using " + variables.get("RDBMS_DRIVER") + " connector")
print("EXECUTING QUERY...")
print('MYSQL_HOSTNAME=' + HOSTNAME)
print('MYSQL_USER=' + MYSQL_USER)
print('MYSQL_PORT=', PORT)
print('MYSQL_DATABASE=' + DATABASE)
print('MYSQL_QUERY=' + SQL_QUERY)
if OUTPUT_FILE:
    print('OUTPUT_FILE=' + OUTPUT_FILE)

database_url = '{0}+{1}://{2}:{3}@{4}:{5}/{6}'.format(RDBMS_NAME,RDBMS_DRIVER,MYSQL_USER,MYSQL_PASSWORD,HOSTNAME,PORT,DATABASE)
engine = create_engine(database_url)

with engine.connect() as conn, conn.begin():
    #pd.read_sql() can take either a SQL query as a parameter or a table name
    dataframe = pd.read_sql(SQL_QUERY, conn)

columns_name = dataframe.columns
columns_number = len(columns_name)

if IS_LABELED_DATA == 'True':
  label_index= dataframe.columns.get_loc(LABEL)
  data_indices=[x for i,x in enumerate(range(columns_number)) if i!=label_index]
  data  = dataframe.values[:,data_indices]
  label = dataframe.values[:,label_index]
  data_df = pd.DataFrame(data=data,columns=columns_name[data_indices])
  label_df = pd.DataFrame(data=label,columns=[columns_name[label_index]])
  LABEL_TRAIN_DF_JSON = label_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_df.to_json(orient='split')
  
elif IS_LABELED_DATA == 'False':
  data = dataframe.values
  data_df = pd.DataFrame(data=data,columns=columns_name)
  
DATAFRAME_JSON = dataframe.to_json(orient='split')
COLUMNS_NAME_JSON = pd.Series(columns_name).to_json()
DATA_TRAIN_DF_JSON = data_df.to_json(orient='split')
DATA_TEST_DF_JSON = data_df.to_json(orient='split')

if IS_LABELED_DATA == 'True':
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
    dataframe=data_df.join(label_df)
    
variables.put("DATAFRAME_JSON", DATAFRAME_JSON)
variables.put("COLUMNS_NAME_JSON", COLUMNS_NAME_JSON)
variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
variables.put("IS_LABELED_DATA", IS_LABELED_DATA)

# Write results to the task result in CSV format
result = dataframe.to_csv(index=False).encode('utf-8')
resultMetadata.put("file.extension", ".csv")
resultMetadata.put("file.name", "result.csv")
resultMetadata.put("content.type", "text/csv")

# If an OUTPUT_FILE path in the dataspace is designated, then write to this file.
if OUTPUT_FILE:
    dataframe.to_csv(path_or_buf=OUTPUT_FILE, index=False)

print("END Import_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
    </task>
    <task name="Export_to_MySQL">
      <description>
        <![CDATA[ This task allows to export data to MySQL database.
It requires the following third-party credentials: MYSQL_USERNAME and MYSQL_PASSWORD. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables: 
$MYSQL_TABLE (required) is the table name.
$INSERT_MODE (required) indicates the behavior to follow when the table exists in the database amongst: 
. fail: If table exists, do nothing.
. replace: If table exists, drop it, recreate it, and insert data.
. append: (default) If table exists, insert data. Create if does not exist.
$INPUT_FILE (required) is the relative path in the data space of the CSV file that contains data to be imported. The string could also be a URL. Valid URL schemes include http, ftp, s3, and file. 
This task uses also the task variable RMDB_DRIVER as a driver to connect to the database. The specified default driver "mysqlconnector" as well as "pymysql" are already provided for this task. To use another driver, make sure you have it properly installed before. ]]>
      </description>
      <variables>
        <variable name="MYSQL_TABLE" value="diabetes" inherited="false" />
        <variable name="RDBMS_DRIVER" value="mysqlconnector" inherited="false" />
        <variable name="INSERT_MODE" value="append" inherited="false" model="PA:LIST(fail, replace, append)"/>
        <variable name="INPUT_FILE" value="pima-indians-diabetes.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="http://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Parse_Endpoint"/>
      </depends>
      <inputFiles>
        <files  includes="$INPUT_FILE" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3'
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import sys

RDBMS_NAME = 'mysql'
HOSTNAME = variables.get("MYSQL_HOSTNAME")
PORT = int(variables.get("MYSQL_PORT"))
DATABASE = variables.get("MYSQL_DATABASE")
MYSQL_USER = variables.get("MYSQL_USER")
RDBMS_DRIVER = variables.get("RDBMS_DRIVER")
MYSQL_URL_KEY = "mysql://<username>@<hostname>:<port>"
# This key is used for getting the password from 3rd party credentials.
MYSQL_PASSWORD_KEY = "mysql://" + MYSQL_USER + "@" + HOSTNAME + ":" + str(PORT)
MYSQL_PASSWORD=credentials.get(MYSQL_PASSWORD_KEY)
INPUT_FILE = variables.get("INPUT_FILE")
SQL_TABLE = variables.get("MYSQL_TABLE")
INSERT_MODE = variables.get("INSERT_MODE")

if not HOSTNAME:
    print("ERROR: The MYSQL_HOSTNAME variable is not provided by the user.")
    sys.exit(1)
if not PORT:
    print("ERROR: The MYSQL_PORT variable is not provided by the user. Using the default value:", PORT)
    PORT = 3360
if not DATABASE:
    print("ERROR: The MYSQL_DATABASE variable is not provided by the user.")
    sys.exit(1)
if not MYSQL_USER:
    print("ERROR: The MYSQL_USER is not provided by the user.")
    sys.exit(1)
if not MYSQL_PASSWORD:
    print("ERROR: Please add your mysql password to 3rd-party credentials in the scheduler-portal under the key :\"" + MYSQL_URL_KEY + "\"")
    sys.exit(1)
if not INPUT_FILE:
    print("ERROR: The INPUT_FILE is not provided by the user.")
    sys.exit(1)
if not SQL_TABLE:
    print("ERROR: The MYSQL_TABLE is not provided by the user.")
    sys.exit(1)
if not INSERT_MODE:
    INSERT_MODE = 'append'


# Please refer to SQLAlchemy doc for more info about database urls.
# http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls
# Never print this to avoid displaying your credentials in the logs

print("BEGIN Export_Data to " + RDBMS_NAME + " database using " + variables.get("RDBMS_DRIVER") + " connector")
print("INSERTING DATA IN MYSQL...")
print('MYSQL_HOSTNAME='+HOSTNAME)
print('MYSQL_PORT=', PORT)
print('MYSQL_USER=', MYSQL_USER)
print('MYSQL_DATABASE='+DATABASE)
print('MYSQL_TABLE='+SQL_TABLE)
database_url = '{0}+{1}://{2}:{3}@{4}:{5}/{6}'.format(RDBMS_NAME,RDBMS_DRIVER,MYSQL_USER,MYSQL_PASSWORD,HOSTNAME,PORT,DATABASE)
engine = create_engine(database_url)
dataframe = pd.read_csv(INPUT_FILE, sep='\s+|;|,',index_col=None, engine='python')
with engine.connect() as conn, conn.begin():
     dataframe.to_sql(SQL_TABLE, conn, schema=None, if_exists=INSERT_MODE, index=True, index_label=None, chunksize=None, dtype=None)
            
print("END Export_Data")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Parse_Endpoint">
      <description>
        <![CDATA[ This task aims to parse PCA endpoint in order to retrieve a HOSTNAME and a PORT number to use them as an input in the data connector tasks. ]]>
      </description>
      <depends>
        <task ref="Start_MySQL"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
def mysqlEndpoint = variables.get("ENDPOINT")
def mysqlHost
def mysqlPort

if (mysqlEndpoint != null){
  mysqlEndpoint = mysqlEndpoint.replace("http://", "")
  mysqlHost = mysqlEndpoint.split(":")[0]
  variables.put("MYSQL_HOSTNAME", mysqlHost)
  mysqlPort = mysqlEndpoint.split(":")[1]
  variables.put("MYSQL_PORT", mysqlPort)
  sleep(3000)
}
else{
  throw new IOException("[ERROR] MySQL endpoint not found")
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Start_MySQL"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ Start the MySQL server as a service. ]]>
      </description>
      <variables>
        <variable name="SERVICE_ID" value="MySQL" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/PCA/PCAUserGuide.html"/>
      </genericInformation>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <pre>
        <script>
          <code language="groovy">
            <![CDATA[
/*********************************************************************************
* THIS PRESCRIPT MAPS THE SERVICE SPECIFIC VARIABLES WITH GENERIC VARIABLE NAMES *
* THIS AVOIDS YOU FROM CHANGING THE WHOLE PCA WORKFLOWS IMPLEMENTATIONS          *
*********************************************************************************/

variables.put("INSTANCE_NAME", variables.get("MYSQL_INSTANCE_NAME"))
variables.put("USER", variables.get("MYSQL_USER"))
variables.put("PASSWORD", variables.get("MYSQL_PASSWORD"))
variables.put("DATABASE", variables.get("MYSQL_DATABASE"))
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription

println("BEGIN " + variables.get("PA_TASK_NAME"))

/*********************************************************************************
* THIS PART IS IMAGE SPECIFIC. IF YOU NEED TO MODIFY SOMETHING, DO IT HERE       *
/********************************************************************************/
def user = variables.get("USER")
def password = variables.get("PASSWORD")
def database = variables.get("DATABASE")
/********************************************************************************/

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def sessionId = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pcaUrl = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def apiClient = new ApiClient()
apiClient.setBasePath(pcaUrl)
//apiClient.setDebugging(true)
def serviceInstanceRestApi = new ServiceInstanceRestApi(apiClient)

def serviceId = variables.get("SERVICE_ID")
def instanceName = variables.get("INSTANCE_NAME")
println("SERVICE_ID:    " + serviceId)
println("INSTANCE_NAME: " + instanceName)

// Check existing service instances
boolean instance_exists = false
List<ServiceInstanceData> service_instances = serviceInstanceRestApi.getServiceInstancesUsingGET()

for (ServiceInstanceData serviceInstanceData : service_instances) {
    if ( (serviceInstanceData.getServiceId() == serviceId) && (serviceInstanceData.getInstanceStatus()  == "RUNNING")){
        if (serviceInstanceData.getVariables().get("INSTANCE_NAME") == instanceName) {
            instance_exists = true
            def instanceId = serviceInstanceData.getInstanceId()
            endpoint = serviceInstanceData.getInstanceEndpoints().entrySet().iterator().next().getValue()
            println("INSTANCE_ID: " + instanceId)
            println("ENDPOINT:    " + endpoint)
            variables.put("INSTANCE_ID", instanceId)
            variables.put("ENDPOINT", endpoint)
            break
        }
  	}
}

println("INSTANCE_EXISTS ? " + instance_exists)

if (!instance_exists){
    // Prepare service description
    ServiceDescription serviceDescription = new ServiceDescription()
    serviceDescription.setBucketName("cloud-automation")
    serviceDescription.setWorkflowName(serviceId)
    serviceDescription.putVariablesItem("INSTANCE_NAME", instanceName)

/*********************************************************************************
* THIS PART IS IMAGE SPECIFIC. IF YOU NEED TO MODIFY SOMETHING, DO IT HERE       *
/********************************************************************************/
    serviceDescription.putVariablesItem("USER", user)
    serviceDescription.putVariablesItem("PASSWORD", password)
     serviceDescription.putVariablesItem("DATABASE", database)
/********************************************************************************/
    // Run service
    def serviceInstanceData = serviceInstanceRestApi.createRunningServiceInstanceUsingPOST(sessionId, serviceDescription)

    // Acquire service Instance ID
    def serviceInstanceId = serviceInstanceData.getInstanceId()

    // Create synchro channel
    def channel = "Service_Instance_" + serviceInstanceId
    println("CHANNEL: " + channel)
    synchronizationapi.createChannelIfAbsent(channel, false)
    synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")

    // Acquire service endpoint
    serviceInstanceData = serviceInstanceRestApi.getServiceInstanceUsingGET(serviceInstanceId)
    def instanceId = serviceInstanceData.getInstanceId()
    endpoint = serviceInstanceData.getInstanceEndpoints().entrySet().iterator().next().getValue()

    println("INSTANCE_ID: " + instanceId)
    println("ENDPOINT: " + endpoint)

    variables.put("INSTANCE_ID", instanceId)
    variables.put("ENDPOINT", endpoint)
}

println("END " + variables.get("PA_TASK_NAME"))
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="MySQL_Service_Action"
    
    
    onTaskError="cancelJob" >
      <variables>
        <variable name="INSTANCE_ID" value="" inherited="true" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/PCA/PCAUserGuide.html"/>
      </genericInformation>
      <depends>
        <task ref="Import_from_MySQL"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN " + variables.get("PA_TASK_NAME") + " ---")

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def sessionId = schedulerapi.getSession()

// Define PCA URL
def schedulerRestUrl = variables.get("PA_SCHEDULER_REST_URL")
def pcaUrl = schedulerRestUrl.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def apiClient = new ApiClient()
apiClient.setBasePath(pcaUrl)
//apiClient.setDebugging(true)
def serviceInstanceRestApi = new ServiceInstanceRestApi(apiClient)

def instanceId = variables.get("INSTANCE_ID") as int
println("INSTANCE_ID: " + instanceId)
assert instanceId != null

// Execute action on service
ServiceDescription service = new ServiceDescription()
service.setBucketName("cloud-automation")
service.setWorkflowName(variables.get("ACTION"))
serviceInstanceRestApi.launchServiceInstanceActionUsingPUT(sessionId, instanceId, service)

println("--- END " + variables.get("PA_TASK_NAME") + " ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>