<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="Elasticsearch_Database_Interaction" projectName="1. Elasticsearch Workflows"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="ELASTICSEARCH_INSTANCE_NAME" value="elasticsearch-server-1" />
  </variables>
  <description>
    <![CDATA[ This workflow shows how to use PCA to ease deployment of service dependencies and interact with a MongoDB database. It is a complete example putting together the use of 
1) a PCA service to create an elasticsearch database and,
2)  an elasticsearch connector from data-connectors bucket to interact with this database along with its tow modes: Import and Export. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="elastic-logstash-kibana"/>
    <info name="Documentation" value="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html"/>
    <info name="group" value="public-objects"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/elasticsearch.png"/>
  </genericInformation>
  <taskFlow>
    <task name="Start_Elasticsearch_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ Start the Elasticsearch server as a service. ]]>
      </description>
      <variables>
        <variable name="ELASTICSEARCH_SERVICE_ID" value="Elasticsearch" inherited="false" />
        <variable name="ELASTICSEARCH_INSTANCE_NAME" value="elasticsearch-server-1" inherited="true" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/elasticsearch.png"/>
        <info name="task.documentation" value="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html"/>
      </genericInformation>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Start_Elasticsearch_Service ---")

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Check existing service instances
def service_id = variables.get("ELASTICSEARCH_SERVICE_ID")
def instance_name = variables.get("ELASTICSEARCH_INSTANCE_NAME")
println("*_service_id:    " + service_id)
println("*_instance_name: " + instance_name)

boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
	if ( (service_instance_data.getServiceId() == service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      if (service_instance_data.getVariables().get("INSTANCE_NAME") == instance_name) {
        instance_exists = true
        instance_id = service_instance_data.getInstanceId()
  		endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
        println("*_instance_id: " + instance_id)
        println("*_endpoint:    " + endpoint)
        variables.put("elasticsearch_instance_id", instance_id)
        variables.put("elasticsearch_endpoint", endpoint)
        break
      }
  	}
}

println("instance_exists: " + instance_exists)

if (!instance_exists){
  // Prepare service description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(service_id) 
  serviceDescription.putVariablesItem("ELASTICSEARCH_INSTANCE_NAME_CL", instance_name)
  
  // Run service
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire service Instance ID
  def service_instance_id = service_instance_data.getInstanceId()
  println("service_instance_id: " + service_instance_id)
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id
  println("channel: " + channel)
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire service endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  instance_name = service_instance_data.getVariables().get("INSTANCE_NAME")
  instance_id = service_instance_data.getInstanceId()
  endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
  
  println("*_instance_name: " + instance_name)
  println("*_instance_id: " + instance_id)
  println("*_endpoint: " + endpoint)
  
  variables.put("elasticsearch_instance_id", instance_id)
  variables.put("elasticsearch_endpoint", endpoint)
  
  result = '<meta http-equiv="refresh" content="1; url=' + variables.get("elasticsearch_endpoint") + '" />'
  result+= '<h2><span style="color:black">Please wait while redirecting...</span></h2>'
  resultMetadata.put("content.type", "text/html")
}

println("--- END Start_Elasticsearch_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Parse_Endpoint">
      <description>
        <![CDATA[ This task parse the service endpoint to retrieve the hostname and the port. ]]>
      </description>
      <depends>
        <task ref="Start_Elasticsearch_Service"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
def elasticsearch_endpoint = variables.get("elasticsearch_endpoint")
def ELASTICSEARCH_HOST
def ELASTICSEARCH_PORT

if (elasticsearch_endpoint != null){
  elasticsearch_endpoint = elasticsearch_endpoint.replace("http://", "")
  ELASTICSEARCH_HOST = elasticsearch_endpoint.split(":")[0]
  variables.put("ELASTICSEARCH_HOSTNAME", ELASTICSEARCH_HOST)
  ELASTICSEARCH_PORT = elasticsearch_endpoint.split(":")[1]
  variables.put("ELASTICSEARCH_PORT", ELASTICSEARCH_PORT)
}
else{
  throw new IOException("Elasticsearch endpoint not found")
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Finish_Elasticsearch_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ Finish the Elasticsearch service. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/elasticsearch.png"/>
        <info name="task.documentation" value="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html"/>
      </genericInformation>
      <depends>
        <task ref="Import_from_ElasticSearch"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Finish_Elasticsearch_Service ---")

import org.ow2.proactive.pca.service.client.ApiClient
import org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
import org.ow2.proactive.pca.service.client.model.ServiceInstanceData
import org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

instance_id = variables.get("elasticsearch_instance_id")
println("*_instance_id: " + instance_id)
assert instance_id != null

// Finish service
ServiceDescription service = new ServiceDescription()
service.setBucketName("cloud-automation") 
service.setWorkflowName("Finish_Elasticsearch")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, instance_id, service)

println("--- END Finish_Elasticsearch_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Import_from_ElasticSearch">
      <description>
        <![CDATA[ Load data from Elasticsearch.
If your server requires authentification, then use the $ELASTICSEARCH_USER variable then add the corresponding password to 3rd party credentials under the key: elasticsearch://<ELASTICSEARCH_USER>@<ELASTICSEARCH_HOSTNAME>:<ELASTICSEARCH_PORT>
It also requires a $QUERY and an $INDEX to fetch data. By default, it will fetch all documents from the specified index. $ELASTICSEARCH_SIZE is the maximum amount of hits to be returned.
The imported data is exported in a JSON format. ]]>
      </description>
      <variables>
        <variable name="ELASTICSEARCH_QUERY" value="{&quot;query&quot;: { &quot;match&quot;: {&quot;mass&quot;: &quot;77&quot;} }}" inherited="false" />
        <variable name="ELASTICSEARCH_INDEX" value="star_wars" inherited="false" />
        <variable name="ELASTICSEARCH_SIZE" value="10" inherited="false" model="PA:Integer"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/elasticsearch.svg"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_nosql"/>
      </genericInformation>
      <depends>
        <task ref="Export_to_ElasticSearch"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
# In the Java Home location field, use the value: "/usr" to force using the JRE provided in the docker image below (Recommended).
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters
containerName = 'activeeon/dlm3'
dockerRunCommand =  'docker run '
dockerParameters = '--rm '
# Prepare ProActive home volume
paHomeHost = variables.get("PA_SCHEDULER_HOME")
paHomeContainer = variables.get("PA_SCHEDULER_HOME")
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' '
# Prepare working directory (For Dataspaces and serialized task file)
workspaceHost = localspace
workspaceContainer = localspace
workspaceVolume = '-v '+localspace +':'+localspace+' '
# Prepare container working directory
containerWorkingDirectory = '-w '+workspaceContainer+' '
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import pandas as pd
import json
from elasticsearch import Elasticsearch

print("BEGIN Import Data from Elasticsearch")

HOSTNAME = variables.get("ELASTICSEARCH_HOSTNAME")
PORT = int(variables.get("ELASTICSEARCH_PORT"))
INDEX = variables.get("ELASTICSEARCH_INDEX")
SEARCH_SIZE = int(variables.get("ELASTICSEARCH_SIZE"))
USER = variables.get("ELASTICSEARCH_USER")
PASSWORD = None

if not HOSTNAME:
    print("[ERROR] ELASTICSEARCH_HOSTNAME not defined by the user.")
    sys.exit(1)
if not PORT:
    PORT = 9200
    print("[WARNING] ELASTICSEARCH_PORT not defined by the user. Using the default value: " + PORT)
if not INDEX:
    INDEX = ""
    print("[WARNING] ELASTICSEARCH_INDEX not defined by the user. Searching in all indices.")
if not SEARCH_SIZE:
    SEARCH_SIZE = 10
if USER:
    # This key is used for getting the password from 3rd party credentials.
    ELASTICSEARCH_PASSWORD_KEY = "elasticsearch://" + USER + "@" + HOSTNAME + ":" + str(PORT)
    PASSWORD = credentials.get(ELASTICSEARCH_PASSWORD_KEY)
    if not PASSWORD:
        print("[ERROR] The ELASTICSEARCH_USER is used in junction with a password. Please add the corresponding password to 3rd-party credentials in the scheduler-portal under the key :\"" + ELASTICSEARCH_URL_KEY + "\"")
        sys.exit(1)
if variables.get("ELASTICSEARCH_QUERY"):
    # This is a workaround to force the variable string value into json
    exec("QUERY=json.loads(variables.get('ELASTICSEARCH_QUERY'))")
else:
    #print("ELASTICSEARCH_QUERY not defined by the user.")
    #sys.exit(1)
    print("[WARNING] ELASTICSEARCH_QUERY not defined by the user. Fetching all documents.")
    QUERY = { "query": { "match_all": {} } }

def connect(host, port, username, password):
    """ A util for making a connection to elasticsearch """

    # Connect to cluster over HTTP:
    es_header = [{
        'host': host,
        'port': port}]
    # If a username is provided, then connect through SSL
    if username:
        es_header[0]['use_ssl'] = True
        es_header[0]['http_auth'] = (username,password)
    # Instantiate the new Elasticsearch connection:
    es = Elasticsearch(es_header)
    return es

def read_elasticsearch(index, query, search_size, host, port, username, password):

    # Connect to elasticsearch
    es = connect(host=host, port=port, username=username, password=password)


    # Make a query to the specific DB and Collection
    res = es.search(index=index, size=search_size, body=query)
    print("%d documents found" % res['hits']['total'])

    return res

print("EXECUTING QUERY...")
print('ELASTICSEARCH_HOSTNAME=%s', HOSTNAME)
print('ELASTICSEARCH_PORT=%s', PORT)
print('ELASTICSEARCH_USER=%s', USER)
print('ELASTICSEARCH_INDEX=%s', INDEX)
print('ELASTICSEARCH_QUERY=%s', json.dumps(QUERY))
print('ELASTICSEARCH_SIZE=%s', SEARCH_SIZE)

query_results= read_elasticsearch(INDEX, QUERY, SEARCH_SIZE, HOSTNAME,  PORT,  USER,  PASSWORD)

#**************Preview Data*********************
result = json.dumps(query_results).encode('utf-8')
resultMetadata.put("file.extension", ".json")
resultMetadata.put("file.name", "result.json")
resultMetadata.put("content.type", "application/json")
#***********************************************
print("END Import Data from Elasticsearch")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Export_to_ElasticSearch">
      <description>
        <![CDATA[ This task allows exporting data to ElasticSearch.
It uses the following variables:
$ELASTICSEARCH_USER (optional) If your server requires authentification, then please add the corresponding password to 3rd party credentials under the key: elasticsearch://<ELASTICSEARCH_USER>@<ELASTICSEARCH_HOSTNAME>:<ELASTICSEARCH_PORT>
$ELASTICSEARCH_INDEX (required) the index to use. It is created if it does not exist
$ELASTICSEARCH_DOC_TYPE (required) the documents type.
$ELASTICSEARCH_INPUT (required) A JSON Object/Array to be indexed in ElasticSearch. This variable can:
 - A String describing the JSON Object/Array
 - A relative path in the data space of a JSON file. ]]>
      </description>
      <variables>
        <variable name="ELASTICSEARCH_INDEX" value="star_wars" inherited="false" />
        <variable name="ELASTICSEARCH_DOC_TYPE" value="people" inherited="false" />
        <variable name="ELASTICSEARCH_INPUT" value="star_wars_people.json" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/elasticsearch.svg"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/user/ProActiveUserGuide.html#_nosql"/>
      </genericInformation>
      <depends>
        <task ref="Parse_Endpoint"/>
      </depends>
      <inputFiles>
        <files  includes="$ELASTICSEARCH_INPUT" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
# In the Java Home location field, use the value: "/usr" to force using the JRE provided in the docker image below (Recommended).
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters
containerName = 'activeeon/dlm3'
dockerRunCommand =  'docker run '
dockerParameters = '--rm '
# Prepare ProActive home volume
paHomeHost = variables.get("PA_SCHEDULER_HOME")
paHomeContainer = variables.get("PA_SCHEDULER_HOME")
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' '
# Prepare working directory (For Dataspaces and serialized task file)
workspaceHost = localspace
workspaceContainer = localspace
workspaceVolume = '-v '+localspace +':'+localspace+' '
# Prepare container working directory
containerWorkingDirectory = '-w '+workspaceContainer+' '
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from bson.json_util import dumps, loads

ELASTICSEARCH_URL_KEY = "elasticsearch://<username>@<hostname>:<port>"

def get_input_json(input):
    """
    A util for detecting whether the input string is a valid json or a file path/url
    """
    if input.startswith(("{", "[")):
        print("Exporting a JSON String")
        return loads(input)
    else:
        print("Exporting JSON File:{0}".format(input))
        json_data = open(input, "r").read()
        return loads(json_data)

def connect(host, port, username, password):
    """ A util for making a connection to elasticsearch """

    # Connect to cluster over HTTP:
    es_header = [{
        'host': host,
        'port': port}]
    # If a username is provided, then connect through SSL
    if username:
        es_header[0]['use_ssl'] = True
        es_header[0]['http_auth'] = (username,password)
    else:
    # Instantiate the new Elasticsearch connection:
    es = Elasticsearch(es_header)
    return es

def write_elasticsearch(index, inputs, doc_type, host, port, username, password):

    # Connect to elasticsearch
    es = connect(host=host, port=port, username=username, password=password)

    # Create an index if it does not exist
    if not es.indices.exists(index=index):
        es.indices.create(index=index,body={})

    if isinstance(inputs, list):
        res = bulk(es, inputs, index=index,doc_type=doc_type, raise_on_error=True)
        if res[0] == len(inputs):
            print("Successfully inserted [{0}/{1}] documents.".format(res[0], len(inputs)))
            return 'True'
        else:
            print("Failed to insert all documents. Only [{0}/{1}] were inserted".format(res[0], len(inputs)))
            return 'False'
    elif isinstance(inputs,dict):
        res = es.index(index=index, doc_type=doc_type, body=inputs)
        if res:
            print("Successfully inserted 1 documents")
            return 'True'
        else:
            print("Failed to insert document.")
            return 'False'

HOSTNAME = variables.get("ELASTICSEARCH_HOSTNAME")
PORT = int(variables.get("ELASTICSEARCH_PORT"))
INDEX = variables.get("ELASTICSEARCH_INDEX")
DOC_TYPE = variables.get("ELASTICSEARCH_DOC_TYPE")
INPUT = get_input_json(variables.get("ELASTICSEARCH_INPUT"))
USER = variables.get("ELASTICSEARCH_USER")
PASSWORD= None

if not HOSTNAME:
    print("[ERROR] ELASTICSEARCH_HOSTNAME not defined by the user.")
    sys.exit(1)
if not PORT:
    PORT = 9200
    print("[WARNING] ELASTICSEARCH_PORT not defined by the user. Using the default value: " + PORT)
if not INDEX:
    print("[ERROR] ELASTICSEARCH_INDEX not defined by the user.")
    sys.exit(1)
if not DOC_TYPE:
    print("[ERROR] ELASTICSEARCH_DOC_TYPE not defined by the user.")
    sys.exit(1)
if USER:
    # This key is used for getting the password from 3rd party credentials.
    ELASTICSEARCH_PASSWORD_KEY = "elasticsearch://" + USER + "@" + HOSTNAME + ":" + str(PORT)
    PASSWORD = credentials.get(ELASTICSEARCH_PASSWORD_KEY)
    if not PASSWORD:
        print("[ERROR] The ELASTICSEARCH_USER is used in junction with a password. Please add the corresponding password to 3rd-party credentials in the scheduler-portal under the key :\"" + ELASTICSEARCH_URL_KEY + "\"")
        sys.exit(1)

print('BEGIN Export Data to ElasticSearch')
print('ELASTICSEARCH_HOSTNAME=%s', HOSTNAME)
print('ELASTICSEARCH_PORT=%s', PORT)
print('ELASTICSEARCH_USER=%s', USER)
print('ELASTICSEARCH_INDEX=%s', INDEX)

# Insert INPUT in ElasticSearch
write_elasticsearch(INDEX, INPUT, DOC_TYPE, HOSTNAME,  PORT,  USER,  PASSWORD)
print("END Export Data")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
  </taskFlow>
</job>