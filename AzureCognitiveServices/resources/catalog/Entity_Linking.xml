<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="Entity Linking" projectName="Knowledge"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="TEXT" value="For months, the four scientific instruments at the heart of the James Webb Space Telescope have been sealed in what looks like a huge pressure cooker. It&#39;s a test chamber that simulates the grueling operating conditions they will face after Webb is launched into orbit in 2018. But in fact, &quot;pressure cooker&quot; is an apt metaphor for the whole project. The infrared Webb observatory is the biggest, most complex, and most expensive science mission that NASA has ever attempted. Like that of its predecessor, the Hubble Space Telescope, Webb&#39;s construction has been plagued by redesigns, schedule slips, and cost overruns that have strained relationships with contractors, international partners, and supporters in the U.S. Congress. Lately the project has largely stuck to its schedule and its $8 billion budget. But plenty could still go wrong, and the stakes are high: Both the future of space-based astronomy and NASA&#39;s ability to build complex science missions depend on its success." />
    <variable name="SELECTION" value="" />
    <variable name="OFFSET" value="-1" model="PA:Integer"/>
    <variable name="OUTPUT_FORMAT" value="HTML" model="PA:LIST(CSV, HTML, JSON)"/>
  </variables>
  <description>
    <![CDATA[ Welcome to the Microsoft Entity Linking Intelligence Service, a web service created to help developers with tasks relating to entity linking. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="azure-cognitive-services"/>
    <info name="pca.action.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_entity_linkingintelligence.svg"/>
    <info name="Documentation" value="https://docs.microsoft.com/en-us/azure/cognitive-services/entitylinking/home"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="EntityLinking">
      <description>
        <![CDATA[ This task wraps the Entity Linking API of Microsoft which is a natural language processing tool to analyze text and link named-entities to relevant entries in a knowledge base.
The task requires this third-party credential : $ENTITY_LINKING_API_KEY which provides access to this API. Please refer to the User documentation to learn how to add third-party credentials.
$TEXT (required) is the text input.
$SELECTION (optional) is the specific word or phrase within the text that is to be entity linked.
$OFFSET (optional) is the location (in offset by characters) of the selected word or phrase within the input text. 
The task's output $ENTITY_LINKING_OUTPUT is the result of the API call in a JSON format. ]]>
      </description>
      <variables>
        <variable name="TEXT" value="For months, the four scientific instruments at the heart of the James Webb Space Telescope have been sealed in what looks like a huge pressure cooker. It&#39;s a test chamber that simulates the grueling operating conditions they will face after Webb is launched into orbit in 2018. But in fact, &quot;pressure cooker&quot; is an apt metaphor for the whole project. The infrared Webb observatory is the biggest, most complex, and most expensive science mission that NASA has ever attempted. Like that of its predecessor, the Hubble Space Telescope, Webb&#39;s construction has been plagued by redesigns, schedule slips, and cost overruns that have strained relationships with contractors, international partners, and supporters in the U.S. Congress. Lately the project has largely stuck to its schedule and its $8 billion budget. But plenty could still go wrong, and the stakes are high: Both the future of space-based astronomy and NASA&#39;s ability to build complex science missions depend on its success." inherited="true" />
        <variable name="SELECTION" value="" inherited="true" />
        <variable name="OFFSET" value="-1" inherited="true" model="PA:Integer"/>
        <variable name="OUTPUT_FORMAT" value="HTML" inherited="true" model="PA:LIST(CSV, HTML, JSON)"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_entity_linkingintelligence.svg"/>
      </genericInformation>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import requests
import json
import urllib
from pprint import pprint
import pandas as pd


# You can customize the api server location
api_location="westus"

# READ TASK VARIABLES
if 'variables' in locals():
    if variables.get("TEXT") is not None:
        TEXT = variables.get("TEXT")
    if variables.get("SELECTION") is not None:
        SELECTION = variables.get("SELECTION")
    if variables.get("OFFSET") is not None:
        OFFSET = int(variables.get("OFFSET"))
    if variables.get("OUTPUT_FORMAT") is not None:
        OUTPUT_FORMAT = variables.get("OUTPUT_FORMAT")
    # Provide a valid subscription API token
    if credentials.get("ENTITY_LINKING_API_KEY") is not None:
        subscription_key = credentials.get("ENTITY_LINKING_API_KEY")
    else:
        print("You first need to add your Azure Cognitive Services API key (ENTITY_LINKING_API_KEY) to the third party credentials")
        sys.exit(1)

# Send API request
headers   = {
    "Ocp-Apim-Subscription-Key": subscription_key,
    'Content-Type': 'text/plain'
}

# Set API request parameters
params=None
# If SELECTION is not defined, skip
if SELECTION is not None and len(SELECTION)>0:
    params={'selection': SELECTION,'offset': OFFSET}
    #params = urllib.parse.urlencode({'selection': SELECTION,'offset': OFFSET})

print("******** Query params ********")
pprint(params)

# Congitive Services - Text Analytics API URL:
service_url = "https://{0}.api.cognitive.microsoft.com/entitylinking/v1.0/link".format(api_location)

if params is not None:
    response  = requests.post(service_url, headers=headers, data=TEXT.encode('utf-8'), params=params)
else:
    response  = requests.post(service_url, headers=headers, data=TEXT.encode('utf-8'))

# Get a JSON response
api_results = response.json()

# Print the results
#pprint(api_results)

if 'variables' in locals():
    variables.put('ENTITY_LINKING_OUTPUT', api_results)

print("BEGIN Export_Results")

OUTPUT_DATA = api_results["entities"]

# Convert the results into HTML
table = []
for document in OUTPUT_DATA:
    try: 
       score= document["score"]
    except KeyError:
        score = None        
        pass
    try: 
       wiki_id= document["wikipediaId"]
    except KeyError:
        wiki_id = None        
        pass
    try: 
       name= document["name"]
    except KeyError:
        name = None        
        pass         
    res = ", ".join(["{0}".format(match["entries"]) for match in document["matches"]])
    table.append("<tr><td>{0}</td><td>{1}</td><td>{2}</td><td>[{3}]</td>".format(name, wiki_id, score,res))
    
css_style="""table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%;
}
td {
  border: 1px solid #999999;
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  border-left: 2px solid #999999;
}"""
html = ("<table><tr><th>name</th><th>wikipediaId</th><th>score</th><th>matches</th></tr>{0}</table>").format("\n".join(table))
html_container="""                                               
            <!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <meta name="description" content="Azure Cognitive Services - EntityLinking API Results">
                    <style>{0}</style>
                  </head>
                  <body>{1}</body></html>""".format(css_style,html)


if OUTPUT_DATA != None and 'resultMetadata' in locals(): 
    #dataframe = pd.read_json(json.dumps(OUTPUT_DATA), orient='table', encoding='utf-8')
    dataframe=pd.read_html(html_container,header=0, encoding='utf-8')[0]
    
    if OUTPUT_FORMAT == 'JSON':
        result = json.dumps(api_results).encode('utf-8')
        resultMetadata.put("file.extension", ".json")
        resultMetadata.put("file.name", "result.json")
        resultMetadata.put("content.type", "application/json")
    elif OUTPUT_FORMAT == 'CSV':
        result = dataframe.to_csv(index=False)
        resultMetadata.put("file.extension", ".csv")
        resultMetadata.put("file.name", "result.csv")
        resultMetadata.put("content.type", "text/csv")
    elif OUTPUT_FORMAT == 'HTML':
        result = html_container.encode('utf-8')#dataframe.to_html(index=False).encode('utf-8')
        resultMetadata.put("file.extension", ".html")
        resultMetadata.put("file.name", "result.html")
        resultMetadata.put("content.type", "text/html")
        resultMetadata.put("charset", "UTF-8")
    print("END Export_Results")  
else:
    print('It is not possible to export the data')

# Uncomment this to render the HTML result locally in your python notebook
#from IPython.display import HTML
#HTML(html_container)
]]>
                </code>
              </script>
            </scriptExecutable>
          </task>
        </taskFlow>
      </job>