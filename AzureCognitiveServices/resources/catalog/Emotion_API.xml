<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="Emotion_API" tags="Azure,Cognitive Services,Cognitive,Emotion,Machine Learning,Azure Cognitive Services,Deep Learning" projectName="Vision" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2" >
  <variables>
    <variable name="AZURE_EMOTION_API_ENDPOINT" value="https://westus.api.cognitive.microsoft.com" description="Endpoint of the Azure Emotion API"/>
    <variable name="IMAGE_URL" value="https://upload.wikimedia.org/wikipedia/commons/c/c3/RH_Louise_Lillian_Gish.jpg" description="URL of the image"/>
    <variable name="OUTPUT_FORMAT" value="HTML" model="PA:LIST(HTML, JSON)" description="Format of the output file"/>
  </variables>
  <description>
    <![CDATA[ Welcome to the Microsoft Emotion API, which allows you to build more personalized apps with Microsoft as cutting edge cloud-based emotion recognition algorithm. <https://azure.microsoft.com/en-us/services/cognitive-services/emotion/> ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="ai-azure-cognitive-services"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_emotion.png"/>
    <info name="Documentation" value="https://azure.microsoft.com/en-us/services/cognitive-services/emotion/"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Emotion_API" >
      <description>
        <![CDATA[ Welcome to the Microsoft Emotion API, which allows you to build more personalized apps with Microsoft as cutting edge cloud-based emotion recognition algorithm. https://azure.microsoft.com/en-us/services/cognitive-services/emotion/ ]]>
      </description>
      <variables>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True" description="If true, the workflow tasks will be executed inside a docker container"/>
        <variable inherited="true" name="DOCKER_IMAGE" value="activeeon/dlm3" description="Name of the docker image"/>
        <variable name="AZURE_EMOTION_API_ENDPOINT" value="https://westus.api.cognitive.microsoft.com" inherited="true" description="Endpoint of the Azure Emotion API"/>
        <variable name="IMAGE_URL" value="https://upload.wikimedia.org/wikipedia/commons/c/c3/RH_Louise_Lillian_Gish.jpg" inherited="true" description="URL of the image"/>
        <variable name="OUTPUT_FORMAT" value="HTML" inherited="true" model="PA:LIST(HTML, JSON)" description="Format of the output file"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_emotion.png"/>
        <info name="task.documentation" value="https://azure.microsoft.com/en-us/services/cognitive-services/emotion/"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Emotion_API")

import http.client, urllib.request, urllib.parse, urllib.error, base64, requests, json
import pandas as pd
from pandas.io.json import json_normalize
from pprint import pprint


if 'variables' in locals():
  if variables.get("IMAGE_URL") is not None:
      IMAGE_URL = variables.get("IMAGE_URL")
  if variables.get("OUTPUT_FORMAT") is not None:
      OUTPUT_FORMAT = variables.get("OUTPUT_FORMAT")
  # Replace the subscription_key string value with your valid subscription key.
  if credentials.get("AZURE_EMOTION_API_KEY") is not None:
      subscription_key = credentials.get("AZURE_EMOTION_API_KEY")
  else:
      print("You first need to add your Azure Cognitive Services API key (AZURE_EMOTION_API_KEY) to the third party credentials")
      sys.exit(1)

  # Replace or verify the region.
  #
  # You must use the same region in your REST API call as you used to obtain your subscription keys.
  # For example, if you obtained your subscription keys from the westus region, replace
  # "westcentralus" in the URI below with "westus".
  #
  # NOTE: Free trial subscription keys are generated in the westcentralus region, so if you are using
  # a free trial subscription key, you should not need to change this region.
  #uri_base = 'https://westcentralus.api.cognitive.microsoft.com'
  uri_base = variables.get("AZURE_EMOTION_API_ENDPOINT")

# Request headers.
headers = {
    'Content-Type': 'application/json',
    'Ocp-Apim-Subscription-Key': subscription_key,
}

# Request parameters.
params = {
    'returnFaceId': 'true',
    'returnFaceLandmarks': 'false',
    'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise',
}

print("******** Query params ********")
pprint(params)

# Body. The URL of a JPEG image to analyze.
body = {'url': IMAGE_URL}

try:
  # Execute the REST API call and get the response.
  #response = requests.request('POST', uri_base + '/face/v1.0/detect', json=body, data=None, headers=headers, params=params)
  response = requests.request('POST', uri_base + '/emotion/v1.0/recognize', json=body, data=None, headers=headers, params=params)
  print('Response:')
  print(response)
  parsed = json.loads(response.text)
  if len(response.text) > 2 and response.status_code == 200:
    emotion = parsed[0]["scores"]
  else:
    emotion = {'error': 'face or emotion not detected'}
  #rect = parsed[0]["faceAttributes"]["emotion"]
  #print(rect)
  #print (json.dumps(max(rect, key=rect.get), sort_keys=True, indent=2))
except Exception as ex:
  print('Error:')
  print(ex)

output_json = json.dumps(parsed)
print(output_json)

if 'variables' in locals():
  variables.put("AZURE_EMOTION_API_OUTPUT_JSON", output_json)

df = json_normalize(emotion)

table = []
table.append("""<tr><td><a href="{0}"><img src="{0}" height="150" width="150"/></a></td><td>{1}</td>""".format(IMAGE_URL, df.to_html()))

css_style="""
table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%;
}
td {
  border: 1px solid #999999;
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  border-left: 2px solid #999999
}
"""

html = """<table><th>Image</th><th>Reponse</th></tr>{0}</table>""".format("\n".join(table))

html_container = """<!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <meta name="description" content="Face API">
                    <style>{0}</style>
                  </head>
                  <body>{1}</body></html>
""".format(css_style, html)

if 'variables' in locals():
  if OUTPUT_FORMAT == 'JSON':
       result = output_json.encode('utf-8')
       resultMetadata.put("file.extension", ".json")
       resultMetadata.put("file.name", "result.json")
       resultMetadata.put("content.type", "application/json")
  elif OUTPUT_FORMAT == 'HTML':
       result = html_container.encode('utf-8')
       resultMetadata.put("file.extension", ".html")
       resultMetadata.put("file.name", "result.html")
       resultMetadata.put("content.type", "text/html")

print("END Emotion_API")
]]>
                </code>
              </script>
            </scriptExecutable>
            <controlFlow block="none"></controlFlow>
          </task>
        </taskFlow>
        <metadata>
          <visualization>
            <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:1139px;
            height:566px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-333.9875030517578px;left:-497.5px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_175" style="top: 339px; left: 502.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_emotion.png" width="20px">&nbsp;<span class="name">Emotion_API</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 542px; top: 369px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
            xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
            xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
          </visualization>
        </metadata>
      </job>