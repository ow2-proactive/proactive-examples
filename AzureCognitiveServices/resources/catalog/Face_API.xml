<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="Face_API" tags="Azure,Cognitive Services,Cognitive,Face,Machine Learning,Azure Cognitive Services,Deep Learning" projectName="Vision" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2" >
  <variables>
    <variable name="AZURE_FACE_API_ENDPOINT" value="https://eastus2.api.cognitive.microsoft.com" description="Endpoint of the Azure Face API"/>
    <variable name="IMAGE_URL" value="https://upload.wikimedia.org/wikipedia/commons/c/c3/RH_Louise_Lillian_Gish.jpg" description="URL of the image"/>
    <variable name="OUTPUT_FORMAT" value="HTML" model="PA:LIST(JSON, HTML)" description="Format of the output file"/>
  </variables>
  <description>
    <![CDATA[ The cloud-based Face API provides developers with access to advanced face algorithms. Microsoft Face algorithms enable face attribute detection and face recognition. Learn how to analyze content in different ways with our quickstarts, tutorials, and samples. <https://azure.microsoft.com/en-us/services/cognitive-services/face/> ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="ai-azure-cognitive-services"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_face.png"/>
    <info name="Documentation" value="https://azure.microsoft.com/en-us/services/cognitive-services/face/"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Face_API" >
      <description>
        <![CDATA[ The cloud-based Face API provides developers with access to advanced face algorithms. Microsoft Face algorithms enable face attribute detection and face recognition. Learn how to analyze content in different ways with our quickstarts, tutorials, and samples. https://azure.microsoft.com/en-us/services/cognitive-services/face/ ]]>
      </description>
      <variables>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True" description="If true, the workflow tasks will be executed inside a docker container"/>
        <variable inherited="true" name="DOCKER_IMAGE" value="activeeon/dlm3" description="Name of the docker image"/>
        <variable name="AZURE_FACE_API_ENDPOINT" value="https://eastus2.api.cognitive.microsoft.com" inherited="true" description="Endpoint of the Azure Face API"/>
        <variable name="IMAGE_URL" value="https://upload.wikimedia.org/wikipedia/commons/c/c3/RH_Louise_Lillian_Gish.jpg" inherited="true" description="URL of the image"/>
        <variable name="OUTPUT_FORMAT" value="HTML" inherited="true" model="PA:LIST(HTML, JSON)" description="Format of the output file"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_face.png"/>
        <info name="task.documentation" value="https://azure.microsoft.com/en-us/services/cognitive-services/face/"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file language="groovy" url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw"/>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Face_API")

import http.client, urllib.request, urllib.parse, urllib.error, base64, requests, json
import pandas as pd
from pprint import pprint

from pandas.io.json import json_normalize

if 'variables' in locals():
  if variables.get("IMAGE_URL") is not None:
      IMAGE_URL = variables.get("IMAGE_URL")
  if variables.get("OUTPUT_FORMAT") is not None:
      OUTPUT_FORMAT = variables.get("OUTPUT_FORMAT")
  # Replace the subscription_key string value with your valid subscription key.
  if credentials.get("AZURE_FACE_API_KEY") is not None:
      subscription_key = credentials.get("AZURE_FACE_API_KEY")
  else:
      print("You first need to add your Azure Cognitive Services API key (AZURE_FACE_API_KEY) to the third party credentials")
      sys.exit(1)
  # Replace or verify the region.
  #
  # You must use the same region in your REST API call as you used to obtain your subscription keys.
  # For example, if you obtained your subscription keys from the westus region, replace
  # "westcentralus" in the URI below with "westus".
  #
  # NOTE: Free trial subscription keys are generated in the westcentralus region, so if you are using
  # a free trial subscription key, you should not need to change this region.
  #uri_base = 'https://westcentralus.api.cognitive.microsoft.com'
  uri_base = variables.get("AZURE_FACE_API_ENDPOINT")

# Request headers.
headers = {
    'Content-Type': 'application/json',
    'Ocp-Apim-Subscription-Key': subscription_key,
}

# Request parameters.
params = {
    'returnFaceId': 'true',
    'returnFaceLandmarks': 'false',
    'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise',
}

print("******** Query params ********")
pprint(params)

# Body. The URL of a JPEG image to analyze.
body = {'url': IMAGE_URL}

try:
  # Execute the REST API call and get the response.
  response = requests.request('POST', uri_base + '/face/v1.0/detect', json=body, data=None, headers=headers, params=params)
  print('Response:')
  print(response)
  parsed = json.loads(response.text)
  """
  '[{"faceId":"79af2eb9-2653-4a65-8b7e-5311b3643f18","faceRectangle":{"top":131,"left":177,"width":162,"height":162},"faceAttributes":{"smile":0.0,"headPose":{"pitch":0.0,"roll":0.1,"yaw":-32.9},"gender":"female","age":22.9,"facialHair":{"moustache":0.0,"beard":0.0,"sideburns":0.0},"glasses":"NoGlasses","emotion":{"anger":0.0,"contempt":0.0,"disgust":0.0,"fear":0.0,"happiness":0.0,"neutral":0.986,"sadness":0.009,"surprise":0.005},"blur":{"blurLevel":"low","value":0.06},"exposure":{"exposureLevel":"goodExposure","value":0.67},"noise":{"noiseLevel":"low","value":0.0},"makeup":{"eyeMakeup":true,"lipMakeup":true},"accessories":[],"occlusion":{"foreheadOccluded":false,"eyeOccluded":false,"mouthOccluded":false},"hair":{"bald":0.0,"invisible":false,"hairColor":[{"color":"brown","confidence":1.0},{"color":"black","confidence":0.87},{"color":"other","confidence":0.51},{"color":"blond","confidence":0.08},{"color":"red","confidence":0.08},{"color":"gray","confidence":0.02}]}}}]'
  """
  #print (json.dumps(parsed, sort_keys=True, indent=2))
except Exception as e:
  print('Error:')
  print(e)

output_json = json.dumps(parsed)
print(output_json)

if 'variables' in locals():
  variables.put("AZURE_FACE_API_OUTPUT_JSON", output_json)

df = json_normalize(parsed)

table = []
table.append("""<tr><td><a href="{0}"><img src="{0}" height="150" width="150"/></a></td><td>{1}</td>""".format(IMAGE_URL, df.to_html()))

css_style="""
table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%;
}
td {
  border: 1px solid #999999;
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  border-left: 2px solid #999999
}
"""

html = """<table><th>Image</th><th>Reponse</th></tr>{0}</table>""".format("\n".join(table))

html_container = """<!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <meta name="description" content="Face API">
                    <style>{0}</style>
                  </head>
                  <body>{1}</body></html>
""".format(css_style, html)

if 'variables' in locals():
  if OUTPUT_FORMAT == 'JSON':
       result = output_json.encode('utf-8')
       resultMetadata.put("file.extension", ".json")
       resultMetadata.put("file.name", "result.json")
       resultMetadata.put("content.type", "application/json")
  elif OUTPUT_FORMAT == 'HTML':
       result = html_container.encode('utf-8')
       resultMetadata.put("file.extension", ".html")
       resultMetadata.put("file.name", "result.html")
       resultMetadata.put("content.type", "text/html")

print("END Face_API")
]]>
                </code>
              </script>
            </scriptExecutable>
            <controlFlow block="none"></controlFlow>
          </task>
        </taskFlow>
        <metadata>
          <visualization>
            <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:1139px;
            height:566px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-333.9875030517578px;left:-497.5px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_187" style="top: 339px; left: 502.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_face.png" width="20px">&nbsp;<span class="name">Face_API</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 542px; top: 369px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
            xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
            xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
          </visualization>
        </metadata>
      </job>