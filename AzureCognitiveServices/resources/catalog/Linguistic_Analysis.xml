<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="Linguistic Analysis" projectName="Language"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2">
  <variables>
    <variable name="TEXT" value="Hi, Tom! How are you today?&quot;" />
    <variable name="LANGUAGE" value="en" />
    <variable name="OUTPUT_FORMAT" value="HTML" model="PA:LIST(CSV, HTML, JSON)"/>
  </variables>
  <description>
    <![CDATA[ The Linguistic Analysis API provides access to natural language processing (NLP) tools that identify the structure of text. The current release provides three types of analysis:
1. Sentence separation and tokenization
2. Part-of-speech tagging
3. Constituency parsing ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="azure-cognitive-services"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_bing_spell_check.png"/>
    <info name="Documentation" value="https://docs.microsoft.com/en-us/azure/cognitive-services/linguisticanalysisapi/home"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="LinguisticAnalysis">
      <description>
        <![CDATA[ This task wraps the Linguistic Analysis API of Microsoft which provides access to natural language processing (NLP) tools that identify the structure of text. The current release provides three types of analysis:
1. Sentence separation and tokenization
2. Part-of-speech tagging
3. Constituency parsing
The task requires this third-party credential : $LINGUISTIC_ANALYTICS_API_KEY which provides access to this API. Please refer to the User documentation to learn how to add third-party credentials. This task has three attributes: 
$TEXT (required) is the text to be analyzed. Its maximum length is 65536..
$LANGUAGE (required) is the language of the input text.
$KIND (required) is the analyzers array.
The task's output $BING_SPELL CHECK_OUTPUT is the result of the API call in a JSON format. ]]>
      </description>
      <variables>
        <variable name="TEXT" value="Hi, Tom! How are you today?&quot;" inherited="true" />
        <variable name="LANGUAGE" value="en" inherited="true" />
        <variable name="OUTPUT_FORMAT" value="HTML" inherited="true" model="PA:LIST(CSV, HTML, JSON)"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/azure/api_bing_spell_check.png"/>
        <info name="task.documentation" value="https://docs.microsoft.com/en-us/azure/cognitive-services/linguisticanalysisapi/home"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <pre>
        <script>
          <code language="bash">
            <![CDATA[

]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import requests
import json
import urllib
from pprint import pprint
import pandas as pd

# You can customize the api server location
api_location="westus"

# Congitive Services - Linguistic Analysis API URL:
linguistic_analysis_url = "https://{0}.api.cognitive.microsoft.com/linguistics/v1.0/analyze".format(api_location)


analyzers={"POS_Tags":"4fa79af1-f22c-408d-98bb-b7d7aeef7f04",
          "Constituency_Tree":"22a6b758-420f-4745-8a3c-46835a67c0d2",
          "Tokens":"08ea174b-bfdb-4e64-987e-602f85da7f72"}
analyzers_names={"4fa79af1-f22c-408d-98bb-b7d7aeef7f04":"POS_Tags",
                  "22a6b758-420f-4745-8a3c-46835a67c0d2":"Constituency_Tree",
                  "08ea174b-bfdb-4e64-987e-602f85da7f72":"Tokens"}


# READ TASK VARIABLES
if 'variables' in locals():
    if variables.get("TEXT") is not None:
        TEXT = variables.get("TEXT")
    else:
        print("You first need to specify the text")
        sys.exit(1)
    if variables.get("LANGUAGE") is not None:
        LANGUAGE = variables.get("LANGUAGE")
    else:
        print("You first need to specify the LANGUAGE")
        sys.exit(1)
    if variables.get("OUTPUT_FORMAT") is not None:
        OUTPUT_FORMAT = variables.get("OUTPUT_FORMAT")
    #if variables.get("KIND") is not None:
        #KIND = variables.get("KIND")
    #else:
        #print("You first need to specify the analyzer Ids")
        #sys.exit(1)
    # Provide a valid subscription API token
    if credentials.get("LINGUISTIC_ANALYTICS_API_KEY") is not None:
        subscription_key = credentials.get("LINGUISTIC_ANALYTICS_API_KEY")
    else:
        print("You first need to add your Azure Cognitive Services API key (LINGUISTIC_ANALYTICS_API_KEY) to the third party credentials")
        sys.exit(1)

# Send API request
headers = {
    # Request headers
    'Content-Type': 'application/json',
    'Ocp-Apim-Subscription-Key': subscription_key
}


# Set API request parameters
#{
#   "language" : "en",
#   "analyzerIds" : ["4fa79af1-f22c-408d-98bb-b7d7aeef7f04", "22a6b758-420f-4745-8a3c-46835a67c0d2"], 
#    "text" : "Hi, Tom! How are you today?" 
#}
params={'text': TEXT, 'language' : LANGUAGE, 'analyzerIds': list(analyzers.values())}

print("******** Query params ********")
pprint(params)


response = requests.post(linguistic_analysis_url, headers=headers, data=json.dumps(params))
response.raise_for_status()

# Get a JSON response
api_results = response.json()

# Print the results
#pprint(api_results)

if 'variables' in locals():
    variables.put('LINGUISTIC_ANALYSIS_OUTPUT', api_results)
    
print("BEGIN Export_Results")

table = []
for document in api_results:
    try: 
       analyzer_name= analyzers_names[document["analyzerId"]]
    except KeyError:
        analyzer_name = None        
        pass
    try: 
       json_results= json.dumps(document["result"],indent=4,sort_keys=True)
    except KeyError:
        json_results = None        
        pass         
    #print(json_results)
    table.append("""<tr><td>{0}</td><td align="left"><PRE><CODE>{1}</PRE></CODE></td>""".format(analyzer_name, json_results))
#text-align: center;
css_style="""table {
  border: 1px solid #999999;
  border-collapse: collapse;
  width: 100%;
}
td {
  border: 1px solid #999999;
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  border-left: 2px solid #999999;
}"""
html = ("""<table><tr><th width=20%>Analyzer Name</th><th>Results</th></tr>{0}</table>""").format("\n".join(table))
html_container="""                                                                        
            <!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <meta name="description" content="Linguistic Analysis API Results">
                    <style>{0}</style>
                  </head>
                  <body>{1}</body></html>""".format(css_style,html)

if api_results != None and 'resultMetadata' in locals(): 
    dataframe=pd.read_html(html_container,header=0, encoding='utf-8')[0]
    
    if OUTPUT_FORMAT == 'JSON':
        result = json.dumps(api_results).encode('utf-8')
        resultMetadata.put("file.extension", ".json")
        resultMetadata.put("file.name", "result.json")
        resultMetadata.put("content.type", "application/json")
    elif OUTPUT_FORMAT == 'CSV':
        result = dataframe.to_csv(index=False).encode('utf-8')
        resultMetadata.put("file.extension", ".csv")
        resultMetadata.put("file.name", "result.csv")
        resultMetadata.put("content.type", "text/csv")
    elif OUTPUT_FORMAT == 'HTML':
        result = html_container.encode('utf-8')
        resultMetadata.put("file.extension", ".html")
        resultMetadata.put("file.name", "result.html")
        resultMetadata.put("content.type", "text/html")
    print("END Export_Results")  
else:
    print('It is not possible to export the data')

# Uncomment this to render the HTML result locally in your python notebook
#from IPython.display import HTML
#HTML(html_container)
]]>
                </code>
              </script>
            </scriptExecutable>
          </task>
        </taskFlow>
      </job>