<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.13" xsi:schemaLocation="urn:proactive:jobdescriptor:3.13 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.13/schedulerjob.xsd"  name="Iterative_ETL" projectName="2. Advanced Workflows" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="DATABASE" value="demo" />
    <variable name="HOST" value="xx" />
    <variable name="PORT" value="37021" model="PA:Integer"/>
    <variable name="USER" value="demo" />
    <variable name="CREDENTIALS_KEY" value="mysql://${USER}@${HOST}:${PORT}" model="PA:Credential"/>
    <variable name="spark_service_instance_id" value="xx"  model="PA:NOT_EMPTY_STRING"/>
  </variables>
  <description>
    <![CDATA[ Iterative workflow performing ETL (Extract Transform Load) operations. For each iteration, data are retrieved from a csv file stored in Amazon S3, and pushed to a MySQL server. Then a Scala Spark task retrieves data from the MySQL server, transforms data, and stores data in a HDFS (Hadoop File System). If a cumulative count of rows in the HDFS is reached, the loop condition is broken and a web notification is sent. This workflow requires a full running Spark HDFS platform, with a running MySQL server. ]]>
  </description>
  <genericInformation>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/etl.png"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="MySQL_to_CSV" 
    
    
    
    
    fork="true">
      <description>
        <![CDATA[ This task allows to import data from MySQL database.
It requires the following third-party credential:  {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
DATABASE (required) is the database name.
HOST (required) is the hostname or the IP of the machine hosting the DB.
PORT (required) is the listening port.
USER (required) is the user.
SQL_QUERY (required) is the user's sql query.
OUTPUT_FILE (optional) is a relative path in the data space used to save the results in a CSV file.
OUTPUT_TYPE (optional) if set to HTML, it allows to preview the results in Scheduler Portal in an HTML format. Default is HTML. ]]>
      </description>
      <variables>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
        <variable inherited="true" name="DOCKER_IMAGE" value="activeeon/dlm3"/>
        <variable name="DATABASE" value="" inherited="true" />
        <variable name="HOST" value="" inherited="true" />
        <variable name="PORT" value="" inherited="true" model="PA:Integer"/>
        <variable name="USER" value="" inherited="true" />
        <variable name="CREDENTIALS_KEY" value="mysql://${USER}@${HOST}:${PORT}" inherited="true" model="PA:Credential"/>
        <variable name="SQL_QUERY" value="select * from iris" inherited="false" />
        <variable name="OUTPUT_TYPE" value="CSV" inherited="false" model="PA:List(CSV,HTML)"/>
        <variable name="OUTPUT_FILE" value="mysql_extract_job_${PA_JOB_ID}_iter_${PA_TASK_ITERATION}.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Feed_MySQL"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw" language="groovy"></file>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/data-connectors/resources/ImportFromSqlDB/raw" language="cpython">
            <arguments>
              <argument value="mysql"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
      <metadata>
        <positionTop>
            447.5
        </positionTop>
        <positionLeft>
            219
        </positionLeft>
      </metadata>
    </task>
    <task name="Feed_MySQL" 
    
    
    
    
    fork="true">
      <description>
        <![CDATA[ This task allows to export data to MySQL database.
It requires the following third-party credential: {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
DATABASE (required) is the database name.
HOST (required) is the hostname or the IP of the machine hosting the DB.
PORT (required) is the listening port.
USER (required) is the user.
TABLE (required) is the table name.
INSERT_MODE (required) indicates the behavior to follow when the table exists in the database amongst:
. fail: If table exists, do nothing.
. replace: If table exists, drop it, recreate it, and insert data.
. append: (default) If table exists, insert data. Create if does not exist.
INPUT_FILE (required) is the relative path in the data space of the CSV file that contains data to be imported. The string could also be a URL. Valid URL schemes include http, ftp, s3, and file. ]]>
      </description>
      <variables>
        <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
        <variable inherited="true" name="DOCKER_IMAGE" value="activeeon/dlm3"/>
        <variable name="DATABASE" value="" inherited="true" />
        <variable name="HOST" value="" inherited="true" />
        <variable name="PORT" value="" inherited="true" model="PA:Integer"/>
        <variable name="USER" value="" inherited="true" />
        <variable name="CREDENTIALS_KEY" value="mysql://${USER}@${HOST}:${PORT}" inherited="true" model="PA:Credential"/>
        <variable name="TABLE" value="iris" inherited="false" />
        <variable name="INSERT_MODE" value="append" inherited="false" model="PA:LIST(fail, replace, append)"/>
        <variable name="INPUT_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/iris.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png"/>
        <info name="task.documentation" value="user/ProActiveUserGuide.html#_sql"/>
      </genericInformation>
      <depends>
        <task ref="Start"/>
      </depends>
      <inputFiles>
        <files  includes="$INPUT_FILE" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_vars/raw" language="groovy"></file>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/data-connectors/resources/ExportToSqlDB/raw" language="cpython">
            <arguments>
              <argument value="mysql"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            272.5
        </positionTop>
        <positionLeft>
            227
        </positionLeft>
      </metadata>
    </task>
    <task name="MySQL_to_Spark_to_HDFS" 
    
    
    
    
    fork="true">
      <description>
        <![CDATA[ A workflow to submit a Spark job from a docker container, to read/write files from/to HDFS. This workflow requires a Spark/HDFS platform. ]]>
      </description>
      <variables>
        <variable name="MYSQL_TABLE" value="iris" inherited="false" />
        <variable name="output_hdfs_csv_file_name" value="iris_PROCESSED.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="PRE_SCRIPT_AS_FILE" value="script.scala"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$spark_token_name"/>
      </genericInformation>
      <depends>
        <task ref="Feed_MySQL"/>
      </depends>
      <inputFiles>
        <files  includes="$csv_file_name" accessMode="transferFromGlobalSpace"/>
        <files  includes="mysql-connector-java-8.0.22.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <pre>
        <script>
          <code language="scalaw">
            <![CDATA[
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.types.DoubleType

import org.apache.spark.sql.SQLContext

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path

import java.nio.file.{Paths, Files}
import java.net.URI


object Application {
    
  def main() {
    val spark = SparkSession
          .builder
          .appName("HDFS-MySQL-Spark app")
          .getOrCreate()
    val sc = spark.sparkContext

    // Get args
    val args = sc.getConf.get("spark.driver.args").split("\\s+")
    val mysql_host = args(0)
    val mysql_port = args(1)
    val mysql_user = args(2)
    val mysql_psswd = args(3)
    val mysql_db_name = args(4)
    val mysql_table_name = args(5)
    val hdfs_url = args(6)
    val output_hdfs_csv_file_name = args(7)
      
    // MySQL to DF
    val sqlcontext = new org.apache.spark.sql.SQLContext(sc)
    val df = sqlcontext.read.format("jdbc").option("url", "jdbc:mysql://" + mysql_host + ":" + mysql_port + "/" + mysql_db_name ).option("driver", "com.mysql.jdbc.Driver").option("dbtable", mysql_table_name).option("user", mysql_user).option("password", mysql_psswd).load()
        
      
    // process DF
    val processed_df = df.withColumn("CopiedColumn",col("species"))
  	println("PROCESSED DF")
    processed_df.show()
      
    // Write DF to HDFS as CSV
    processed_df.write.mode(SaveMode.Overwrite).csv(hdfs_url + "/" + output_hdfs_csv_file_name)

  }
}

Application.main()
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
// Retrieve variables
def mysql_host = variables.get("HOST")
def mysql_port = variables.get("PORT")
def mysql_user = variables.get("USER")
def mysql_cred_keys = variables.get("CREDENTIALS_KEY")
def mysql_passwd = credentials.get(mysql_cred_keys)
def mysql_database = variables.get("DATABASE")
def mysql_table = variables.get("MYSQL_TABLE")
def spark_network_name = variables.get("spark_network_name")
def spark_master_url = variables.get("spark_master_url")
def hdfs_url = "hdfs://" + variables.get("hdfs_namenode_host_port")
def output_hdfs_csv_file_name = variables.get("output_hdfs_csv_file_name")

// Submit the Spark job
def spark_shell_command = "/usr/local/spark/bin/spark-shell --driver-memory 800m --executor-memory 800m --master " + spark_master_url + " --jars /usr/local/spark/jars/*,mysql-connector-java-8.0.22.jar -I /localspace/script.scala --conf spark.driver.args='" + mysql_host + " " + mysql_port + " " + mysql_user + " " + mysql_passwd + " " + mysql_database + " " + mysql_table + " " + hdfs_url + " " + output_hdfs_csv_file_name + "'"
cmd = ["docker", "run", "--rm", "--net", spark_network_name, "-w", "/localspace", "-v", localspace + ":/localspace", "activeeon/hdfs-spark:latest", "bash", "-c", spark_shell_command]
println cmd

cmd.execute().waitForProcessOutput(System.out, System.err)
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            268.5
        </positionTop>
        <positionLeft>
            461
        </positionLeft>
      </metadata>
    </task>
    <task name="HDFS_to_CSV" 
    
    
    
    
    fork="true">
      <description>
        <![CDATA[ A workflow to submit a Spark job from a docker container, to read/write files from/to HDFS. This workflow requires a Spark/HDFS platform. ]]>
      </description>
      <variables>
        <variable name="spark_hdfs_instance_id" value="" inherited="false" />
        <variable name="OUTPUT_FILE" value="hdfs_extract_job_${PA_JOB_ID}_iter_${PA_TASK_ITERATION}.csv" inherited="false" />
        <variable name="input_hdfs_csv_file_name" value="iris_PROCESSED.csv" inherited="false" />
      </variables>
      <genericInformation>
        <info name="PRE_SCRIPT_AS_FILE" value="script.scala"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$spark_token_name"/>
      </genericInformation>
      <depends>
        <task ref="MySQL_to_Spark_to_HDFS"/>
      </depends>
      <pre>
        <script>
          <code language="scalaw">
            <![CDATA[
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.types.DoubleType

import org.apache.spark.sql.SQLContext

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path

import java.nio.file.{Paths, Files}
import java.net.URI


object Application {
    
  def main() {
    val spark = SparkSession
          .builder
          .appName("HDFS-to-CSV app")
          .getOrCreate()
    val sc = spark.sparkContext

    // Get args
    val args = sc.getConf.get("spark.driver.args").split("\\s+")
    val hdfs_url = args(0)
    val input_hdfs_csv_file_name = args(1)
      
    // HDFS CSV to DF
    val df = spark
      			.read
      			.options(Map("inferSchema"->"true","delimiter"->",","header"->"false"))
      			.csv(hdfs_url + "/" + input_hdfs_csv_file_name)

      
    println("CSV>>")
    df.collect.foreach(println)
  	println("<<CSV")
  }
}

Application.main()
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import org.apache.commons.lang3.StringUtils

def spark_network_name = variables.get("spark_network_name")
def spark_master_url = variables.get("spark_master_url")
def hdfs_url = "hdfs://" + variables.get("hdfs_namenode_host_port")
def input_hdfs_csv_file_name = variables.get("input_hdfs_csv_file_name")
def OUTPUT_FILE = variables.get("OUTPUT_FILE")
def output_file_path = new File(localspace, OUTPUT_FILE).getAbsolutePath()

// Submit the Spark job
def spark_shell_command = "/usr/local/spark/bin/spark-shell --driver-memory 800m --executor-memory 800m --master " + spark_master_url + " --jars /usr/local/spark/jars/* -I /localspace/script.scala --conf spark.driver.args='" + hdfs_url + " " + input_hdfs_csv_file_name + "'"
cmd = ["docker", "run", "--rm", "--net", spark_network_name, "-v", localspace + ":/localspace", "activeeon/hdfs-spark:latest", "bash", "-c", spark_shell_command]
println cmd

def output_with_csv = new StringBuilder()
cmd.execute().waitForProcessOutput(output_with_csv, System.err)

new File(output_file_path).text = StringUtils.substringBetween(output_with_csv.toString(), "CSV>>", "<<CSV").replaceAll("\\[","").replaceAll("\\]","").replaceFirst("\n","")
]]>
          </code>
        </script>
      </scriptExecutable>
      <outputFiles>
        <files  includes="$OUTPUT_FILE" accessMode="transferToGlobalSpace"/>
      </outputFiles>
      <metadata>
        <positionTop>
            439.5
        </positionTop>
        <positionLeft>
            491.25
        </positionLeft>
      </metadata>
    </task>
    <task name="Start" 
    
    
    
    
    fork="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
        <info name="Documentation" value="user/ProActiveUserGuide.html#_loop"/>
      </genericInformation>
      <pre>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Retrieve_variables_from_service_instance_id/raw" language="groovy">
            <arguments>
              <argument value="$spark_service_instance_id"/>
              <argument value="spark_network_name"/>
              <argument value="spark_network_name"/>
              <argument value="spark_master_url"/>
              <argument value="spark_master_url"/>
              <argument value="spark_token_name"/>
              <argument value="INSTANCE_NAME"/>
              <argument value="hdfs_namenode_host_port"/>
              <argument value="hdfs_namenode_host_port"/>
            </arguments>
          </file>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="javascript">
            <![CDATA[
print('Loop block start ' + variables.get('PA_TASK_ITERATION'))
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="start"></controlFlow>
      <metadata>
        <positionTop>
            91.5
        </positionTop>
        <positionLeft>
            590
        </positionLeft>
      </metadata>
    </task>
    <task name="Loop" 
    
    
    
    
    fork="true">
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png"/>
      </genericInformation>
      <depends>
        <task ref="MySQL_to_CSV"/>
        <task ref="HDFS_to_CSV"/>
      </depends>
      <inputFiles>
        <files  includes="*_extract_job_${PA_JOB_ID}_iter_${PA_TASK_ITERATION}.csv" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <scriptExecutable>
        <script>
          <code language="R">
            <![CDATA[
# Get variables
job_id <- variables["PA_JOB_ID"]
iteration_id <- variables["PA_TASK_ITERATION"]

# Get SOURCE DATA as dataframe 
mysql_csv_file_path <- file.path( localspace, paste("mysql_extract_job_", job_id, "_iter_", iteration_id, ".csv", sep=''))
mysql_csv <- read.table(file=mysql_csv_file_path, header=TRUE, sep=",")

# Get TARGET DATA as dataframe
hdfs_csv_file_path <- file.path( localspace, paste("hdfs_extract_job_", job_id, "_iter_", iteration_id, ".csv", sep=''))
hdfs_csv <- read.table(file=hdfs_csv_file_path, header=FALSE, sep=",")

# Print nb rows of SOURCE & DATA
nrow_mysql_csv <- nrow(mysql_csv)
nrow_hdfs_csv <- nrow(hdfs_csv)
print(paste ("!! CONTROL !! NB ROWS SOURCE/TARGET ==> MySQL", nrow_mysql_csv, "lines", "HDFS", nrow_hdfs_csv, "lines", sep=' '))

# Store hdfs nb rows for iteration control
variables[paste("nrow_hdfs_extract_job_", job_id, "_iter_", iteration_id, sep='')] <- nrow_hdfs_csv
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow  block="end">
        <loop target="Start">
          <script>
            <code language="groovy">
              <![CDATA[
// Get variables
def job_id = variables.get("PA_JOB_ID")
def iteration_id = variables.get("PA_TASK_ITERATION") as Integer
def nrow_hdfs_csv = variables.get("nrow_hdfs_extract_job_" + job_id + "_iter_" + iteration_id) as Integer

// Get accumulated nb rows
def accu_nrow = nrow_hdfs_csv
def accu_variable_name = "accu_nrow_hdfs_extract_job_" + job_id
if (iteration_id > 0) {
    accu_nrow += variables.get(accu_variable_name)
}

// Update accumulated nb rows
println("STORING NEW ACCU = " + accu_nrow)
variables.put(accu_variable_name, accu_nrow)


if(accu_nrow <= 300) {
    loop = true;
} else {
    loop = false;
}
]]>
            </code>
          </script>
        </loop>
      </controlFlow>
      <metadata>
        <positionTop>
            629.5
        </positionTop>
        <positionLeft>
            591
        </positionLeft>
      </metadata>
    </task>
    <task name="Web_Notification" 
    
    
    
    
    fork="true">
      <description>
        <![CDATA[ Task to send a message to the notification service ]]>
      </description>
      <variables>
        <variable name="MESSAGE" value="[ATRADIUS] NOTIFICATION" inherited="true" />
        <variable name="SEVERITY" value="INFO" inherited="true" model="PA:LIST(INFO,WARNING,ERROR,CRITICAL)"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/web_notification.png"/>
      </genericInformation>
      <depends>
        <task ref="Loop"/>
      </depends>
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/notification-tools/resources/Web_Notification_Script/raw" language="groovy"></file>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            737
        </positionTop>
        <positionLeft>
            625.5
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2830px;
            height:3392px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-86.5px;left:-214px"><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_451" style="top: 447.5px; left: 219px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task allows to import data from MySQL database.
It requires the following third-party credential:  {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
DATABASE (required) is the database name.
HOST (required) is the hostname or the IP of the machine hosting the DB.
PORT (required) is the listening port.
USER (required) is the user.
SQL_QUERY (required) is the user's sql query.
OUTPUT_FILE (optional) is a relative path in the data space used to save the results in a CSV file.
OUTPUT_TYPE (optional) if set to HTML, it allows to preview the results in Scheduler Portal in an HTML format. Default is HTML."><img src="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png" width="20px">&nbsp;<span class="name">MySQL_to_CSV</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_454" style="top: 272.5px; left: 227px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task allows to export data to MySQL database.
It requires the following third-party credential: {key: mysql://<username>@<host>:<port>, value: MYSQL_PASSWORD}. Please refer to the User documentation to learn how to add third-party credentials.
It uses the following variables:
DATABASE (required) is the database name.
HOST (required) is the hostname or the IP of the machine hosting the DB.
PORT (required) is the listening port.
USER (required) is the user.
TABLE (required) is the table name.
INSERT_MODE (required) indicates the behavior to follow when the table exists in the database amongst:
. fail: If table exists, do nothing.
. replace: If table exists, drop it, recreate it, and insert data.
. append: (default) If table exists, insert data. Create if does not exist.
INPUT_FILE (required) is the relative path in the data space of the CSV file that contains data to be imported. The string could also be a URL. Valid URL schemes include http, ftp, s3, and file."><img src="/automation-dashboard/styles/patterns/img/wf-icons/mysql.png" width="20px">&nbsp;<span class="name">Feed_MySQL</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_457" style="top: 268.5px; left: 461px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A workflow to submit a Spark job from a docker container, to read/write files from/to HDFS. This workflow requires a Spark/HDFS platform."><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">MySQL_to_Spark_to_HDFS</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_460" style="top: 439.5px; left: 491.25px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A workflow to submit a Spark job from a docker container, to read/write files from/to HDFS. This workflow requires a Spark/HDFS platform."><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">HDFS_to_CSV</span></a></div><div class="task block-start ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_463" style="top: 91.5px; left: 590px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Start</span></a></div><div class="task block-end ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_466" style="top: 629.5px; left: 591px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/automation-dashboard/styles/patterns/img/wf-icons/controls_loop.png" width="20px">&nbsp;<span class="name">Loop</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_469" style="top: 737px; left: 625.5px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="Task to send a message to the notification service"><img src="/automation-dashboard/styles/patterns/img/wf-icons/web_notification.png" width="20px">&nbsp;<span class="name">Web_Notification</span></a></div><svg style="position:absolute;left:262.5px;top:312.5px" width="25" height="136" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 135 C -10 85 14 50 4 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.3872640000000005,103.85856000000001 L5.970272951695094,84.38674508456259 L-1.4254099136709153,89.89164062897864 L-7.996646419326279,83.42489099823351 L-2.3872640000000005,103.85856000000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.3872640000000005,103.85856000000001 L5.970272951695094,84.38674508456259 L-1.4254099136709153,89.89164062897864 L-7.996646419326279,83.42489099823351 L-2.3872640000000005,103.85856000000001" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:266.5px;top:131.5px" width="384" height="142" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 141 C -10 91 373 50 363 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M64.56553125,101.41753125 L85.73903245153785,102.24387673977733 L77.9498568500641,97.3114591328217 L81.63296033435957,88.85955113971323 L64.56553125,101.41753125" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M64.56553125,101.41753125 L85.73903245153785,102.24387673977733 L77.9498568500641,97.3114591328217 L81.63296033435957,88.85955113971323 L64.56553125,101.41753125" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:266.5px;top:218.5px" width="285" height="145" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 264 0 C 274 -50 -10 94 0 44 " transform="translate(10.5,50.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M219.375804,-5.709204 L201.83065625515667,6.172201245660959 L205.84124034157927,-2.1293176084845773 L198.25076986364127,-7.362362412759753 L219.375804,-5.709204" class="" stroke="#666" fill="#666" transform="translate(10.5,50.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M219.375804,-5.709204 L201.83065625515667,6.172201245660959 L205.84124034157927,-2.1293176084845773 L198.25076986364127,-7.362362412759753 L219.375804,-5.709204" class="" stroke="#666" fill="#666" transform="translate(10.5,50.5)"></path></svg><svg style="position:absolute;left:530.5px;top:308.5px" width="22" height="132" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 1 131 C 11 81 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.7589759999999997,100.354176 L9.865220836946468,80.06344476647209 L3.1381672994638685,86.36794718171811 L-4.121007981335415,80.68425346700823 L3.7589759999999997,100.354176" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.7589759999999997,100.354176 L9.865220836946468,80.06344476647209 L3.1381672994638685,86.36794718171811 L-4.121007981335415,80.68425346700823 L3.7589759999999997,100.354176" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:262.5px;top:487.5px" width="389" height="143" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 368 142 C 378 92 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M302.5080625,102.23225 L285.42423794599813,89.69658334010174 L289.1183827499097,98.14367125060282 L281.335659196601,103.08626309019203 L302.5080625,102.23225" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M302.5080625,102.23225 L285.42423794599813,89.69658334010174 L289.1183827499097,98.14367125060282 L281.335659196601,103.08626309019203 L302.5080625,102.23225" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:531.5px;top:479.5px" width="120" height="151" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 99 150 C 109 100 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M86.91450524999999,113.25 L79.62632452709721,93.35320574187118 L78.02680716601468,102.43293834870468 L68.80926287580189,102.24090382585648 L86.91450524999999,113.25" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M86.91450524999999,113.25 L79.62632452709721,93.35320574187118 L78.02680716601468,102.43293834870468 L68.80926287580189,102.24090382585648 L86.91450524999999,113.25" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:679.5px;top:131.5px" width="22" height="499" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 0 C -10 50 11 448 1 498 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#316b31" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.4698240000000005,108.57100800000002 L4.815408881304073,128.4688818026244 L-2.269660137844209,122.56957701359161 L-9.183160132287526,128.6690456647802 L-2.4698240000000005,108.57100800000002" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M-2.4698240000000005,108.57100800000002 L4.815408881304073,128.4688818026244 L-2.269660137844209,122.56957701359161 L-9.183160132287526,128.6690456647802 L-2.4698240000000005,108.57100800000002" class="" stroke="#316b31" fill="#316b31" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_overlay l1 component label" id="jsPlumb_1_493" style="position: absolute; transform: translate(-50%, -50%); left: 690px; top: 380.5px;">loop</div><svg style="position:absolute;left:630.5px;top:669.5px" width="61" height="68" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 40 67 C 50 17 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M39.8616475,48.283332250000015 L33.91118770118107,27.94637013215557 L31.712090637598983,36.89980366934926 L22.527659120530316,36.095926994556585 L39.8616475,48.283332250000015" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M39.8616475,48.283332250000015 L33.91118770118107,27.94637013215557 L31.712090637598983,36.89980366934926 L22.527659120530316,36.095926994556585 L39.8616475,48.283332250000015" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 263px; top: 478px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 263px; top: 438px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 267px; top: 303px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 267px; top: 263px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 531px; top: 299px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 531px; top: 259px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 532px; top: 470px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 532px; top: 430px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 630px; top: 122px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint loop-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 680px; top: 122px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 631px; top: 660px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 631px; top: 620px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint loop-source-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected _jsPlumb_endpoint_full" style="position: absolute; height: 20px; width: 20px; left: 681px; top: 620px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#316b31" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 671px; top: 767px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 671px; top: 727px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>