<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.11" xsi:schemaLocation="urn:proactive:jobdescriptor:3.11 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.11/schedulerjob.xsd"  name="Summarize_Data" projectName="4. Features Extraction" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2" >
  <variables>
    <variable name="DOCKER_ENABLED" value="True" model="PA:Boolean"/>
  </variables>
  <description>
    <![CDATA[ Create a set of statistical measures that describe each column in the input data. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="machine-learning-new"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
    <info name="Documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_filter_data"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Summarize_Data" >
      <description>
        <![CDATA[ Create a set of statistical measures that describe each column in the input data. ]]>
      </description>
      <variables>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
        <variable name="DOCKER_IMAGE" value="activeeon/dlm3" inherited="true" />
        <variable name="TASK_ENABLED" value="True" inherited="false" model="PA:Boolean"/>
        <variable name="GLOBAL_MODEL_TYPE" value="KMeans" inherited="false" model="PA:LIST(KMeans,PolynomialFeatures)"/>
        <variable name="REF_COLUMN" value="" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_filter_data"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = variables.get("DOCKER_IMAGE") 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
__file__ = variables.get("PA_TASK_NAME")

if str(variables.get("TASK_ENABLED")).lower() != 'true':
  print("Task " + __file__ + " disabled")
  quit()

print("BEGIN " + __file__)

import sys, bz2, uuid
import pandas as pd
import numpy as np

REF_COLUMN        = variables.get("REF_COLUMN")
LABEL_COLUMN      = variables.get("LABEL_COLUMN")
GLOBAL_MODEL_TYPE = variables.get("GLOBAL_MODEL_TYPE")

assert REF_COLUMN is not None and REF_COLUMN is not ""

IGNORE_COLUMNS = [REF_COLUMN]
if LABEL_COLUMN is not None and LABEL_COLUMN is not "":
  IGNORE_COLUMNS.append(LABEL_COLUMN)

input_variables = {'task.dataframe_id': None}
for key in input_variables.keys():
  for res in results:
    value = res.getMetadata().get(key)
    if value is not None:
      input_variables[key] = value
      break

dataframe_id = input_variables['task.dataframe_id']
print("dataframe id (in): ", dataframe_id)

dataframe_json = variables.get(dataframe_id)
assert dataframe_json is not None
dataframe_json = bz2.decompress(dataframe_json).decode()

dataframe = pd.read_json(dataframe_json, orient='split')

columns = dataframe.drop(IGNORE_COLUMNS, axis=1, inplace=False).columns.values
ncolumns = columns.shape[0]
bins = [10] * ncolumns
print(columns, ncolumns, bins)

#-------------------------------------------------------------
def compute_global_model(df, columns, bins, model_type="KMeans"):
  from sklearn.cluster import KMeans
  from sklearn.preprocessing import PolynomialFeatures

  models = {}
  for j, column in enumerate(columns):
    column_df = df[column]
    X = column_df.values

    if model_type == "KMeans":
      model = KMeans(n_clusters=bins[j], random_state=0).fit(X.reshape(-1,1))

    if model_type == "PolynomialFeatures":
      model = PolynomialFeatures().fit(X.reshape(-1,1))

    models[column] = model
  return models

def compute_features(sub_df, columns, bins, model, model_type="KMeans"):
  import scipy.stats.stats as st
  row = []
  for j, column in enumerate(columns):
    column_df = sub_df[column]
    X = column_df.values
    
    if model is not None:
      if model_type == "KMeans":
        result = model[column].predict(X.reshape(-1,1))

      if model_type == "PolynomialFeatures":
        result = model[column].transform(X.reshape(-1,1)).tolist()
    else:
        result = X
    
    # compute feature histogram
    #counts, bin_edges = np.histogram(result, bins=bins[j], density=False)
    #column_hist = counts

    # compute normalized feature histogram
    counts, bin_edges = np.histogram(result, bins=bins[j], density=True)
    column_hist = counts * np.diff(bin_edges)
    
    row.extend(column_hist)

    # add extra features
    kurtosis = st.kurtosis(X.reshape(-1,1))[0]
    skew = st.skew(X.reshape(-1,1))[0]
    min_value = column_df.min()
    max_value = column_df.max()
    mean_value = column_df.mean()
    median_value = column_df.median()
    row.extend([kurtosis, skew, min_value, max_value, mean_value, median_value])
  return row

def get_summary(df, columns, bins, model, model_type, ref_column, label_column=None):  
  data = {}
  IDs = df[ref_column].unique()
  for i, ID in enumerate(IDs):
    sub_df = df.loc[df[ref_column] == ID]
    row = [ID]
    
    features = compute_features(sub_df, columns, bins, model, model_type)
    row.extend(features)
    
    if label_column is not None and label_column is not "":
        assert sub_df[label_column].unique().shape[0] == 1
        label = sub_df[label_column].unique()[0]
        row.extend([label])
    
    data[i] = row
  return data
#-------------------------------------------------------------

model = None
if GLOBAL_MODEL_TYPE is not None and GLOBAL_MODEL_TYPE is not "":
  print('Computing the global model using ', GLOBAL_MODEL_TYPE)
  model = compute_global_model(dataframe, columns, bins, GLOBAL_MODEL_TYPE)
  print('Finished')

print('Summarizing data...')
data = get_summary(dataframe, columns, bins, model, GLOBAL_MODEL_TYPE, REF_COLUMN, LABEL_COLUMN)
print('Finished')

dataframe = pd.DataFrame.from_dict(data, orient='index')
cols_len = len(dataframe.columns)
dataframe.columns = list(range(0, cols_len))

COLUMNS_NAME = {0: REF_COLUMN}
if LABEL_COLUMN is not None and LABEL_COLUMN is not "":
  COLUMNS_NAME = {0: REF_COLUMN, cols_len-1: LABEL_COLUMN}
dataframe.rename(index=str, columns=COLUMNS_NAME, inplace=True)

dataframe_json = dataframe.to_json(orient='split').encode()
compressed_data = bz2.compress(dataframe_json)

dataframe_id = str(uuid.uuid4())
variables.put(dataframe_id, compressed_data)

print("dataframe id (out): ", dataframe_id)
print('dataframe size (original):   ', sys.getsizeof(dataframe_json), " bytes")
print('dataframe size (compressed): ', sys.getsizeof(compressed_data), " bytes")
print(dataframe.head())

resultMetadata.put("task.name", __file__)
resultMetadata.put("task.dataframe_id", dataframe_id)

LIMIT_OUTPUT_VIEW = variables.get("LIMIT_OUTPUT_VIEW")
LIMIT_OUTPUT_VIEW = 5 if LIMIT_OUTPUT_VIEW is None else int(LIMIT_OUTPUT_VIEW)
if LIMIT_OUTPUT_VIEW > 0:
  print("task result limited to: ", LIMIT_OUTPUT_VIEW, " rows")
  dataframe = dataframe.head(LIMIT_OUTPUT_VIEW).copy()

with pd.option_context('display.max_colwidth', -1):
  result = dataframe.to_html(escape=False)

css_style="""
table {
  border: 1px solid #999999;
  text-align: center;
  border-collapse: collapse;
  width: 100%; 
}
td {
  border: 1px solid #999999;         
  padding: 3px 2px;
  font-size: 13px;
  border-bottom: 1px solid #999999;
  #border-bottom: 1px solid #FF8C00;  
  border-bottom: 1px solid #0B6FA4;   
}
th {
  font-size: 17px;
  font-weight: bold;
  color: #FFFFFF;
  text-align: center;
  background: #0B6FA4;
  #background: #E7702A;       
  #border-left: 2px solid #999999
  border-bottom: 1px solid #FF8C00;            
}
"""
result = """<!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8" />
                <style>{0}</style>
              </head>
              <body>{1}</body></html>
""".format(css_style, result)
result = result.encode('utf-8')
resultMetadata.put("file.extension", ".html")
resultMetadata.put("file.name", "output.html")
resultMetadata.put("content.type", "text/html")

print("END " + __file__)
]]>
            </code>
          </script>
        </scriptExecutable>
        <controlFlow block="none"/>
        <post>
          <script>
            <code language="groovy">
              <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
            </code>
          </script>
        </post>
      </task>
    </taskFlow>
    <metadata>
      <visualization>
        <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:1139px;
            height:566px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-334.00001525878906px;left:-488.5px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_1530" style="top: 339.003px; left: 493.509px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/filled_filter.png" width="20px">&nbsp;<span class="name">Summarize_Data</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 540px; top: 369px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
        xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
        xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
      </visualization>
    </metadata>
  </job>