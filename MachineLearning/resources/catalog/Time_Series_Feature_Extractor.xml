<?xml version="1.0" encoding="UTF-8"?>
<job xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="urn:proactive:jobdescriptor:3.8"
	xsi:schemaLocation="urn:proactive:jobdescriptor:3.8 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.8/schedulerjob.xsd"
	name="Time_Series_Feature_Extractor" projectName="4. Features Extraction"
	priority="normal" onTaskError="continueJobExecution"
	maxNumberOfExecution="2">
	<description>
    <![CDATA[ Extracts features by considering the temporal distribution of data. ]]>
  </description>
	<genericInformation>
		<info name="bucketName" value="machine-learning" />
		<info name="workflow.icon"
			value="/automation-dashboard/styles/patterns/img/wf-icons/time_series.png" />
		<info name="Documentation"
			value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_time_series_feature_extractor" />
		<info name="group" value="public-objects" />
	</genericInformation>
	<taskFlow>
		<task name="Time_Series_Feature_Extractor">
			<description>
        <![CDATA[ Extracts features by considering the temporal distribution of data. ]]>
      </description>
			<variables>
				<variable name="SESSION_COLUMN" value="id_block" inherited="false" />
				<variable name="SLIDING_STEP" value="30000" inherited="false" />
				<variable name="WINDOW_SIZE" value="30000" inherited="false"
					model="PA:Integer" />
				<variable name="START_DATE_TIME" value="20101107"
					inherited="false" />
				<variable name="PATTERN_COLUMN" value="pattern_id"
					inherited="false" />
				<variable name="PATTERNS_COUNT_FEATURES" value="False"
					inherited="false" />
				<variable name="STATE_VARIABLES" value="status,date"
					inherited="false" />
				<variable name="COUNT_VARIABLES" value="ip_from,ip_to,pid,date,time"
					inherited="false" />
				<variable name="STATE_COUNT_FEATURES_VARIABLES" value="True"
					inherited="false" />
			</variables>
			<genericInformation>
				<info name="task.icon"
					value="/automation-dashboard/styles/patterns/img/wf-icons/time_series.png" />
			</genericInformation>
			<forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
				<envScript>
					<script>
						<code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
						</code>
					</script>
				</envScript>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="cpython">
            <![CDATA[
print("BEGIN Time Series Feature Extraction")

from dateutil.parser import *
from datetime import *
from pandas import Timestamp
import pandas as pd
import numpy as np
import time
import pytz
import wget

column_id = []
timestamps = []
features = []
final_timeseries_dict = {}
time_series_null = []
timestamps_label = []


SESSION_COLUMN = str(variables.get("SESSION_COLUMN"))
SLIDING_STEP = int(variables.get("SLIDING_STEP"))
WINDOW_SIZE = int(variables.get("WINDOW_SIZE"))
START_DATE_TIME = str(variables.get("START_DATE_TIME"))
STRUCTURED_FILE = variables.get("DATAFRAME_JSON")
FILE_OUT_FEATURES = "features"
PATTERN_COLUMN = variables.get("PATTERN_COLUMN")
PATTERNS_COUNT_FEATURES = variables.get("PATTERNS_COUNT_FEATURES")
STATE_VARIABLES_INTER = variables.get("STATE_VARIABLES")
COUNT_VARIABLES_INTER = variables.get("COUNT_VARIABLES")
STATE_COUNT_FEATURES_VARIABLES = variables.get("STATE_COUNT_FEATURES_VARIABLES")
STATE_VARIABLES = STATE_VARIABLES_INTER.split(",")
COUNT_VARIABLES = COUNT_VARIABLES_INTER.split(",")
df_pattern_features = pd.DataFrame.empty
df_state_features = pd.DataFrame.empty
df_count_features = pd.DataFrame.empty


# read the csv log file and put it in a dataframe
df_structured_logs = pd.read_json(STRUCTURED_FILE,orient='split')
file = wget.download("https://s3.us-east-2.amazonaws.com/activeontry/result.csv")
df_structured_logs = pd.read_csv(file)

#data preprocessing to add a new column containg date and time
df_structured_logs['date_time'] = df_structured_logs['date'] +' '+ df_structured_logs['time']
df_structured_logs['date_time'] = pd.to_datetime(df_structured_logs['date_time'])

pattern_number = int(df_structured_logs[PATTERN_COLUMN].max())

#is usefull when there is multiple identifiers in a single row
def id_extraction(session_col=None):
  ids_list = session_col.split(' ')
  return ids_list


#Parse the dataframe row by row to update the feature vector of each block
feature_vector = []
dict_block_features = {}
variables_name = list(df_structured_logs)
state_features_names = []
dict_states = {}
dict_variables_set = {}
dict_variables_blk = {}
dict_block_features_state_1 = {}
dict_block_features_state = {}
detected_blocks = []


for i in range (len(STATE_VARIABLES)):
  variables_count = df_structured_logs[STATE_VARIABLES[i]].value_counts()
  for j in range(len(variables_count.keys())):
    dict_states[STATE_VARIABLES[i]]=variables_count.keys()
    state_features_names.append(variables_count.keys()[j])

start_date_time_period = parse(START_DATE_TIME)
start_date_time_period = pd.to_datetime(start_date_time_period)
print(start_date_time_period)
print ("step date preview")
end_date = start_date_time_period + timedelta(minutes=WINDOW_SIZE)

nb_timestamps = 0
while not df_structured_logs[df_structured_logs.date_time > start_date_time_period].empty:
  df_features = pd.DataFrame.empty
  nb_timestamps = nb_timestamps + 1
  print(nb_timestamps)
  end_date_time_period = start_date_time_period + timedelta(minutes=WINDOW_SIZE)
  df_structured_logs_filtered_time = df_structured_logs[(start_date_time_period<df_structured_logs.date_time) & (df_structured_logs.date_time<=end_date_time_period)]
  print("=====================================================")
  print(str(start_date_time_period)+' '+ str(end_date_time_period))
  print("=====================================================")
  label = str(start_date_time_period)+' '+ str(end_date_time_period)
  timestamps_label.append(label)
  #df_structured_logs.query("date_time<'end_date_time_period' & date_time>='end_date_time_period'")
  for index,row in df_structured_logs_filtered_time.iterrows():
    #current_date_time = parse(row['date']+' '+row['time'])
    ids_list = id_extraction(row[SESSION_COLUMN])

#===================================== Features (count pattern) =================================
    if PATTERNS_COUNT_FEATURES=='True':
      j = int(row[PATTERN_COLUMN]-1)
      for i in range(len(ids_list)):
        # update existing entry
        if ids_list[i] in dict_block_features:
          features = dict_block_features.get(ids_list[i])
          features[j] = features[j] + 1
          dict_block_features[ids_list[i]] = features
        # add new entry
        else:
          feature_vector = [0] * pattern_number
          feature_vector[j] = feature_vector[j] + 1
          dict_block_features[ids_list[i]] = feature_vector
#===================================== Features (count state + variables) ======================================
    if STATE_COUNT_FEATURES_VARIABLES=='True':
      for f in range(len(ids_list)):
        # update existing entry
        if ids_list[f] in dict_block_features_state:
          features_count = dict_block_features_state.get(ids_list[f])
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]] == dict_states[STATE_VARIABLES[i]][j]:
                features_count[m] = features_count[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = features_count
            m = m+1

          for h in range (len(COUNT_VARIABLES)):
            if row[COUNT_VARIABLES[h]] not in dict_variables_blk[ids_list[f]][COUNT_VARIABLES[h]]:
              dict_variables_blk[ids_list[f]][COUNT_VARIABLES[h]].append(row[COUNT_VARIABLES[h]])
              features_count[m] = features_count[m] + 1
              dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = features_count
        # add new entry
        else:
          feature_vector_state_variables = [0]*(len(state_features_names)+len(COUNT_VARIABLES))
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]]==dict_states[STATE_VARIABLES[i]][j]:
                feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
                dict_block_features_state[ids_list[f]] = feature_vector_state_variables
              m = m+1

          for h in range (len(COUNT_VARIABLES)):
            dict_variables_set[COUNT_VARIABLES[h]] = []
            dict_variables_set[COUNT_VARIABLES[h]].append(row[COUNT_VARIABLES[h]])
            feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
            dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = feature_vector_state_variables
            dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
            m = m+1
          dict_variables_blk[ids_list[f]] = dict_variables_set

  start_date_time_period = start_date_time_period + timedelta(minutes=SLIDING_STEP)
  frames = []
  if PATTERNS_COUNT_FEATURES=='True':
    features = dict_block_features.values()
    block_ids = dict_block_features.keys()
    df_pattern_features = pd.DataFrame(dict_block_features, index = ["pattern "+ str(i) for i in range(1,pattern_number+1)]).T
    frames.append(df_pattern_features)
  if STATE_COUNT_FEATURES_VARIABLES=='True':
    df_state_features = pd.DataFrame(dict_block_features_state_1,index = state_features_names+COUNT_VARIABLES).T
    frames.append(df_state_features)
  #frames = [df_pattern_features, df_state_features]
  df_features = pd.concat(frames, axis=1)
  nb_features = len(df_features.columns)
  feature_vector_null = [0]*nb_features
  time_series_null.append(feature_vector_null)
  for index,row in df_features.iterrows():
    #update the time series vector of an existing id
    if index in final_timeseries_dict:
      timeseries_feature_vector = final_timeseries_dict.get(index)
      if len(timeseries_feature_vector)<nb_timestamps-1:
        for i in range(nb_timestamps-len(timeseries_feature_vector)-2):
          final_timeseries_dict[index].append(feature_vector_null)
      final_timeseries_dict[index].append(list(row))
    #create a new time_series_vector of a never treated id
    else:
      time_series_vector_new = []
      time_series_vector_new.append(time_series_null)
      time_series_vector_new.append(list(row))
      final_timeseries_dict[index] =time_series_vector_new
print("je suis ici")
#padd with zero vectors the final resulted dictionary
#toadd
for key, value in final_timeseries_dict.items():
  while len(final_timeseries_dict[key])<nb_timestamps:
    final_timeseries_dict[key].append(feature_vector_null)
print(final_timeseries_dict)

print("END Time Series Feature Extraction")
]]>
					</code>
				</script>
			</scriptExecutable>
			<controlFlow block="none"></controlFlow>
		</task>
	</taskFlow>
</job>