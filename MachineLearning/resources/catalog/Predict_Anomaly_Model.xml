<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.10" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Predict_Anomaly_Model" onTaskError="continueJobExecution" priority="normal" projectName="7. Predict" xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd">
  <description>
    <![CDATA[ Generate predictions using a trained model. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="machine-learning"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
<info name="Documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_predict_anomaly_model"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task name="Predict_Anomaly_Model">
      <description>
        <![CDATA[ Generate predictions using a trained model. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_predict_anomaly_model"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Predict_Anomaly_Model")

import os
import pickle
import pandas as pd

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn import metrics


MODEL_BIN = variables.get("MODEL")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
is_oneclass_algorithm = variables.get("DATA_TEST_DF_JSON")
IS_UNSUPERVIDED_ALGORITHM = variables.get("UNSUPERVISED_ALGORITHM")
ALGORITHM_NAME = variables.get("ALGORITHM_NAME")


# ONE-CLASS SVM
if ALGORITHM_NAME == 'OneClassSVM' and MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON, orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data  = loaded_model.predict(data_test_df.values)
  predict_data_df = pd.DataFrame(predict_data)
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  
  # ONE-CLASS MEASURE MEASURES 
  try: 
    is_oneclass_algorithm = variables.get("ONECLASS_MEASURE")  
    LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")   

    if is_oneclass_algorithm == 'True' and LABEL_TEST_DF_JSON != None:  
      label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')
      print("**********************CLASSIFICATION MEASURES**********************")
      accuracy_score_result = metrics.accuracy_score(label_test_df.values.ravel(), predict_data)
      precision_score_result = metrics.precision_score(label_test_df.values.ravel(), predict_data, average='micro')
      recall_score_result = metrics.recall_score(label_test_df.values.ravel(), predict_data, average='micro')
      f1_score_result = metrics.f1_score(label_test_df.values.ravel(), predict_data, average='micro') 
      auc_score_result = metrics.roc_auc_score(label_test_df.values.ravel(), predict_data, average='micro')       
      print("ACCURACY SCORE: %.2f" % accuracy_score_result)
      print("PRECISION SCORE: %.2f" % precision_score_result)
      print("RECALL SCORE:\n%s" %  recall_score_result)
      print("F1 SCORE:\n%s" %  f1_score_result)     
      print("AREA UNDER CURVE(AUC):\n%s" %  auc_score_result)         
      print("*********************************************************************************")
   
  except NameError:
    print(predict_data_df)
  print("END Predict_Anomaly_Model")    
  
# LOCAL OUTLIER FACTOR MEASURES  
elif ALGORITHM_NAME == 'LocalOutlierFactor' and MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON, orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data  = loaded_model.fit_predict(data_test_df.values)
  predict_data_df = pd.DataFrame(predict_data)
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  
  # LOCAL OUTLIER FACTOR MEASURES
  try: 
    is_oneclass_algorithm = variables.get("ONECLASS_MEASURE") 
    LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON") 
    
    if is_oneclass_algorithm == 'True' and LABEL_TEST_DF_JSON != None:     
      label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')  
      print("**********************CLASSIFICATION MEASURES**********************")
      accuracy_score_result = metrics.accuracy_score(label_test_df.values.ravel(), predict_data)
      precision_score_result = metrics.precision_score(label_test_df.values.ravel(), predict_data, average='micro')
      recall_score_result = metrics.recall_score(label_test_df.values.ravel(), predict_data, average='micro')
      f1_score_result = metrics.f1_score(label_test_df.values.ravel(), predict_data, average='micro') 
      auc_score_result = metrics.roc_auc_score(label_test_df.values.ravel(), predict_data, average='micro')       
      print("ACCURACY SCORE: %.2f" % accuracy_score_result)
      print("PRECISION SCORE: %.2f" % precision_score_result)
      print("RECALL SCORE:\n%s" %  recall_score_result)
      print("F1 SCORE:\n%s" %  f1_score_result)     
      print("AREA UNDER CURVE(AUC):\n%s" %  auc_score_result)         
      print("*********************************************************************************")
   
  except NameError:
    print(predict_data_df)
  print("END Predict_Anomaly_Model")    
  
else:
  print('Please check your ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
  </taskFlow>
</job>
