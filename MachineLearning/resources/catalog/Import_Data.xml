<?xml version="1.0" encoding="UTF-8"?>
<job xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="urn:proactive:jobdescriptor:3.8"
	xsi:schemaLocation="urn:proactive:jobdescriptor:3.8 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.8/schedulerjob.xsd"
	name="Import_Data" projectName="2. Input and Output Data" priority="normal"
	onTaskError="continueJobExecution" maxNumberOfExecution="2">
	<description>
    <![CDATA[ Load data from external sources. ]]>
  </description>
	<genericInformation>
		<info name="bucketName" value="machine-learning" />
		<info name="pca.action.icon"
			value="/automation-dashboard/styles/patterns/img/wf-icons/import_data.png" />
		<info name="Documentation"
			value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_import_data" />
		<info name="group" value="public-objects" />
	</genericInformation>
	<taskFlow>
		<task name="Import_Data">
			<description>
        <![CDATA[ Load data from external sources. ]]>
      </description>
			<variables>
				<variable name="FILE_URL"
					value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/pima-indians-diabetes.csv"
					inherited="false" />
				<variable name="FILE_DELIMITER" value=";" inherited="false" />
				<variable name="IS_LABELED_DATA" value="True" inherited="false" />
			</variables>
			<genericInformation>
				<info name="task.icon"
					value="/automation-dashboard/styles/patterns/img/wf-icons/import_data.png" />
			</genericInformation>
			<forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
				<envScript>
					<script>
						<code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
						</code>
					</script>
				</envScript>
			</forkEnvironment>
			<scriptExecutable>
				<script>
					<code language="cpython">
            <![CDATA[
print("BEGIN Import_Data")

import pandas as pd
import numpy as np

URL_GET = "https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/pima-indians-diabetes.csv"
FILE_SEP = ";"
IS_LABELED_DATA = 'True'
try:
  URL_GET = str(variables.get("FILE_URL"))
  FILE_SEP = str(variables.get("FILE_DELIMITER"))
  IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
except NameError:
  pass

dataframe = pd.read_csv(URL_GET,FILE_SEP)
columns_name = dataframe.columns
columns_number = len(columns_name)

if IS_LABELED_DATA == 'True':
  data  = dataframe.values[:,0:columns_number-1]
  label = dataframe.values[:,columns_number-1]
  
  data_df = pd.DataFrame(data=data,columns=columns_name[0:columns_number-1])
  label_df = pd.DataFrame(data=label,columns=[columns_name[columns_number-1]])
  
  DATAFRAME_JSON = dataframe.to_json(orient='split')
  COLUMNS_NAME_JSON = pd.Series(columns_name).to_json()
  DATA_TRAIN_DF_JSON = data_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_df.to_json(orient='split')
  LABEL_TRAIN_DF_JSON = label_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_df.to_json(orient='split')
  
  try:
    variables.put("DATAFRAME_JSON", DATAFRAME_JSON)
    variables.put("COLUMNS_NAME_JSON", COLUMNS_NAME_JSON)
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
    variables.put("IS_LABELED_DATA", IS_LABELED_DATA)
    
    #**************Preview Data*********************
    result = dataframe.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")
    #***********************************************
  except NameError:
    pass
  
  print("END Import_Data")
    
elif IS_LABELED_DATA == 'False':
  data = dataframe.values
  data_df = pd.DataFrame(data=data,columns=columns_name)
  
  DATAFRAME_JSON = dataframe.to_json(orient='split')
  COLUMNS_NAME_JSON = pd.Series(columns_name).to_json()
  DATA_TRAIN_DF_JSON = data_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_df.to_json(orient='split')
  
  try:
    variables.put("DATAFRAME_JSON", DATAFRAME_JSON)
    variables.put("COLUMNS_NAME_JSON", COLUMNS_NAME_JSON)
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("IS_LABELED_DATA", IS_LABELED_DATA)
    
    #**************Preview Data*********************
    result = dataframe.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "output.html")
    resultMetadata.put("content.type", "text/html")
    #***********************************************
  except NameError:
    pass
  
  print("END Import_Data")
else:
  print('The import data is failure')
]]>
					</code>
				</script>
			</scriptExecutable>
			<controlFlow block="none"></controlFlow>
			<post>
				<script>
					<code language="groovy">
            <![CDATA[
variables.put("PREVIOUS_PA_TASK_NAME", variables.get("PA_TASK_NAME"))
]]>
					</code>
				</script>
			</post>
		</task>
	</taskFlow>
</job>