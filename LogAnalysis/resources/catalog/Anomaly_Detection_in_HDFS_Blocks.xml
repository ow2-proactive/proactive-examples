<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.9"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.9 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.9/schedulerjob.xsd"
    name="Anomaly_Detection_in_HDFS_Blocks" projectName="Log Analysis"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2"
>
  <variables>
    <variable name="instance_name" value="visdom-server-1" />
  </variables>
  <description>
    <![CDATA[ Detect anomalies in blocks reported in HDFS logs ]]>
  </description>
    <genericInformation>
    <info name="bucketName" value="machine-learning-workflows"/>
    <info name="pca.action.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_analysis.png"/>
    <info name="Documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_log_analysis"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Log_Parser">
      <description>
        <![CDATA[ Extracts a group of event templates, whereby raw logs can be structured. ]]>
      </description>
      <variables>
        <variable name="LOG_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/HDFS_2k.log" inherited="false" />
        <variable name="PATTERNS_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/patterns.csv" inherited="false" />
        <variable name="STRUCTURED_LOG_FILE" value="HTML" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_parser.png"/>
      </genericInformation>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN LOG PARSING")

import pandas as pd
import numpy as np
import wget
import re
import time as clock
from datetime import datetime, timedelta
from time import gmtime, strftime
from argparse import ArgumentParser
from collections import OrderedDict

TIME_FORMAT = '%H%M%S'
DATE_FORMAT = '%d%m%Y'
PATTERNS_FILE = variables.get("PATTERNS_FILE")
LOG_FILE = variables.get("LOG_FILE")
STRUCTURED_LOG_FILE = variables.get("STRUCTURED_LOG_FILE")

INTERVAL   = 10000
LOG_FILE = wget.download(LOG_FILE)
PATTERN_FILE = PATTERNS_FILE
STRUCTERED_LOG_FILE = STRUCTURED_LOG_FILE
#===================================== Detect the different patterns =================================
print("Reading the Pattern_file")
df_patterns = pd.read_csv(PATTERN_FILE, sep = ';')
df_columns = pd.Series([''])
table = list()
for index, row in df_patterns.iterrows():
  for e in row[2].split(','):
    if e.strip() != '*':
      table.append(e.strip())
table.append('pattern_id')
myList = list(OrderedDict.fromkeys(table))
df_columns = pd.Series(myList)
print("The different patterns included in the Pattern_file were extracted")
#===================================== Parse raw logs =================================   
df_structured_logs = pd.DataFrame(columns = df_columns)
print("Processing " + LOG_FILE)
k = 0
t = clock.time()
#variables = list()
my_dict = OrderedDict()
print("Logs patterns matching is in progress")
with open(LOG_FILE) as infile:
  for line in infile:
    k = k + 1
    if k % INTERVAL == 0:
      elapsed_time = clock.time() - t
      print(str(k) + " " + str(elapsed_time) + "sec " + line)
    for index,variable_name in df_columns.iteritems():
      vide = np.nan
      my_dict.__setitem__(variable_name.strip(),vide)
    for index, row in df_patterns.iterrows():
      p = row[1]
      pattern = re.compile(p, re.IGNORECASE)
      m = pattern.match(line)
      if m:
        #print('Match found: ', m.group())
        i = 0
        for e in row[2].split(','):
          i = i+1
          if e.strip() != '*':
            var = m.group(i)
            if e.strip() == "date":
              if len(e.strip())<8:
                if len(var)==5:
                  idx=3
                elif len(var)==6:
                  idx=4
                str1_split1 = var[:idx]
                str1_split2 = var[idx:]
                tranformed_date =  str1_split1 + '20' + str1_split2
                my_dict.__setitem__(e.strip(),datetime.strptime(tranformed_date, DATE_FORMAT))
              else:
                my_dict.__setitem__(e.strip(),datetime.strptime(var, DATE_FORMAT))
            elif e.strip() == "time":
              my_dict.__setitem__(e.strip(),datetime.strptime(var.strip(), TIME_FORMAT).time())
            else:
              my_dict.__setitem__(e.strip(),repr(var.strip()).strip("0"))
              my_dict.__setitem__('pattern_id', int(row[0]))
        break
    df_inter = pd.DataFrame([my_dict.values()], columns=df_columns)
    df_structured_logs = df_structured_logs.append(df_inter, ignore_index=True)
        
print("All logs were matched")
#===================================== Preview results =================================
STRUCTURED_LOG_FILE=STRUCTURED_LOG_FILE.lower()
if STRUCTURED_LOG_FILE.endswith('csv'):
  result = df_structured_logs.to_csv()
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", result+".csv")
  resultMetadata.put("content.type", "text/csv")
elif STRUCTURED_LOG_FILE.endswith('html'):
  result = df_structured_logs.to_html()
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", result+".html")
  resultMetadata.put("content.type", "text/html") 
else:
  print('Your data is empty')
  
#===================================== Save the linked variables =================================    
df_json_logs = df_structured_logs.to_json(orient='split')
    
print("Finshed " + LOG_FILE + "PARSING")

variables.put("DATAFRAME_JSON", df_json_logs) 

print("END LOG PARSING")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Feature_Vector_Extractor">
      <description>
        <![CDATA[ Encodes structured data into numerical feature vectors whereby machine learning models can be applied. ]]>
      </description>
      <variables>
        <variable name="SESSION_COLUMN" value="id_block" inherited="false" />
        <variable name="FILE_OUT_FEATURES" value="HTML" inherited="false" />
        <variable name="PATTERN_COLUMN" value="pattern_id" inherited="false" />
        <variable name="PATTERNS_COUNT_FEATURES" value="False" inherited="false" />
        <variable name="STATE_VARIABLES" value="status,date" inherited="false" />
        <variable name="COUNT_VARIABLES" value="ip_from,ip_to,pid,date,time" inherited="false" />
        <variable name="STATE_COUNT_FEATURES_VARIABLES" value="True" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/feature_extraction.png"/>
      </genericInformation>
      <depends>
        <task ref="Log_Parser"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN FEATURE VECTOR EXTRACTION")
import pandas as pd
import numpy as np

SESSION_COLUMN = variables.get("SESSION_COLUMN")
DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
FILE_OUT_FEATURES = variables.get("FILE_OUT_FEATURES")
PATTERN_COLUMN = variables.get("PATTERN_COLUMN")
PATTERNS_COUNT_FEATURES = variables.get("PATTERNS_COUNT_FEATURES")
STATE_COUNT_FEATURES_VARIABLES = variables.get("STATE_COUNT_FEATURES_VARIABLES")
STRUCTURED_LOGS = variables.get("DATAFRAME_JSON")
#===================================== Extract variables =================================
STATE_VARIABLES_INTER = variables.get("STATE_VARIABLES")
COUNT_VARIABLES_INTER = variables.get("COUNT_VARIABLES")
STATE_VARIABLES = STATE_VARIABLES_INTER.split(",")
COUNT_VARIABLES = COUNT_VARIABLES_INTER.split(",")
print("State Variables:")
print(STATE_VARIABLES)
print("Count Variables:")
print(COUNT_VARIABLES)

df_pattern_features = pd.DataFrame.empty
df_state_features = pd.DataFrame.empty
df_count_features = pd.DataFrame.empty

df_structured_logs  = pd.read_json(DATAFRAME_JSON,orient='split')
pattern_number = int(df_structured_logs[PATTERN_COLUMN].max())

#is usefull when there is multiple identifiers in a single row
def id_extraction(session_col=None):
  session_col = str(session_col)
  ids_list = session_col.split(' ')
  return ids_list

feature_vector = []
dict_block_features = {}
variables_name = list(df_structured_logs)
state_features_names = []
dict_states = {}
#dict_variables_set = {}
dict_variables_blk = {}
dict_block_features_state = {}
dict_block_features_state_1 = {}
dict_variables_set = {}

#===================================== Extract the state variables =================================
for i in range (len(STATE_VARIABLES)):
    variables_count = df_structured_logs[STATE_VARIABLES[i]].value_counts()
    for j in range(len(variables_count.keys())):
      dict_states[STATE_VARIABLES[i]]=variables_count.keys()
      state_features_names.append(variables_count.keys()[j])
#     del dict_states["''"]

for index,row in df_structured_logs.iterrows():
  if not(row[SESSION_COLUMN] == None):
    ids_list = id_extraction(row[SESSION_COLUMN])

#===================================== Features (count pattern) =================================
    if PATTERNS_COUNT_FEATURES=='True':
      j = int(row[PATTERN_COLUMN]-1)
      for i in range(len(ids_list)):
        #dict_variables_blk = {}
        # update existing entry
        if ids_list[i] in dict_block_features:
          features = dict_block_features.get(ids_list[i])
          features[j] = features[j] + 1
          dict_block_features[ids_list[i]] = features
        # add new entry
        else:
          feature_vector = [0] * pattern_number
          feature_vector[j] = feature_vector[j] + 1
          dict_block_features[ids_list[i]] = feature_vector
                
#===================================== Features (count state + variables) ======================================
    if STATE_COUNT_FEATURES_VARIABLES=='True':
      for f in range(len(ids_list)):
        # update existing entry
        if ids_list[f] in dict_block_features_state_1:
          features_count = dict_block_features_state_1.get(ids_list[f])
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]] == dict_states[STATE_VARIABLES[i]][j]:
                features_count[m] = features_count[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = features_count
                dict_block_features_state_1[ids_list[f]] = features_count
              m = m+1

          for h in range (len(COUNT_VARIABLES)):
            table_of_variable = dict_variables_blk[ids_list[f]].get(COUNT_VARIABLES[h])
            if (str(row[COUNT_VARIABLES[h]]) not in table_of_variable) and not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_blk[ids_list[f]][COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              features_count[m] = features_count[m] + 1
              dict_block_features_state_1[ids_list[f]] = features_count
            m = m+1

        # add new entry
        else:
          feature_vector_state_variables = [0]*(len(state_features_names)+len(COUNT_VARIABLES))
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]]==dict_states[STATE_VARIABLES[i]][j]:
                feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = feature_vector_state_variables
                dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
              m = m+1
          dict_variables_set_1 = {}
          for h in range (len(COUNT_VARIABLES)):
            dict_variables_set_1[COUNT_VARIABLES[h]] = []
            if not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_set_1[COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
            m = m+1
            dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
            dict_variables_blk[ids_list[f]] = dict_variables_set_1

#===================================== Save the different features in a dataframe ======================================
frames = []
if PATTERNS_COUNT_FEATURES=='True':
  features = dict_block_features.values()
  block_ids = dict_block_features.keys()
  df_pattern_features = pd.DataFrame(dict_block_features, index = ["pattern "+ str(i) for i in range(1,pattern_number+1)]).T
  frames.append(df_pattern_features)
if STATE_COUNT_FEATURES_VARIABLES=='True':
  df_state_features = pd.DataFrame(dict_block_features_state_1,index = state_features_names+COUNT_VARIABLES).T
  frames.append(df_state_features)
if not frames:
  df_features = pd.DataFrame.empty
  print("ERROR: No features extracted, check your input variables")
else:
  df_features = pd.concat(frames, axis=1)

#===================================== Preview results =================================
  if FILE_OUT_FEATURES.endswith('.csv'):
    result = df_features.to_csv()
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", result+".csv")
    resultMetadata.put("content.type", "text/csv")
  elif FILE_OUT_FEATURES.endswith('.html'):
    result = df_features.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", result+".html")
    resultMetadata.put("content.type", "text/html") 
  else:
    print('Your data is empty')

#===================================== Save the linked variables =================================  
  columns_name = df_features.columns
  df_json_features = df_features.to_json(orient='split')
  variables.put("DATA_TRAIN_DF_JSON", df_json_features)
  variables.put("DATA_TEST_DF_JSON", df_json_features) 
  variables.put("DATAFRAME_JSON", df_json_features)
  variables.put("COLUMNS_NAME_JSON", pd.Series(columns_name).to_json()) 

print("END FEATURE VECTOR EXTRACTION")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Train_Clustering_Model">
      <description>
        <![CDATA[ Train a clustering model. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/train.png"/>
      </genericInformation>
      <depends>
        <task ref="K_Means"/>
        <task ref="Split_Data"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Train_Custering_Model")

from time import time
import pandas as pd
import numpy as np
import pickle

IS_CLUSTERING_ALGORITHM = variables.get("CLUSTERING_ALGORITHM")
DATA_TRAIN_DF_JSON = variables.get("DATA_TRAIN_DF_JSON")

if IS_CLUSTERING_ALGORITHM == 'True' and DATA_TRAIN_DF_JSON != None:
  ALGORITHM_NAME = variables.get("ALGORITHM_NAME")
  data_train_df = pd.read_json(DATA_TRAIN_DF_JSON,  orient='split')

  # CLUSTERING LEARNING
  if ALGORITHM_NAME == 'KMeans':
    from sklearn.cluster import KMeans
    n_clusters_para = variables.get("N_CLUSTERS_PARA")
    max_iter_para = variables.get("MAX_ITERATIONS_PARA")
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = KMeans(n_clusters= int(n_clusters_para), max_iter= int(max_iter_para), n_jobs = int(n_jobs_para))

  if ALGORITHM_NAME == 'MeanShift':
    from sklearn.cluster import MeanShift
    cluster_all_para = variables.get("CLUSTER_ALL_PARA")
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = MeanShift(cluster_all = cluster_all_para, n_jobs = int(n_jobs_para))
  
  model.fit(data_train_df.values)
  model_bin = pickle.dumps(model)
  variables.put("MODEL", model_bin)
 
  print("END Train_Custering_Model")
  
else:
  print('Please check your ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="K_Means">
      <description>
        <![CDATA[ Kmeans clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. ]]>
      </description>
      <variables>
        <variable name="N_CLUSTERS" value="2" inherited="false" />
        <variable name="MAX_ITERATIONS" value="300" inherited="false" />
        <variable name="N_JOBS" value="1" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/ml_clustering.png"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
N_CLUSTERS = variables.get("N_CLUSTERS")
MAX_ITERATIONS = variables.get("MAX_ITERATIONS")
N_JOBS = variables.get("N_JOBS")

variables.put("N_CLUSTERS_PARA", N_CLUSTERS)
variables.put("MAX_ITERATIONS_PARA", MAX_ITERATIONS)
variables.put("N_JOBS_PARA", N_JOBS)
variables.put("ALGORITHM_NAME", "KMeans")
variables.put("CLUSTERING_ALGORITHM", "True")
variables.put("CLUSTERING_MEASURE", "True")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Predict_Clustering_Model">
      <description>
        <![CDATA[ Generate predictions using a trained model. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
      </genericInformation>
      <depends>
        <task ref="Train_Clustering_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Predict_Custering_Model")

import os
import pickle
import pandas as pd
from sklearn.metrics.cluster import adjusted_mutual_info_score
from sklearn.metrics.cluster import completeness_score
from sklearn.metrics.cluster import homogeneity_score
from sklearn.metrics.cluster import v_measure_score
from sklearn.metrics import mutual_info_score  

IS_CLUSTERING_ALGORITHM = variables.get("CLUSTERING_ALGORITHM")
MODEL_BIN = variables.get("MODEL")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")

if MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON,orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data = list(loaded_model.predict(data_test_df.values))
  predict_data_df = pd.DataFrame(predict_data)

  # CLUSTERING MEASURES
  try:
    DATA_LABEL_DF_JSON = variables.get("LABEL_TEST_DF_JSON")
    if IS_CLUSTERING_ALGORITHM == 'True' and DATA_LABEL_DF_JSON  != None:
      print("**********************CLUSTERING MEASURES**********************")
      label_test_df  = pd.read_json(DATA_LABEL_DF_JSON, orient='split')
      adjusted_mutual_info_score_result = adjusted_mutual_info_score(label_test_df.values.ravel(), predict_data)
      completeness_score_result = completeness_score(label_test_df.values.ravel(), predict_data)
      homogeneity_score_result = homogeneity_score(label_test_df.values.ravel(), predict_data)
      mutual_info_score_result = mutual_info_score(label_test_df.values.ravel(), predict_data)
      v_measure_score_result = v_measure_score(label_test_df.values.ravel(), predict_data)
      print("ADJUSTED MUTUAL INFORMATION: %.2f" % adjusted_mutual_info_score_result)
      print("COMPLETENESS SCORE: %.2f" % completeness_score_result)
      print("HOMOGENEITY METRIC: %.2f" % homogeneity_score_result)
      print("MUTUAL INFORMATION: %.2f" % mutual_info_score_result)
      print("V-MEASURE CLUSTER MEASURE: %.2f" % v_measure_score_result)
      print("******************************************************************************")

  except NameError:
    IS_CLUSTERING_ALGORITHM = None
    DATA_LABEL_DF_JSON = None
  
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  print("END Predict_Custering_Model")

else:
  print('Please check your ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Export_Results">
      <description>
        <![CDATA[ Export the results. ]]>
      </description>
      <variables>
        <variable name="OUTPUT_FILE" value="HTML" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Clustering_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Export_Results")

import pandas as pd
import numpy as np

OUTPUT_FILE = variables.get("OUTPUT_FILE")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
PREDICT_DATA = variables.get("PREDICT_DATA_JSON")

if DATA_TEST_DF_JSON != None and PREDICT_DATA != None: 
  data_test_df  = pd.read_json(DATA_TEST_DF_JSON, orient='split')   
  predict_data  = pd.read_json(PREDICT_DATA, orient='split')    
  frame_prediction = pd.DataFrame(predict_data)    
  prediction_result = data_test_df.assign(predictions=frame_prediction.values)
  prediction_result = prediction_result.sort_index(ascending=True) 
  
  if OUTPUT_FILE == 'CSV':
    result = prediction_result.to_csv()
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", "result.csv")
    resultMetadata.put("content.type", "text/csv")
  elif OUTPUT_FILE == 'HTML':
    result = prediction_result.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "result.html")
    resultMetadata.put("content.type", "text/html")          
  print("END Export_Results")  
else:
  print('It is not possible to export the data')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Bind_or_Start_Visdom_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ The simplest task, ran by a groovy engine. ]]>
      </description>
      <variables>
        <variable name="service_model" value="http://models.activeeon.com/pca/visdom" inherited="false" />
        <variable name="instance_name" value="visdom-server-1" inherited="true" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <inputFiles>
        <files  includes="cloud-automation-service-client-1.0.0-all.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-1.0.0-all.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import org.ow2.proactive.pca.client.api.CloudAutomationApi;

schedulerapi.connect()

pcaUrl = variables.get("PA_SCHEDULER_REST_URL").replaceAll("/rest\\z", "/cloud-automation-service");
sessionId = schedulerapi.getSession();

api = new CloudAutomationApi(pcaUrl, sessionId);

endpoint = api.getServiceEndpointOrCreate(variables.get("service_model"), variables.get("instance_name"));
endpoint = endpoint.replaceAll("\n",'');

println "Service " + variables.get("service_model") + " is available on " + endpoint

variables.put("endpoint", endpoint)
result = '<meta http-equiv="refresh" content="1; url=http://' + endpoint + '/" />'
result+= '<h2><span style="color:black">Please wait while redirecting...</span></h2>'
resultMetadata.put("content.type", "text/html")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Visdom_Visualize_Results">
      <description>
        <![CDATA[ Plot the different results obtained by a predictive model using Visdom ]]>
      </description>
      <variables>
        <variable name="TARGETED_CLASS" value="1" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Clustering_Model"/>
        <task ref="Bind_or_Start_Visdom_Service"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre/" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN DATA VISUALIZATION")

import pandas as pd
import numpy as np
from visdom import Visdom
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import *

DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
PREDICT_DATA = variables.get("PREDICT_DATA_JSON")
IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
TARGETED_CLASS = str(variables.get("TARGETED_CLASS"))
VISDOM_HOST,VISDOM_PORT = variables.get("endpoint").split(":")
print("Connecting to %s" % VISDOM_PORT)
vis = Visdom(server="http://"+VISDOM_HOST,port=int(VISDOM_PORT))
assert vis.check_connection()
data_test  = pd.read_json(DATA_TEST_DF_JSON, orient='split')   
predict_data_df  = pd.read_json(PREDICT_DATA, orient='split')  
idx_test_df = data_test.index.values
data_test_df_columns = list(data_test.columns.values)
data_test_df = data_test.values
nb_columns = data_test_df.shape[1]
PREDICTION_LIMIT = data_test_df.shape[0]
predict_df = predict_data_df.iloc[:,-1]
predict_data = [str(predict_df[x]) for x in range(PREDICTION_LIMIT)]

try:
  is_classification_algorithm = variables.get("CLASSIFICATION_MEASURE")
except NameError:
    is_classification_algorithm = 'False'

if is_classification_algorithm == 'True':
  LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")
  label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')
  df_classes = label_test_df.iloc[:,-1].unique()
else:
  df_classes = predict_data_df.iloc[:,-1].unique()

classes = [str(df_classes[x]) for x in range(0,len(df_classes))]
classes.sort()
classes_stats = [0]*len(classes)

  
# REGRESSION PLOTS
try:
  is_regression_algorithm = variables.get("REGRESSION_MEASURE")
except NameError:
    is_regression_algorithm = 'False'

if is_regression_algorithm == 'True':
  list_detected_simples_text = vis.text("List of detected samples in class "+TARGETED_CLASS+" :\n", opts = dict(title = 'List of indexes'))
  targeted_class_line = vis.line(Y = np.array([0]), X = np.array([0]), opts = dict(xlabel = 'Index', ylabel = 'Targeted Class', title = "Detected samples in class "+TARGETED_CLASS+" \n"))
  features_line = vis.line(X=np.column_stack(([0]*nb_columns)), Y=np.column_stack(np.asarray(data_test_df[0][:])), opts=dict(legend=data_test_df_columns,xlabel = 'Index', ylabel = 'Feature Value', title = 'Values of extracted features for each sample'))
  count = 0 
  for x in range(PREDICTION_LIMIT):
    if int(float(predict_data[x]))==int(float(TARGETED_CLASS)):
      message = "%s\n"%(idx_test_df[x])
      vis.text(message, win=list_detected_simples_text, append=True)
      vis.line(Y = np.array([1]), X = np.array([count]), win = targeted_class_line, update = 'append')
    else:
      vis.line(Y = np.array([0]), X = np.array([count]), win = targeted_class_line, update = 'append')
      count += 1
    vis.line(X=np.column_stack(([count]*nb_columns)), Y=np.column_stack((np.asarray(data_test_df[x][:]))), win=features_line, update='append')
else:
  list_detected_simples_text = vis.text("List of detected samples in class "+TARGETED_CLASS+" :\n", opts = dict(title = 'List of indexes'))
  targeted_class_line = vis.line(Y = np.array([0]), X = np.array([0]), opts = dict(xlabel = 'Index', ylabel = 'Targeted Class', title = "Detected samples in class "+TARGETED_CLASS+" \n"))
  statistic_pie = vis.pie(X=classes_stats, opts=dict(legend=classes,  title = 'Detected classes',))
  features_line = vis.line(X=np.column_stack(([0]*nb_columns)), Y=np.column_stack(np.asarray(data_test_df[0][:])), opts=dict(legend=data_test_df_columns,xlabel = 'Index', ylabel = 'Feature Value', title = 'Values of extracted features for each sample'))
  count = 0
  for x in range(PREDICTION_LIMIT):
    i=0
    while i<len(classes):
      if predict_data[x] == classes[i]:
        classes_stats[i] += 1
        if classes[i]== TARGETED_CLASS:
          message = "%s\n"%(idx_test_df[x])
          vis.text(message, win=list_detected_simples_text, append=True)
          vis.line(Y = np.array([1]), X = np.array([count]), win = targeted_class_line, update = 'append')
        else:
          vis.line(Y = np.array([0]), X = np.array([count]), win = targeted_class_line, update = 'append')
        count += 1
        vis.pie(X = classes_stats, win = statistic_pie, opts=dict(legend=classes,  title = 'Classification results'))
        i = len(classes)
      i+= 1
    vis.line(X=np.column_stack(([count]*nb_columns)), Y=np.column_stack((np.asarray(data_test_df[x][:]))), win=features_line, update='append')

try:
  is_clustering_algorithm = variables.get("CLUSTERING_MEASURE")
except NameError:
    is_clustering_algorithm = 'False' 
  
if IS_LABELED_DATA == 'True' and not(is_clustering_algorithm == 'True'):
  LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")
  label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split') 
  score_text = vis.text("Model scoring:\n")
  if len(classes)==2:
    fpr, tpr, thresholds = metrics.roc_curve(np.asarray(label_test_df), np.asarray(predict_data_df))
    vis.line(X=fpr, Y=tpr, opts=dict(xlabel = 'False Positive Rate', ylabel = 'True Positive Rate', title = 'ROC Curve'))
  # CLASSIFICATION MEASURES
  try:
    is_classification_algorithm = variables.get("CLASSIFICATION_MEASURE")
    if is_classification_algorithm == 'True':
      X_matrix = confusion_matrix(label_test_df, predict_data_df)
      confusion_bar = vis.bar(X = X_matrix, opts=dict(stacked=True, legend=classes, rownames=classes, title = 'Predictive model performance'))
      vis.text("CLASSIFICATION MEASURES", win=score_text, append=True)
      vis.text("ACCURACY SCORE:", win=score_text, append=True)
      vis.text(str(accuracy_score(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
      vis.text("PRECISION SCORE:", win=score_text, append=True)
      vis.text(str(precision_score(label_test_df.values.ravel(), predict_data_df,average='micro')), win=score_text, append=True)
      vis.text("CONFUSION MATRIX:", win=score_text, append=True)
      vis.text(str(confusion_matrix(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
  except NameError:
    classification_algorithm = None

  # REGRESSION MEASURES
  try:
    is_regression_algorithm = variables.get("REGRESSION_MEASURE")
    if is_regression_algorithm == 'True':
      vis.text("REGRESSION MEASURES", win=score_text, append=True)
      vis.text("MEAN SQUARED ERROR:", win=score_text, append=True)
      vis.text(str(mean_squared_error(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
      vis.text("MEAN ABSOLUTE ERROR:", win=score_text, append=True)
      vis.text(str(mean_absolute_error(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
      vis.text("R2 SCORE:", win=score_text, append=True)
      vis.text(str(r2_score(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
  except NameError:        
    is_regression_algorithm = None
   
print("END DATA VISUALIZATION")
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Wait_Until_Validation"
    
    
    onTaskError="pauseJob" >
      <description>
        <![CDATA[ Task to pause the job and send a validation message to the notification service ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Visdom_Visualize_Results"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="python">
            <![CDATA[
# Please fill variables
notification_message = 'Please validate to terminate the Visdom service'

# Don't change code below unless you know what you are doing
from org.ow2.proactive.addons.webhook import Webhook

jobid = variables.get("PA_JOB_ID")
schedulerURL =  variables.get("PA_SCHEDULER_REST_URL")

print schedulerURL
# get sessionid
schedulerapi.connect()

# pause job
schedulerapi.pauseJob(jobid)


# send web validation
print "Sending web validation..."
url = schedulerURL.replace("/rest", "") +'/notification-service/notifications'
headers = '{\"Content-Type\" : \"application/json\" }'
notification_content = '{\"description\": \"'+notification_message+'\", \"jobId\": \"'+jobid+'\" , \"validation\": \"true\"}'
Webhook.execute ( 'POST', url, headers, notification_content);
print "Web Validation sent"
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Terminate_Visdom_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ The simplest task, ran by a groovy engine. ]]>
      </description>
      <variables>
        <variable name="service_model" value="http://models.activeeon.com/pca/visdom" inherited="false" />
        <variable name="instance_name" value="visdom-server-1" inherited="true" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
      </genericInformation>
      <depends>
        <task ref="Wait_Until_Validation"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-1.0.0-all.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-1.0.0-all.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
import org.ow2.proactive.pca.client.api.CloudAutomationApi;

schedulerapi.connect()
pcaUrl = variables.get("PA_SCHEDULER_REST_URL").replaceAll("/rest\\z", "/cloud-automation-service");
sessionId = schedulerapi.getSession();

api = new CloudAutomationApi(pcaUrl, sessionId);

println "Terminating " + variables.get("service_model") + " / " + variables.get("instance_name");
try{
  api.terminateServiceFromInstanceName(variables.get("service_model"), variables.get("instance_name"));
}catch(Exception ex){
  println "Service was already terminated"
}
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Split_Data">
      <description>
        <![CDATA[ Divide the data into two sets. ]]>
      </description>
      <variables>
        <variable name="TRAIN_SIZE" value="0.7" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/split_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Feature_Vector_Extractor"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Split_Data")

from sklearn import model_selection
import pandas as pd

TRAIN_SIZE = 0.7
try:
  IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
  TRAIN_SIZE = float(variables.get("TRAIN_SIZE"))
except NameError:
  pass
test_size = 1 - TRAIN_SIZE

if IS_LABELED_DATA == 'True':
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
    
  data = dataframe.values[:,0:columns_number-1]
  label = dataframe.values[:,columns_number-1]
  indice = dataframe.index.values
  
  data_train, data_test, label_train, label_test, idx_train, idx_test = model_selection.train_test_split(data, label, indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name[0:columns_number-1],index=idx_train)
  label_train_df = pd.DataFrame(data=label_train,columns=[columns_name[columns_number-1]])
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name[0:columns_number-1],index=idx_test)
  label_test_df = pd.DataFrame(data=label_test,columns=[columns_name[columns_number-1]])
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  LABEL_TRAIN_DF_JSON = label_train_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
  except NameError:
    pass
  
  print("END Split_Data")
  
elif IS_LABELED_DATA == 'False' or IS_LABELED_DATA == None:
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
  indice = dataframe.index.values
  data = dataframe.values
  
  data_train, data_test, idx_train, idx_test = model_selection.train_test_split(data,indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name,index=idx_train)
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name,index=idx_test)
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
  except NameError:
    pass
  
  print("END Split_Data")
else:
  print('The data could not be split, please check ypur ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>