<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.10"
     xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd"
    name="Anomaly_Detection_in_HDFS_Blocks" projectName="Log Analysis"
    priority="normal"
    onTaskError="continueJobExecution"
     maxNumberOfExecution="2"
>
  <variables>
    <variable name="DOCKER_ENABLED" value="True" model="PA:Boolean"/>
    <variable name="visdom_instance_name" value="visdom-server-1" />
  </variables>
  <description>
    <![CDATA[ Detect anomalies in blocks reported in HDFS logs ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="machine-learning-workflows"/>
    <info name="Documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_log_analysis"/>
    <info name="group" value="public-objects"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_analysis.png"/>
  </genericInformation>
  <taskFlow>
    <task name="Log_Parser">
      <description>
        <![CDATA[ Extracts a group of event templates, whereby raw logs can be structured. ]]>
      </description>
      <variables>
        <variable name="LOG_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/HDFS_2k.log" inherited="false" />
        <variable name="PATTERNS_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/patterns.csv" inherited="false" />
        <variable name="STRUCTURED_LOG_FILE" value="HTML" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_parser.png"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Log_Parser ---")

import pandas as pd
import numpy as np
import wget
import re
import time as clock
from datetime import datetime, timedelta
from time import gmtime, strftime
from argparse import ArgumentParser
from collections import OrderedDict

TIME_FORMAT = '%H%M%S'
DATE_FORMAT = '%d%m%Y'
PATTERNS_FILE = variables.get("PATTERNS_FILE")
LOG_FILE = variables.get("LOG_FILE")
STRUCTURED_LOG_FILE = variables.get("STRUCTURED_LOG_FILE")

INTERVAL   = 10000
LOG_FILE = wget.download(LOG_FILE)
PATTERN_FILE = PATTERNS_FILE
STRUCTERED_LOG_FILE = STRUCTURED_LOG_FILE
#===================================== Detect the different patterns =================================
print("Reading the Pattern_file")
df_patterns = pd.read_csv(PATTERN_FILE, sep = ';')
df_columns = pd.Series([''])
table = list()
for index, row in df_patterns.iterrows():
  for e in row[2].split(','):
    if e.strip() != '*':
      table.append(e.strip())
table.append('pattern_id')
myList = list(OrderedDict.fromkeys(table))
df_columns = pd.Series(myList)
print("The different patterns included in the Pattern_file were extracted")
#===================================== Parse raw logs =================================   
df_structured_logs = pd.DataFrame(columns = df_columns)
print("Processing " + LOG_FILE)
k = 0
t = clock.time()
#variables = list()
my_dict = OrderedDict()
print("Logs patterns matching is in progress")
with open(LOG_FILE) as infile:
  for line in infile:
    k = k + 1
    if k % INTERVAL == 0:
      elapsed_time = clock.time() - t
      print(str(k) + " " + str(elapsed_time) + "sec " + line)
    for index,variable_name in df_columns.iteritems():
      vide = np.nan
      my_dict.__setitem__(variable_name.strip(),vide)
    for index, row in df_patterns.iterrows():
      p = row[1]
      pattern = re.compile(p, re.IGNORECASE)
      m = pattern.match(line)
      if m:
        #print('Match found: ', m.group())
        i = 0
        for e in row[2].split(','):
          i = i+1
          if e.strip() != '*':
            var = m.group(i)
            if e.strip() == "date":
              if len(e.strip())<8:
                if len(var)==5:
                  idx=3
                elif len(var)==6:
                  idx=4
                str1_split1 = var[:idx]
                str1_split2 = var[idx:]
                tranformed_date =  str1_split1 + '20' + str1_split2
                my_dict.__setitem__(e.strip(),datetime.strptime(tranformed_date, DATE_FORMAT))
              else:
                my_dict.__setitem__(e.strip(),datetime.strptime(var, DATE_FORMAT))
            elif e.strip() == "time":
              my_dict.__setitem__(e.strip(),datetime.strptime(var.strip(), TIME_FORMAT).time())
            else:
              my_dict.__setitem__(e.strip(),repr(var.strip()).strip("0"))
              my_dict.__setitem__('pattern_id', int(row[0]))
        break
    df_inter = pd.DataFrame([my_dict.values()], columns=df_columns)
    df_structured_logs = df_structured_logs.append(df_inter, ignore_index=True)
        
print("All logs were matched")
#===================================== Preview results =================================
STRUCTURED_LOG_FILE=STRUCTURED_LOG_FILE.lower()
if STRUCTURED_LOG_FILE.endswith('csv'):
  result = df_structured_logs.to_csv()
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", result+".csv")
  resultMetadata.put("content.type", "text/csv")
elif STRUCTURED_LOG_FILE.endswith('html'):
  result = df_structured_logs.to_html()
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", result+".html")
  resultMetadata.put("content.type", "text/html") 
else:
  print('Your data is empty')
#===================================== Save the linked variables =================================    
df_json_logs = df_structured_logs.to_json(orient='split')
    
print("Finshed " + LOG_FILE + "PARSING")

variables.put("DATAFRAME_JSON", df_json_logs) 

print("--- END Log_Parser ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Feature_Vector_Extractor">
      <description>
        <![CDATA[ Encodes structured data into numerical feature vectors whereby machine learning models can be applied. ]]>
      </description>
      <variables>
        <variable name="SESSION_COLUMN" value="id_block" inherited="false" />
        <variable name="FILE_OUT_FEATURES" value="HTML" inherited="false" />
        <variable name="PATTERN_COLUMN" value="pattern_id" inherited="false" />
        <variable name="PATTERNS_COUNT_FEATURES" value="False" inherited="false" model="PA:Boolean"/>
        <variable name="STATE_VARIABLES" value="status,date" inherited="false" />
        <variable name="COUNT_VARIABLES" value="ip_from,ip_to,pid,date,time" inherited="false" />
        <variable name="STATE_COUNT_FEATURES_VARIABLES" value="True" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/feature_extraction.png"/>
      </genericInformation>
      <depends>
        <task ref="Log_Parser"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Feature_Vector_Extractor ---")

import pandas as pd
import numpy as np

SESSION_COLUMN = variables.get("SESSION_COLUMN")
DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
FILE_OUT_FEATURES = variables.get("FILE_OUT_FEATURES")
PATTERN_COLUMN = variables.get("PATTERN_COLUMN")
PATTERNS_COUNT_FEATURES = variables.get("PATTERNS_COUNT_FEATURES")
STATE_COUNT_FEATURES_VARIABLES = variables.get("STATE_COUNT_FEATURES_VARIABLES")
STRUCTURED_LOGS = variables.get("DATAFRAME_JSON")
#===================================== Extract variables =================================
STATE_VARIABLES_INTER = variables.get("STATE_VARIABLES")
COUNT_VARIABLES_INTER = variables.get("COUNT_VARIABLES")
STATE_VARIABLES = STATE_VARIABLES_INTER.split(",")
COUNT_VARIABLES = COUNT_VARIABLES_INTER.split(",")
print("State Variables:")
print(STATE_VARIABLES)
print("Count Variables:")
print(COUNT_VARIABLES)

df_pattern_features = pd.DataFrame.empty
df_state_features = pd.DataFrame.empty
df_count_features = pd.DataFrame.empty

df_structured_logs  = pd.read_json(DATAFRAME_JSON,orient='split')
pattern_number = int(df_structured_logs[PATTERN_COLUMN].max())

#is usefull when there is multiple identifiers in a single row
def id_extraction(session_col=None):
  session_col = str(session_col)
  ids_list = session_col.split(' ')
  return ids_list

feature_vector = []
dict_block_features = {}
variables_name = list(df_structured_logs)
state_features_names = []
dict_states = {}
#dict_variables_set = {}
dict_variables_blk = {}
dict_block_features_state = {}
dict_block_features_state_1 = {}
dict_variables_set = {}

#===================================== Extract the state variables =================================
for i in range (len(STATE_VARIABLES)):
    variables_count = df_structured_logs[STATE_VARIABLES[i]].value_counts()
    for j in range(len(variables_count.keys())):
      dict_states[STATE_VARIABLES[i]]=variables_count.keys()
      state_features_names.append(variables_count.keys()[j])
#     del dict_states["''"]

for index,row in df_structured_logs.iterrows():
  if not(row[SESSION_COLUMN] == None):
    ids_list = id_extraction(row[SESSION_COLUMN])

#===================================== Features (count pattern) =================================
    if PATTERNS_COUNT_FEATURES=='True':
      j = int(row[PATTERN_COLUMN]-1)
      for i in range(len(ids_list)):
        #dict_variables_blk = {}
        # update existing entry
        if ids_list[i] in dict_block_features:
          features = dict_block_features.get(ids_list[i])
          features[j] = features[j] + 1
          dict_block_features[ids_list[i]] = features
        # add new entry
        else:
          feature_vector = [0] * pattern_number
          feature_vector[j] = feature_vector[j] + 1
          dict_block_features[ids_list[i]] = feature_vector
                
#===================================== Features (count state + variables) ======================================
    if STATE_COUNT_FEATURES_VARIABLES=='True':
      for f in range(len(ids_list)):
        # update existing entry
        if ids_list[f] in dict_block_features_state_1:
          features_count = dict_block_features_state_1.get(ids_list[f])
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]] == dict_states[STATE_VARIABLES[i]][j]:
                features_count[m] = features_count[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = features_count
                dict_block_features_state_1[ids_list[f]] = features_count
              m = m+1

          for h in range (len(COUNT_VARIABLES)):
            table_of_variable = dict_variables_blk[ids_list[f]].get(COUNT_VARIABLES[h])
            if (str(row[COUNT_VARIABLES[h]]) not in table_of_variable) and not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_blk[ids_list[f]][COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              features_count[m] = features_count[m] + 1
              dict_block_features_state_1[ids_list[f]] = features_count
            m = m+1

        # add new entry
        else:
          feature_vector_state_variables = [0]*(len(state_features_names)+len(COUNT_VARIABLES))
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]]==dict_states[STATE_VARIABLES[i]][j]:
                feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = feature_vector_state_variables
                dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
              m = m+1
          dict_variables_set_1 = {}
          for h in range (len(COUNT_VARIABLES)):
            dict_variables_set_1[COUNT_VARIABLES[h]] = []
            if not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_set_1[COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
            m = m+1
            dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
            dict_variables_blk[ids_list[f]] = dict_variables_set_1

#===================================== Save the different features in a dataframe ======================================
frames = []
if PATTERNS_COUNT_FEATURES=='True':
  features = dict_block_features.values()
  block_ids = dict_block_features.keys()
  df_pattern_features = pd.DataFrame(dict_block_features, index = ["pattern "+ str(i) for i in range(1,pattern_number+1)]).T
  frames.append(df_pattern_features)
if STATE_COUNT_FEATURES_VARIABLES=='True':
  df_state_features = pd.DataFrame(dict_block_features_state_1,index = state_features_names+COUNT_VARIABLES).T
  frames.append(df_state_features)
if not frames:
  df_features = pd.DataFrame.empty
  print("ERROR: No features extracted, check your input variables")
else:
  df_features = pd.concat(frames, axis=1)

#===================================== Preview results =================================
  if FILE_OUT_FEATURES.endswith('.csv'):
    result = df_features.to_csv()
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", result+".csv")
    resultMetadata.put("content.type", "text/csv")
  elif FILE_OUT_FEATURES.endswith('.html'):
    result = df_features.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", result+".html")
    resultMetadata.put("content.type", "text/html") 
  else:
    print('Your data is empty')

#===================================== Save the linked variables =================================  
  columns_name = df_features.columns
  df_json_features = df_features.to_json(orient='split')
  variables.put("DATA_TRAIN_DF_JSON", df_json_features)
  variables.put("DATA_TEST_DF_JSON", df_json_features) 
  variables.put("DATAFRAME_JSON", df_json_features)
  variables.put("COLUMNS_NAME_JSON", pd.Series(columns_name).to_json()) 

print("--- END Feature_Vector_Extractor ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Train_Clustering_Model">
      <description>
        <![CDATA[ Train a clustering model. ]]>
      </description>
      <variables>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/train.png"/>
      </genericInformation>
      <depends>
        <task ref="K_Means"/>
        <task ref="Split_Data"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Train_Custering_Model ---")

from time import time
import pandas as pd
import numpy as np
import pickle

IS_CLUSTERING_ALGORITHM = variables.get("CLUSTERING_ALGORITHM")
DATA_TRAIN_DF_JSON = variables.get("DATA_TRAIN_DF_JSON")

if IS_CLUSTERING_ALGORITHM == 'True' and DATA_TRAIN_DF_JSON != None:
  ALGORITHM_NAME = variables.get("ALGORITHM_NAME")
  data_train_df = pd.read_json(DATA_TRAIN_DF_JSON, orient='split')

  # CLUSTERING LEARNING
  if ALGORITHM_NAME == 'KMeans':
    from sklearn.cluster import KMeans
    n_clusters_para = variables.get("N_CLUSTERS_PARA")
    max_iter_para = variables.get("MAX_ITERATIONS_PARA")
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = KMeans(n_clusters= int(n_clusters_para), max_iter= int(max_iter_para), n_jobs = int(n_jobs_para))

  if ALGORITHM_NAME == 'MeanShift':
    from sklearn.cluster import MeanShift
    cluster_all_para = variables.get("CLUSTER_ALL_PARA")
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = MeanShift(cluster_all = cluster_all_para, n_jobs = int(n_jobs_para))
  
  model.fit(data_train_df.values)
  model_bin = pickle.dumps(model)
  variables.put("MODEL", model_bin)
else:
  print('Please check your ML pipeline')

print("--- END Train_Custering_Model ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="K_Means">
      <description>
        <![CDATA[ Kmeans clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. ]]>
      </description>
      <variables>
        <variable name="N_CLUSTERS" value="2" inherited="false" />
        <variable name="MAX_ITERATIONS" value="300" inherited="false" />
        <variable name="N_JOBS" value="1" inherited="false" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/ml_clustering.png"/>
      </genericInformation>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN K_Means ---")

N_CLUSTERS = variables.get("N_CLUSTERS")
MAX_ITERATIONS = variables.get("MAX_ITERATIONS")
N_JOBS = variables.get("N_JOBS")

variables.put("N_CLUSTERS_PARA", N_CLUSTERS)
variables.put("MAX_ITERATIONS_PARA", MAX_ITERATIONS)
variables.put("N_JOBS_PARA", N_JOBS)
variables.put("ALGORITHM_NAME", "KMeans")
variables.put("CLUSTERING_ALGORITHM", "True")
variables.put("CLUSTERING_MEASURE", "True")

print("--- END K_Means ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Predict_Clustering_Model">
      <description>
        <![CDATA[ Generate predictions using a trained model. ]]>
      </description>
      <variables>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
      </genericInformation>
      <depends>
        <task ref="Train_Clustering_Model"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Predict_Custering_Model ---")

import os
import pickle
import pandas as pd

from sklearn.metrics.cluster import adjusted_mutual_info_score
from sklearn.metrics.cluster import completeness_score
from sklearn.metrics.cluster import homogeneity_score
from sklearn.metrics.cluster import v_measure_score
from sklearn.metrics import mutual_info_score  

IS_CLUSTERING_ALGORITHM = variables.get("CLUSTERING_ALGORITHM")
MODEL_BIN = variables.get("MODEL")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")

if MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON,orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data = list(loaded_model.predict(data_test_df.values))
  predict_data_df = pd.DataFrame(predict_data)

  # CLUSTERING MEASURES
  try:
    DATA_LABEL_DF_JSON = variables.get("LABEL_TEST_DF_JSON")
    if IS_CLUSTERING_ALGORITHM == 'True' and DATA_LABEL_DF_JSON  != None:
      print("**********************CLUSTERING MEASURES**********************")
      label_test_df  = pd.read_json(DATA_LABEL_DF_JSON, orient='split')
      adjusted_mutual_info_score_result = adjusted_mutual_info_score(label_test_df.values.ravel(), predict_data)
      completeness_score_result = completeness_score(label_test_df.values.ravel(), predict_data)
      homogeneity_score_result = homogeneity_score(label_test_df.values.ravel(), predict_data)
      mutual_info_score_result = mutual_info_score(label_test_df.values.ravel(), predict_data)
      v_measure_score_result = v_measure_score(label_test_df.values.ravel(), predict_data)
      print("ADJUSTED MUTUAL INFORMATION: %.2f" % adjusted_mutual_info_score_result)
      print("COMPLETENESS SCORE: %.2f" % completeness_score_result)
      print("HOMOGENEITY METRIC: %.2f" % homogeneity_score_result)
      print("MUTUAL INFORMATION: %.2f" % mutual_info_score_result)
      print("V-MEASURE CLUSTER MEASURE: %.2f" % v_measure_score_result)
      print("******************************************************************************")
  except NameError:
    IS_CLUSTERING_ALGORITHM = None
    DATA_LABEL_DF_JSON = None
  
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
else:
  print('Please check your ML pipeline')

print("--- END Predict_Custering_Model ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Export_Results">
      <description>
        <![CDATA[ Export the results. ]]>
      </description>
      <variables>
        <variable name="OUTPUT_FILE" value="HTML" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Clustering_Model"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Export_Results ---")

import pandas as pd
import numpy as np

OUTPUT_FILE = variables.get("OUTPUT_FILE")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
PREDICT_DATA = variables.get("PREDICT_DATA_JSON")

if DATA_TEST_DF_JSON != None and PREDICT_DATA != None: 
  data_test_df  = pd.read_json(DATA_TEST_DF_JSON, orient='split')   
  predict_data  = pd.read_json(PREDICT_DATA, orient='split')    
  frame_prediction = pd.DataFrame(predict_data)    
  prediction_result = data_test_df.assign(predictions=frame_prediction.values)
  prediction_result = prediction_result.sort_index(ascending=True) 
  
  if OUTPUT_FILE == 'CSV':
    result = prediction_result.to_csv()
    resultMetadata.put("file.extension", ".csv")
    resultMetadata.put("file.name", "result.csv")
    resultMetadata.put("content.type", "text/csv")
  elif OUTPUT_FILE == 'HTML':
    result = prediction_result.to_html()
    resultMetadata.put("file.extension", ".html")
    resultMetadata.put("file.name", "result.html")
    resultMetadata.put("content.type", "text/html") 
else:
  print('It is not possible to export the data')

print("--- END Export_Results ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Split_Data">
      <description>
        <![CDATA[ Divide the data into two sets. ]]>
      </description>
      <variables>
        <variable name="TRAIN_SIZE" value="0.7" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/split_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Feature_Vector_Extractor"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Split_Data ---")

from sklearn import model_selection
import pandas as pd

TRAIN_SIZE = 0.7
try:
  IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
  TRAIN_SIZE = float(variables.get("TRAIN_SIZE"))
except NameError:
  pass
test_size = 1 - TRAIN_SIZE

if IS_LABELED_DATA == 'True':
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
    
  data = dataframe.values[:,0:columns_number-1]
  label = dataframe.values[:,columns_number-1]
  indice = dataframe.index.values
  
  data_train, data_test, label_train, label_test, idx_train, idx_test = model_selection.train_test_split(data, label, indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name[0:columns_number-1],index=idx_train)
  label_train_df = pd.DataFrame(data=label_train,columns=[columns_name[columns_number-1]])
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name[0:columns_number-1],index=idx_test)
  label_test_df = pd.DataFrame(data=label_test,columns=[columns_name[columns_number-1]])
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  LABEL_TRAIN_DF_JSON = label_train_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
  except NameError:
    pass
  
elif IS_LABELED_DATA == 'False' or IS_LABELED_DATA == None:
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
  indice = dataframe.index.values
  data = dataframe.values
  
  data_train, data_test, idx_train, idx_test = model_selection.train_test_split(data,indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name,index=idx_train)
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name,index=idx_test)
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
  except NameError:
    pass
else:
  print('The data could not be split, please check ypur ML pipeline')

print("--- END Split_Data ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Start_Visdom_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ Start the Visdom server as a service. ]]>
      </description>
      <variables>
        <variable name="visdom_service_id" value="Visdom" inherited="false" />
        <variable name="visdom_instance_name" value="visdom-server-1" inherited="true" />
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html"/>
      </genericInformation>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Start_Visdom_Service ---")

org.ow2.proactive.pca.service.client.ApiClient
org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
org.ow2.proactive.pca.service.client.model.ServiceInstanceData
org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

// Check existing service instances
def service_id = variables.get("visdom_service_id")
def instance_name = variables.get("visdom_instance_name")
println("*_service_id:    " + service_id)
println("*_instance_name: " + instance_name)

boolean instance_exists = false
List<ServiceInstanceData> service_instances = service_instance_rest_api.getServiceInstancesUsingGET()

for (ServiceInstanceData service_instance_data : service_instances) {
	if ( (service_instance_data.getServiceId() == service_id) && (service_instance_data.getInstanceStatus()  == "RUNNING")){
      if (service_instance_data.getVariables().get("INSTANCE_NAME") == instance_name) {
        instance_exists = true
        instance_id = service_instance_data.getInstanceId()
  		endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
        println("*_instance_id: " + instance_id)
        println("*_endpoint:    " + endpoint)
        variables.put("visdom_instance_id", instance_id)
        variables.put("visdom_endpoint", endpoint)
        break
      }
  	}
}

println("instance_exists: " + instance_exists)

if (!instance_exists){
  // Prepare service description
  ServiceDescription serviceDescription = new ServiceDescription()
  serviceDescription.setBucketName("cloud-automation")
  serviceDescription.setWorkflowName(service_id) 
  serviceDescription.putVariablesItem("INSTANCE_NAME", instance_name)
  
  // Run service
  def service_instance_data = service_instance_rest_api.createRunningServiceInstanceUsingPOST(session_id, serviceDescription)
  
  // Acquire service Instance ID
  def service_instance_id = service_instance_data.getInstanceId()
  println("service_instance_id: " + service_instance_id)
  
  // Create synchro channel
  channel = "Service_Instance_" + service_instance_id
  println("channel: " + channel)
  synchronizationapi.createChannelIfAbsent(channel, false)
  synchronizationapi.waitUntil(channel, "RUNNING", "{k,x -> x == true}")
  
  // Acquire service endpoint
  service_instance_data = service_instance_rest_api.getServiceInstanceUsingGET(service_instance_id)
  instance_name = service_instance_data.getVariables().get("INSTANCE_NAME")
  instance_id = service_instance_data.getInstanceId()
  endpoint = service_instance_data.getInstanceEndpoints().entrySet().iterator().next().getValue()
  
  println("INSTANCE_NAME: " + instance_name)
  println("*_instance_id: " + instance_id)
  println("*_endpoint: " + endpoint)
  
  variables.put("visdom_instance_id", instance_id)
  variables.put("visdom_endpoint", endpoint)
  
  result = '<meta http-equiv="refresh" content="1; url=' + endpoint + '" />'
  result+= '<h2><span style="color:black">Please wait while redirecting...</span></h2>'
  resultMetadata.put("content.type", "text/html")
}

println("--- END Start_Visdom_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Visdom_Visualize_Results">
      <description>
        <![CDATA[ Plot the different results obtained by a predictive model using Visdom ]]>
      </description>
      <variables>
        <variable name="TARGETED_CLASS" value="1" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_visdom_visualize_results"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Clustering_Model"/>
        <task ref="Start_Visdom_Service"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Visdom_Visualize_Results ---")

import pandas as pd
import numpy as np
from visdom import Visdom
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import *

DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
PREDICT_DATA = variables.get("PREDICT_DATA_JSON")
IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
TARGETED_CLASS = str(variables.get("TARGETED_CLASS"))

visdom_endpoint = variables.get("visdom_endpoint")
if visdom_endpoint is not None:
  visdom_endpoint = visdom_endpoint.replace("http://", "")
(VISDOM_HOST, VISDOM_PORT) = visdom_endpoint.split(":")
print("Connecting to %s:%s" % (VISDOM_HOST, VISDOM_PORT))
vis = Visdom(server="http://"+VISDOM_HOST,port=int(VISDOM_PORT))
assert vis.check_connection()

data_test  = pd.read_json(DATA_TEST_DF_JSON, orient='split')   
predict_data_df  = pd.read_json(PREDICT_DATA, orient='split')  
idx_test_df = data_test.index.values
data_test_df_columns = list(data_test.columns.values)
data_test_df = data_test.values
nb_columns = data_test_df.shape[1]
PREDICTION_LIMIT = data_test_df.shape[0]
predict_df = predict_data_df.iloc[:,-1]
predict_data = [str(predict_df[x]) for x in range(PREDICTION_LIMIT)]

try:
  is_classification_algorithm = variables.get("CLASSIFICATION_MEASURE")
except NameError:
  is_classification_algorithm = 'False'

if is_classification_algorithm == 'True':
  LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")
  label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')
  df_classes = label_test_df.iloc[:,-1].unique()
else:
  df_classes = predict_data_df.iloc[:,-1].unique()

classes = [str(df_classes[x]) for x in range(0,len(df_classes))]
classes.sort()
classes_stats = [0]*len(classes)

# REGRESSION PLOTS
try:
  is_regression_algorithm = variables.get("REGRESSION_MEASURE")
except NameError:
  is_regression_algorithm = 'False'

if is_regression_algorithm == 'True':
  list_detected_simples_text = vis.text("List of detected samples in class "+TARGETED_CLASS+" :\n", opts = dict(title = 'List of indexes'))
  targeted_class_line = vis.line(Y = np.array([0]), X = np.array([0]), opts = dict(xlabel = 'Index', ylabel = 'Targeted Class', title = "Detected samples in class "+TARGETED_CLASS+" \n"))
  features_line = vis.line(X=np.column_stack(([0]*nb_columns)), Y=np.column_stack(np.asarray(data_test_df[0][:])), opts=dict(legend=data_test_df_columns,xlabel = 'Index', ylabel = 'Feature Value', title = 'Values of extracted features for each sample'))
  count = 0 
  for x in range(PREDICTION_LIMIT):
    if int(float(predict_data[x]))==int(float(TARGETED_CLASS)):
      message = "%s\n"%(idx_test_df[x])
      vis.text(message, win=list_detected_simples_text, append=True)
      vis.line(Y = np.array([1]), X = np.array([count]), win = targeted_class_line, update = 'append')
    else:
      vis.line(Y = np.array([0]), X = np.array([count]), win = targeted_class_line, update = 'append')
      count += 1
    vis.line(X=np.column_stack(([count]*nb_columns)), Y=np.column_stack((np.asarray(data_test_df[x][:]))), win=features_line, update='append')
else:
  list_detected_simples_text = vis.text("List of detected samples in class "+TARGETED_CLASS+" :\n", opts = dict(title = 'List of indexes'))
  targeted_class_line = vis.line(Y = np.array([0]), X = np.array([0]), opts = dict(xlabel = 'Index', ylabel = 'Targeted Class', title = "Detected samples in class "+TARGETED_CLASS+" \n"))
  statistic_pie = vis.pie(X=classes_stats, opts=dict(legend=classes,  title = 'Detected classes',))
  features_line = vis.line(X=np.column_stack(([0]*nb_columns)), Y=np.column_stack(np.asarray(data_test_df[0][:])), opts=dict(legend=data_test_df_columns,xlabel = 'Index', ylabel = 'Feature Value', title = 'Values of extracted features for each sample'))
  count = 0
  for x in range(PREDICTION_LIMIT):
    i=0
    while i<len(classes):
      if predict_data[x] == classes[i]:
        classes_stats[i] += 1
        if classes[i]== TARGETED_CLASS:
          message = "%s\n"%(idx_test_df[x])
          vis.text(message, win=list_detected_simples_text, append=True)
          vis.line(Y = np.array([1]), X = np.array([count]), win = targeted_class_line, update = 'append')
        else:
          vis.line(Y = np.array([0]), X = np.array([count]), win = targeted_class_line, update = 'append')
        count += 1
        vis.pie(X = classes_stats, win = statistic_pie, opts=dict(legend=classes,  title = 'Classification results'))
        i = len(classes)
      i+= 1
    vis.line(X=np.column_stack(([count]*nb_columns)), Y=np.column_stack((np.asarray(data_test_df[x][:]))), win=features_line, update='append')

try:
  is_clustering_algorithm = variables.get("CLUSTERING_MEASURE")
except NameError:
  is_clustering_algorithm = 'False' 
  
if IS_LABELED_DATA == 'True' and not(is_clustering_algorithm == 'True'):
  LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")
  label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split') 
  score_text = vis.text("Model scoring:\n")
  if len(classes)==2:
    fpr, tpr, thresholds = metrics.roc_curve(np.asarray(label_test_df), np.asarray(predict_data_df))
    vis.line(X=fpr, Y=tpr, opts=dict(xlabel = 'False Positive Rate', ylabel = 'True Positive Rate', title = 'ROC Curve'))
  # CLASSIFICATION MEASURES
  try:
    is_classification_algorithm = variables.get("CLASSIFICATION_MEASURE")
    if is_classification_algorithm == 'True':
      X_matrix = confusion_matrix(label_test_df, predict_data_df)
      confusion_bar = vis.bar(X = X_matrix, opts=dict(stacked=True, legend=classes, rownames=classes, title = 'Predictive model performance'))
      vis.text("CLASSIFICATION MEASURES", win=score_text, append=True)
      vis.text("ACCURACY SCORE:", win=score_text, append=True)
      vis.text(str(accuracy_score(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
      vis.text("PRECISION SCORE:", win=score_text, append=True)
      vis.text(str(precision_score(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
      vis.text("CONFUSION MATRIX:", win=score_text, append=True)
      vis.text(str(confusion_matrix(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
  except NameError:
    classification_algorithm = None

  # REGRESSION MEASURES
  try:
    is_regression_algorithm = variables.get("REGRESSION_MEASURE")
    if is_regression_algorithm == 'True':
      vis.text("REGRESSION MEASURES", win=score_text, append=True)
      vis.text("MEAN SQUARED ERROR:", win=score_text, append=True)
      vis.text(str(mean_squared_error(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
      vis.text("MEAN ABSOLUTE ERROR:", win=score_text, append=True)
      vis.text(str(mean_absolute_error(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
      vis.text("COEFFICIENT DE DETERMINATION:", win=score_text, append=True)
      vis.text(str(r2_score(label_test_df.values.ravel(), predict_data_df)), win=score_text, append=True)
  except NameError:        
    is_regression_algorithm = None

print("--- END Visdom_Visualize_Results ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
    <task name="Wait_For_Web_Validation"
    
    
    onTaskError="pauseJob" >
      <description>
        <![CDATA[ Task to pause the job and send a validation message to the notification service ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html"/>
      </genericInformation>
      <depends>
        <task ref="Visdom_Visualize_Results"/>
      </depends>
      <scriptExecutable>
        <script>
          <code language="python">
            <![CDATA[
# Please fill variables
notification_message = 'Please, confirm to stop the Visdom service'

# Don't change code below unless you know what you are doing
from org.ow2.proactive.addons.webhook import Webhook

jobid = variables.get("PA_JOB_ID")
userName = variables.get("PA_USER")
schedulerURL =  variables.get("PA_SCHEDULER_REST_URL")

print schedulerURL
# get sessionid
schedulerapi.connect()

# pause job
schedulerapi.pauseJob(jobid)

# send web validation
print "Sending web validation..."
url = schedulerURL.replace("/rest", "") +'/notification-service/notifications'
headers = '{\"Content-Type\" : \"application/json\" }'
notification_content = '{\"description\": \"'+notification_message+'\", \"jobId\": \"'+jobid+'\" , \"validation\": \"true\", \"userName\":  \"'+userName+'\"}'
Webhook.execute ( 'POST', url, headers, notification_content);
print "Web Validation sent"
]]>
          </code>
        </script>
      </scriptExecutable>
    </task>
    <task name="Finish_Visdom_Service"
    
    
    onTaskError="cancelJob" >
      <description>
        <![CDATA[ Finish the Visdom service. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/visdom.png"/>
        <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html"/>
      </genericInformation>
      <depends>
        <task ref="Wait_For_Web_Validation"/>
      </depends>
      <inputFiles>
        <files  includes="cloud-automation-service-client-8.2.0-SNAPSHOT.jar" accessMode="transferFromGlobalSpace"/>
      </inputFiles>
      <forkEnvironment >
        <additionalClasspath>
          <pathElement path="cloud-automation-service-client-8.2.0-SNAPSHOT.jar"/>
        </additionalClasspath>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
println("--- BEGIN Finish_Visdom_Service ---")

org.ow2.proactive.pca.service.client.ApiClient
org.ow2.proactive.pca.service.client.api.ServiceInstanceRestApi
org.ow2.proactive.pca.service.client.model.ServiceInstanceData
org.ow2.proactive.pca.service.client.model.ServiceDescription

// Get schedulerapi access
schedulerapi.connect()

// Acquire session id
def session_id = schedulerapi.getSession()

// Define PCA URL
def scheduler_rest_url = variables.get("PA_SCHEDULER_REST_URL")
def pca_url = scheduler_rest_url.replaceAll("/rest\\z", "/cloud-automation-service")

// Connect to APIs
def api_client = new ApiClient()
api_client.setBasePath(pca_url)
//api_client.setDebugging(true)
def service_instance_rest_api = new ServiceInstanceRestApi(api_client)

instance_id = variables.get("visdom_instance_id")
println("*_instance_id: " + instance_id)
assert instance_id != null

// Finish service
ServiceDescription service = new ServiceDescription()
service.setBucketName("cloud-automation") 
service.setWorkflowName("Finish_Visdom")
service_instance_rest_api.launchServiceInstanceActionUsingPUT(session_id, instance_id, service)

println("--- END Finish_Visdom_Service ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"></controlFlow>
    </task>
  </taskFlow>
</job>