<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.11" xsi:schemaLocation="urn:proactive:jobdescriptor:3.11 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.11/schedulerjob.xsd"  name="Unsupervised_Anomaly_Detection" projectName="2. Log Analysis" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2" >
  <variables>
    <variable name="DOCKER_ENABLED" value="True" model="PA:Boolean"/>
    <variable name="instance_name" value="visdom-server-1" />
  </variables>
  <description>
    <![CDATA[ Detect anomalies using an Unsupervised One-Class SVM. ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="machine-learning-workflows"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_analysis.png"/>
    <info name="Documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_log_analysis"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="Log_Parser" >
      <description>
        <![CDATA[ Extracts a group of event templates, whereby raw logs can be structured. ]]>
      </description>
      <variables>
        <variable name="LOG_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/HDFS_2k.log" inherited="false" />
        <variable name="PATTERNS_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/patterns.csv" inherited="false" />
        <variable name="STRUCTURED_LOG_FILE" value="HTML" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_parser.png"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Log_Parser ---")

import pandas as pd
import numpy as np
import wget
import re
import time as clock
from datetime import datetime, timedelta
from time import gmtime, strftime
from argparse import ArgumentParser
from collections import OrderedDict

TIME_FORMAT = '%H%M%S'
DATE_FORMAT = '%d%m%Y'
PATTERNS_FILE = variables.get("PATTERNS_FILE")
LOG_FILE = variables.get("LOG_FILE")
STRUCTURED_LOG_FILE = variables.get("STRUCTURED_LOG_FILE")

INTERVAL   = 10000
LOG_FILE = wget.download(LOG_FILE)
PATTERN_FILE = PATTERNS_FILE
STRUCTERED_LOG_FILE = STRUCTURED_LOG_FILE
#===================================== Detect the different patterns =================================
print("Reading the Pattern_file")
df_patterns = pd.read_csv(PATTERN_FILE, sep = ';')
df_columns = pd.Series([''])
table = list()
for index, row in df_patterns.iterrows():
  for e in row[2].split(','):
    if e.strip() != '*':
      table.append(e.strip())
table.append('pattern_id')
myList = list(OrderedDict.fromkeys(table))
df_columns = pd.Series(myList)
print("The different patterns included in the Pattern_file were extracted")
#===================================== Parse raw logs =================================   
df_structured_logs = pd.DataFrame(columns = df_columns)
print("Processing " + LOG_FILE)
k = 0
t = clock.time()
#variables = list()
my_dict = OrderedDict()
print("Logs patterns matching is in progress")
with open(LOG_FILE) as infile:
  for line in infile:
    k = k + 1
    if k % INTERVAL == 0:
      elapsed_time = clock.time() - t
      print(str(k) + " " + str(elapsed_time) + "sec " + line)
    for index,variable_name in df_columns.iteritems():
      vide = np.nan
      my_dict.__setitem__(variable_name.strip(),vide)
    for index, row in df_patterns.iterrows():
      p = row[1]
      pattern = re.compile(p, re.IGNORECASE)
      m = pattern.match(line)
      if m:
        #print('Match found: ', m.group())
        i = 0
        for e in row[2].split(','):
          i = i+1
          if e.strip() != '*':
            var = m.group(i)
            if e.strip() == "date":
              if len(e.strip())<8:
                if len(var)==5:
                  idx=3
                elif len(var)==6:
                  idx=4
                str1_split1 = var[:idx]
                str1_split2 = var[idx:]
                tranformed_date =  str1_split1 + '20' + str1_split2
                my_dict.__setitem__(e.strip(),datetime.strptime(tranformed_date, DATE_FORMAT))
              else:
                my_dict.__setitem__(e.strip(),datetime.strptime(var, DATE_FORMAT))
            elif e.strip() == "time":
              my_dict.__setitem__(e.strip(),datetime.strptime(var.strip(), TIME_FORMAT).time())
            else:
              my_dict.__setitem__(e.strip(),repr(var.strip()).strip("0"))
              my_dict.__setitem__('pattern_id', int(row[0]))
        break
    df_inter = pd.DataFrame([my_dict.values()], columns=df_columns)
    df_structured_logs = df_structured_logs.append(df_inter, ignore_index=True)
        
print("All logs were matched")
#===================================== Preview results =================================
STRUCTURED_LOG_FILE=STRUCTURED_LOG_FILE.lower()
if STRUCTURED_LOG_FILE.endswith('csv'):
  result = df_structured_logs.to_csv()
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", result+".csv")
  resultMetadata.put("content.type", "text/csv")
elif STRUCTURED_LOG_FILE.endswith('html'):
  result = df_structured_logs.to_html()
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", result+".html")
  resultMetadata.put("content.type", "text/html") 
else:
  print('Your data is empty')
  
#===================================== Save the linked variables =================================    
df_json_logs = df_structured_logs.to_json(orient='split')
    
print("Finshed " + LOG_FILE + "PARSING")

variables.put("DATAFRAME_JSON", df_json_logs) 

print("--- END Log_Parser ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Feature_Vector_Extractor" >
      <description>
        <![CDATA[ Encodes structured data into numerical feature vectors whereby machine learning models can be applied. ]]>
      </description>
      <variables>
        <variable name="SESSION_COLUMN" value="id_block" inherited="false" />
        <variable name="FILE_OUT_FEATURES" value="HTML" inherited="false" />
        <variable name="PATTERN_COLUMN" value="pattern_id" inherited="false" />
        <variable name="PATTERNS_COUNT_FEATURES" value="False" inherited="false" model="PA:Boolean"/>
        <variable name="STATE_VARIABLES" value="status,date" inherited="false" />
        <variable name="COUNT_VARIABLES" value="ip_from,ip_to,pid,date,time" inherited="false" />
        <variable name="STATE_COUNT_FEATURES_VARIABLES" value="True" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/feature_extraction.png"/>
      </genericInformation>
      <depends>
        <task ref="Log_Parser"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Feature_Vector_Extractor ---")
import pandas as pd
import numpy as np

SESSION_COLUMN = variables.get("SESSION_COLUMN")
DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
FILE_OUT_FEATURES = variables.get("FILE_OUT_FEATURES")
PATTERN_COLUMN = variables.get("PATTERN_COLUMN")
PATTERNS_COUNT_FEATURES = variables.get("PATTERNS_COUNT_FEATURES")
STATE_COUNT_FEATURES_VARIABLES = variables.get("STATE_COUNT_FEATURES_VARIABLES")
STRUCTURED_LOGS = variables.get("DATAFRAME_JSON")
#===================================== Extract variables =================================
STATE_VARIABLES_INTER = variables.get("STATE_VARIABLES")
COUNT_VARIABLES_INTER = variables.get("COUNT_VARIABLES")
STATE_VARIABLES = STATE_VARIABLES_INTER.split(",")
COUNT_VARIABLES = COUNT_VARIABLES_INTER.split(",")
print("State Variables:")
print(STATE_VARIABLES)
print("Count Variables:")
print(COUNT_VARIABLES)

df_pattern_features = pd.DataFrame.empty
df_state_features = pd.DataFrame.empty
df_count_features = pd.DataFrame.empty

df_structured_logs  = pd.read_json(DATAFRAME_JSON,orient='split')
pattern_number = int(df_structured_logs[PATTERN_COLUMN].max())

#is usefull when there is multiple identifiers in a single row
def id_extraction(session_col=None):
  session_col = str(session_col)
  ids_list = session_col.split(' ')
  return ids_list

feature_vector = []
dict_block_features = {}
variables_name = list(df_structured_logs)
state_features_names = []
dict_states = {}
#dict_variables_set = {}
dict_variables_blk = {}
dict_block_features_state = {}
dict_block_features_state_1 = {}
dict_variables_set = {}

#===================================== Extract the state variables =================================
for i in range (len(STATE_VARIABLES)):
    variables_count = df_structured_logs[STATE_VARIABLES[i]].value_counts()
    for j in range(len(variables_count.keys())):
      dict_states[STATE_VARIABLES[i]]=variables_count.keys()
      state_features_names.append(variables_count.keys()[j])
#     del dict_states["''"]

for index,row in df_structured_logs.iterrows():
  if not(row[SESSION_COLUMN] == None):
    ids_list = id_extraction(row[SESSION_COLUMN])

#===================================== Features (count pattern) =================================
    if PATTERNS_COUNT_FEATURES=='True':
      j = int(row[PATTERN_COLUMN]-1)
      for i in range(len(ids_list)):
        #dict_variables_blk = {}
        # update existing entry
        if ids_list[i] in dict_block_features:
          features = dict_block_features.get(ids_list[i])
          features[j] = features[j] + 1
          dict_block_features[ids_list[i]] = features
        # add new entry
        else:
          feature_vector = [0] * pattern_number
          feature_vector[j] = feature_vector[j] + 1
          dict_block_features[ids_list[i]] = feature_vector
                
#===================================== Features (count state + variables) ======================================
    if STATE_COUNT_FEATURES_VARIABLES=='True':
      for f in range(len(ids_list)):
        # update existing entry
        if ids_list[f] in dict_block_features_state_1:
          features_count = dict_block_features_state_1.get(ids_list[f])
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]] == dict_states[STATE_VARIABLES[i]][j]:
                features_count[m] = features_count[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = features_count
                dict_block_features_state_1[ids_list[f]] = features_count
              m = m+1

          for h in range (len(COUNT_VARIABLES)):
            table_of_variable = dict_variables_blk[ids_list[f]].get(COUNT_VARIABLES[h])
            if (str(row[COUNT_VARIABLES[h]]) not in table_of_variable) and not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_blk[ids_list[f]][COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              features_count[m] = features_count[m] + 1
              dict_block_features_state_1[ids_list[f]] = features_count
            m = m+1

        # add new entry
        else:
          feature_vector_state_variables = [0]*(len(state_features_names)+len(COUNT_VARIABLES))
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]]==dict_states[STATE_VARIABLES[i]][j]:
                feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = feature_vector_state_variables
                dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
              m = m+1
          dict_variables_set_1 = {}
          for h in range (len(COUNT_VARIABLES)):
            dict_variables_set_1[COUNT_VARIABLES[h]] = []
            if not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_set_1[COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
            m = m+1
            dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
            dict_variables_blk[ids_list[f]] = dict_variables_set_1

#===================================== Save the different features in a dataframe ======================================
frames = []
if PATTERNS_COUNT_FEATURES=='True':
  features = dict_block_features.values()
  block_ids = dict_block_features.keys()
  df_pattern_features = pd.DataFrame(dict_block_features, index = ["pattern "+ str(i) for i in range(1,pattern_number+1)]).T
  frames.append(df_pattern_features)
if STATE_COUNT_FEATURES_VARIABLES=='True':
  df_state_features = pd.DataFrame(dict_block_features_state_1,index = state_features_names+COUNT_VARIABLES).T
  frames.append(df_state_features)
if not frames:
  df_features = pd.DataFrame.empty
  print("ERROR: No features extracted, check your input variables")
else:
  df_features = pd.concat(frames, axis=1)

#===================================== Preview results =================================
FILE_OUT_FEATURES = FILE_OUT_FEATURES.lower()
if FILE_OUT_FEATURES.endswith('csv'):
  result = df_features.to_csv()
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", result+".csv")
  resultMetadata.put("content.type", "text/csv")
elif FILE_OUT_FEATURES.endswith('html'):
  result = df_features.to_html()
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", result+".html")
  resultMetadata.put("content.type", "text/html") 
else:
  print('Your data is empty')

#===================================== Save the linked variables =================================  
columns_name = df_features.columns
df_json_features = df_features.to_json(orient='split')
variables.put("DATA_TRAIN_DF_JSON", df_json_features)
variables.put("DATA_TEST_DF_JSON", df_json_features) 
variables.put("DATAFRAME_JSON", df_json_features)
variables.put("COLUMNS_NAME_JSON", pd.Series(columns_name).to_json()) 

print("--- END Feature_Vector_Extractor ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Train_Anomaly_Model" >
      <description>
        <![CDATA[ Train an anomaly model. ]]>
      </description>
      <variables>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/train.png"/>
      </genericInformation>
      <depends>
        <task ref="Split_Data"/>
        <task ref="One_Class_SVM"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Train_Anomaly_Detection ---")

import pandas as pd
import random
import pickle

IS_UNSUPERVIDED_ALGORITHM = variables.get("UNSUPERVISED_ALGORITHM")
DATA_TRAIN_DF_JSON  = variables.get("DATA_TRAIN_DF_JSON")

if IS_UNSUPERVIDED_ALGORITHM == 'True'and DATA_TRAIN_DF_JSON != None:
  ALGORITHM_NAME = variables.get("ALGORITHM_NAME")
  data_train_df = pd.read_json(DATA_TRAIN_DF_JSON, orient='split')
  model = None

if IS_UNSUPERVIDED_ALGORITHM  == 'True' and DATA_TRAIN_DF_JSON != None:
  ALGORITHM_NAME = variables.get("ALGORITHM_NAME")
  data_train_df = pd.read_json(DATA_TRAIN_DF_JSON, orient='split') 
  model = None
  
  if ALGORITHM_NAME == 'OneClassSVM':
    from sklearn import svm
    nu_para = variables.get("NU_PARA")
    kernel_para = variables.get("KERNEL_PARA")
    gamma_para = variables.get("GAMMA_PARA")
    model = svm.OneClassSVM(nu=nu_para, kernel=kernel_para, gamma = gamma_para) 
  
  if ALGORITHM_NAME == 'LocalOutlierFactor':
    from sklearn.neighbors import LocalOutlierFactor
    neighbors_para = variables.get("N_NEIGHBORS_PARA")
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = LocalOutlierFactor(n_neighbors=int(neighbors_para), n_jobs=int(n_jobs_para))
  
  model.fit(data_train_df.values)  
  model_bin = pickle.dumps(model)
  variables.put("MODEL", model_bin)
else:
  print('Please check your ML pipeline')

print("--- END Train_Anomaly_Detection ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Predict_Anomaly_Model" >
      <description>
        <![CDATA[ Generate predictions using a trained model. ]]>
      </description>
      <variables>
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
      </genericInformation>
      <depends>
        <task ref="Train_Anomaly_Model"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Predict_Anomaly_Model ---")

import os
import pickle
import pandas as pd

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn import metrics

MODEL_BIN = variables.get("MODEL")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
is_oneclass_algorithm = variables.get("DATA_TEST_DF_JSON")
IS_UNSUPERVIDED_ALGORITHM = variables.get("UNSUPERVISED_ALGORITHM")
ALGORITHM_NAME = variables.get("ALGORITHM_NAME")

# ONE-CLASS SVM
if ALGORITHM_NAME == 'OneClassSVM' and MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON, orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data  = loaded_model.predict(data_test_df.values)
  predict_data_df = pd.DataFrame(predict_data)
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  
  # ONE-CLASS MEASURE MEASURES 
  try: 
    is_oneclass_algorithm = variables.get("ONECLASS_MEASURE")  
    LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")   

    if is_oneclass_algorithm == 'True' and LABEL_TEST_DF_JSON != None:  
      label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')
      print("**********************CLASSIFICATION MEASURES**********************")
      accuracy_score_result = metrics.accuracy_score(label_test_df.values.ravel(), predict_data)
      precision_score_result = metrics.precision_score(label_test_df.values.ravel(), predict_data, average='micro')
      recall_score_result = metrics.recall_score(label_test_df.values.ravel(), predict_data, average='micro')
      f1_score_result = metrics.f1_score(label_test_df.values.ravel(), predict_data, average='micro') 
      auc_score_result = metrics.roc_auc_score(label_test_df.values.ravel(), predict_data, average='micro')       
      print("ACCURACY SCORE: %.2f" % accuracy_score_result)
      print("PRECISION SCORE: %.2f" % precision_score_result)
      print("RECALL SCORE:\n%s" %  recall_score_result)
      print("F1 SCORE:\n%s" %  f1_score_result)     
      print("AREA UNDER CURVE(AUC):\n%s" %  auc_score_result)         
      print("*********************************************************************************")
   
  except NameError:
    print(predict_data_df)
  print("END Predict_Anomaly_Model")    
  
# LOCAL OUTLIER FACTOR MEASURES  
elif ALGORITHM_NAME == 'LocalOutlierFactor' and MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON, orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data  = loaded_model.fit_predict(data_test_df.values)
  predict_data_df = pd.DataFrame(predict_data)
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  
  # LOCAL OUTLIER FACTOR MEASURES
  try: 
    is_oneclass_algorithm = variables.get("ONECLASS_MEASURE") 
    LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON") 
    
    if is_oneclass_algorithm == 'True' and LABEL_TEST_DF_JSON != None:     
      label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')  
      print("**********************CLASSIFICATION MEASURES**********************")
      accuracy_score_result = metrics.accuracy_score(label_test_df.values.ravel(), predict_data)
      precision_score_result = metrics.precision_score(label_test_df.values.ravel(), predict_data, average='micro')
      recall_score_result = metrics.recall_score(label_test_df.values.ravel(), predict_data, average='micro')
      f1_score_result = metrics.f1_score(label_test_df.values.ravel(), predict_data, average='micro') 
      auc_score_result = metrics.roc_auc_score(label_test_df.values.ravel(), predict_data, average='micro')       
      print("ACCURACY SCORE: %.2f" % accuracy_score_result)
      print("PRECISION SCORE: %.2f" % precision_score_result)
      print("RECALL SCORE:\n%s" %  recall_score_result)
      print("F1 SCORE:\n%s" %  f1_score_result)     
      print("AREA UNDER CURVE(AUC):\n%s" %  auc_score_result)         
      print("*********************************************************************************")
   
  except NameError:
    print(predict_data_df)
else:
  print('Please check your ML pipeline')

print("-- END Predict_Anomaly_Model ---")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Export_Results" >
      <description>
        <![CDATA[ Export the results. ]]>
      </description>
      <variables>
        <variable name="OUTPUT_FILE" value="HTML" inherited="false" />
        <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Anomaly_Model"/>
      </depends>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("--- BEGIN Export_Results ---")

import pandas as pd
import numpy as np

OUTPUT_FILE = variables.get("OUTPUT_FILE")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
PREDICT_DATA = variables.get("PREDICT_DATA_JSON")
OUTPUT_FILE=OUTPUT_FILE.upper()

if DATA_TEST_DF_JSON != None and PREDICT_DATA != None: 
    data_test_df  = pd.read_json(DATA_TEST_DF_JSON, orient='split')   
    predict_data  = pd.read_json(PREDICT_DATA, orient='split')    
    frame_prediction = pd.DataFrame(predict_data)    
    prediction_result = data_test_df.assign(predictions=frame_prediction.values)
    prediction_result = prediction_result.sort_index(ascending=True)
    
    if OUTPUT_FILE == 'HTML':  
        #***************# HTML PREVIEW STYLING #***************#
        styles = [
            dict(selector="th", props=[("font-weight", "bold"),
                               ("text-align", "center"),
                               ("font-size", "15px"),
                               ("background", "#0B6FA4"),
                               ("color", "#FFFFFF")]),
                               ("padding", "3px 7px"),
            dict(selector="td", props=[("text-align", "right"),
                               ("padding", "3px 3px"),
                               ("border", "1px solid #999999"),
                               ("font-size", "13px"),
                               ("border-bottom", "1px solid #0B6FA4")]),
            dict(selector="table", props=[("border", "1px solid #999999"),
                               ("text-align", "center"),
                               ("width", "100%"),
                               ("border-collapse", "collapse")])
        ]
        #******************************************************#

        with pd.option_context('display.max_colwidth', -1):
            result = prediction_result.style.set_table_styles(styles).render().encode('utf-8')
            resultMetadata.put("file.extension", ".html")
            resultMetadata.put("file.name", "output.html")
            resultMetadata.put("content.type", "text/html")
    elif OUTPUT_FILE == 'CSV':      
        result = prediction_result.to_csv()    
        resultMetadata.put("file.extension", ".csv")
        resultMetadata.put("file.name", "result.csv")
        resultMetadata.put("content.type", "text/csv")
else:
  print('It is not possible to export the data')

print("--- END Export_Results ---")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"/>
        </task>
        <task name="Split_Data" >
          <description>
            <![CDATA[ Divide the data into two sets. ]]>
          </description>
          <variables>
            <variable name="TRAIN_SIZE" value="0.7" inherited="false" />
            <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/split_data.png"/>
          </genericInformation>
          <depends>
            <task ref="Feature_Vector_Extractor"/>
          </depends>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("--- BEGIN Split_Data ---")

from sklearn import model_selection
import pandas as pd

TRAIN_SIZE = 0.7
try:
  IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
  TRAIN_SIZE = float(variables.get("TRAIN_SIZE"))
except NameError:
  pass
test_size = 1 - TRAIN_SIZE

if IS_LABELED_DATA == 'True':
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
    
  data = dataframe.values[:,0:columns_number-1]
  label = dataframe.values[:,columns_number-1]
  indice = dataframe.index.values
  
  data_train, data_test, label_train, label_test, idx_train, idx_test = model_selection.train_test_split(data, label, indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name[0:columns_number-1],index=idx_train)
  label_train_df = pd.DataFrame(data=label_train,columns=[columns_name[columns_number-1]])
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name[0:columns_number-1],index=idx_test)
  label_test_df = pd.DataFrame(data=label_test,columns=[columns_name[columns_number-1]])
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  LABEL_TRAIN_DF_JSON = label_train_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
  except NameError:
    pass
  
  print("END Split_Data")
  
elif IS_LABELED_DATA == 'False' or IS_LABELED_DATA == None:
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
  indice = dataframe.index.values
  data = dataframe.values
  
  data_train, data_test, idx_train, idx_test = model_selection.train_test_split(data,indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name,index=idx_train)
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name,index=idx_test)
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
  except NameError:
    pass
else:
  print('The data could not be split, please check ypur ML pipeline')

print("--- END Split_Data ---")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"/>
        </task>
        <task name="One_Class_SVM" >
          <description>
            <![CDATA[ One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. ]]>
          </description>
          <variables>
            <variable name="NU" value="0.1" inherited="false" />
            <variable name="KERNEL" value="rbf" inherited="false" />
            <variable name="GAMMA" value="0.1" inherited="false" />
            <variable name="DOCKER_ENABLED" value="True" inherited="true" model="PA:Boolean"/>
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/ml_anomaly.png"/>
            <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_one_class_svm"/>
          </genericInformation>
          <forkEnvironment javaHome="/usr" >
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("--- BEGIN One_Class_SVM ---")

KERNEL = variables.get("KERNEL")
NU = float(variables.get("NU"))
GAMMA = float(variables.get("GAMMA"))

variables.put("NU_PARA", NU)
variables.put("KERNEL_PARA", KERNEL)
variables.put("GAMMA_PARA", GAMMA)
variables.put("ALGORITHM_NAME", "OneClassSVM")
variables.put("UNSUPERVISED_ALGORITHM", "True")
variables.put("ONECLASS_MEASURE", "True")

print("--- END One_Class_SVM ---")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"/>
        </task>
      </taskFlow>
      <metadata>
        <visualization>
          <![CDATA[ <html><head><link rel="stylesheet" href="/studio/styles/studio-standalone.css"><style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:1122px;
            height:803px;
            }
        </style></head><body><div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-108px;left:-427px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_1226" style="top: 113px; left: 432px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/log_parser.png" width="20px">&nbsp;<span class="name">Log_Parser</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_1229" style="top: 241px; left: 432px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/feature_extraction.png" width="20px">&nbsp;<span class="name">Feature_Vector_Extractor</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_1232" style="top: 497px; left: 503.75px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/train.png" width="20px">&nbsp;<span class="name">Train_Anomaly_Model</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_1235" style="top: 625px; left: 503.75px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/predict.png" width="20px">&nbsp;<span class="name">Predict_Anomaly_Model</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_1238" style="top: 753px; left: 503.75px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/export_data.png" width="20px">&nbsp;<span class="name">Export_Results</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_1241" style="top: 369px; left: 432px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/split_data.png" width="20px">&nbsp;<span class="name">Split_Data</span></a></div><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_1244" style="top: 369px; left: 575.5px;"><a class="task-name"><img src="/automation-dashboard/styles/patterns/img/wf-icons/ml_anomaly.png" width="20px">&nbsp;<span class="name">One_Class_SVM</span></a></div><svg style="position:absolute;left:471.5px;top:152.5px" width="45.5" height="89" pointer-events="none" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 24.5 88 C 34.5 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M25.144359625,65.8307285 L24.91960402101258,44.64230041015633 L20.379554453683394,52.666510647070254 L11.755386168082827,49.40710558147294 L25.144359625,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M25.144359625,65.8307285 L24.91960402101258,44.64230041015633 L20.379554453683394,52.666510647070254 L11.755386168082827,49.40710558147294 L25.144359625,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:471.5px;top:408.5px" width="110" height="89" pointer-events="none" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 89 88 C 99 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M80.38784375,62.2538125 L68.29767524802567,44.8518635978262 L69.05011953404043,54.04065174406419 L60.084514492089866,56.18958781378577 L80.38784375,62.2538125" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M80.38784375,62.2538125 L68.29767524802567,44.8518635978262 L69.05011953404043,54.04065174406419 L60.084514492089866,56.18958781378577 L80.38784375,62.2538125" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:560.5px;top:408.5px" width="80.5" height="89" pointer-events="none" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 69.5 50 59.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M3.5916051249999983,63.998374500000004 L22.094953218722672,53.672382570769685 L12.876661377066316,53.52041046131558 L11.61698918003824,44.38732631870337 L3.5916051249999983,63.998374500000004" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M3.5916051249999983,63.998374500000004 L22.094953218722672,53.672382570769685 L12.876661377066316,53.52041046131558 L11.61698918003824,44.38732631870337 L3.5916051249999983,63.998374500000004" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:560.5px;top:536.5px" width="25.5" height="89" pointer-events="none" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 4.5 88 C 14.5 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M6.950109375,66.78168750000002 L12.19383263091469,46.25114034666338 L5.739082405354392,52.834163932040326 L-1.7536909370449987,47.46216731630898 L6.950109375,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M6.950109375,66.78168750000002 L12.19383263091469,46.25114034666338 L5.739082405354392,52.834163932040326 L-1.7536909370449987,47.46216731630898 L6.950109375,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:544.5px;top:664.5px" width="41.5" height="89" pointer-events="none" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 30.5 50 20.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M-1.080432000000001,66.303232 L11.3951921061979,49.175511685817675 L2.9611229197005473,52.899283558177174 L-2.0087563356249163,45.13395676611713 L-1.080432000000001,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M-1.080432000000001,66.303232 L11.3951921061979,49.175511685817675 L2.9611229197005473,52.899283558177174 L-2.0087563356249163,45.13395676611713 L-1.080432000000001,66.303232" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><svg style="position:absolute;left:471.5px;top:280.5px" width="45.5" height="89" pointer-events="none" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 0 88 C -10 38 34.5 50 24.5 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M-0.6443596250000018,65.8307285 L12.744613831917167,49.40710558147293 L4.120445546316602,52.66651064707025 L-0.41960402101258953,44.64230041015634 L-0.6443596250000018,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" d="M-0.6443596250000018,65.8307285 L12.744613831917167,49.40710558147293 L4.120445546316602,52.66651064707025 L-0.41960402101258953,44.64230041015634 L-0.6443596250000018,65.8307285" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 472px; top: 143px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 496.5px; top: 271px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 496.5px; top: 231px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 561px; top: 527px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 561px; top: 487px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 565.5px; top: 655px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 565.5px; top: 615px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 545px; top: 783px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 545px; top: 743px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 472px; top: 399px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 472px; top: 359px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 620.5px; top: 399px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1"
          xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div></body></html>
 ]]>
        </visualization>
      </metadata>
    </job>