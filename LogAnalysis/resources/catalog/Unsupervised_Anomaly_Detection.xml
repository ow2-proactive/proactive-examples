<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<job xmlns="urn:proactive:jobdescriptor:3.10" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" maxNumberOfExecution="2" name="Unsupervised_Anomaly_Detection" onTaskError="continueJobExecution" priority="normal" projectName="Log Analysis" xsi:schemaLocation="urn:proactive:jobdescriptor:3.10 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.10/schedulerjob.xsd">
  <variables>
    <variable name="instance_name" value="visdom-server-1"/>
    <variable model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
  </variables>
  <description>
    <![CDATA[ Detect anomalies using an Unsupervised One-Class SVM. ]]>
  </description>
  <genericInformation>
<info name="bucketName" value="machine-learning-workflows"/>
<info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_analysis.png"/>
<info name="Documentation" value="https://www.activeeon.com/public_content/documentation/latest/MLOS/MLOSUserGuide.html#_log_analysis"/>
<info name="group" value="public-objects"/>
</genericInformation>
  <taskFlow>
    <task name="Log_Parser">
      <description>
        <![CDATA[ Extracts a group of event templates, whereby raw logs can be structured. ]]>
      </description>
      <variables>
        <variable inherited="false" name="LOG_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/HDFS_2k.log"/>
        <variable inherited="false" name="PATTERNS_FILE" value="https://s3.eu-west-2.amazonaws.com/activeeon-public/datasets/patterns.csv"/>
        <variable inherited="false" name="STRUCTURED_LOG_FILE" value="HTML"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/log_parser.png"/>
      </genericInformation>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN LOG PARSING")

import pandas as pd
import numpy as np
import wget
import re
import time as clock
from datetime import datetime, timedelta
from time import gmtime, strftime
from argparse import ArgumentParser
from collections import OrderedDict

TIME_FORMAT = '%H%M%S'
DATE_FORMAT = '%d%m%Y'
PATTERNS_FILE = variables.get("PATTERNS_FILE")
LOG_FILE = variables.get("LOG_FILE")
STRUCTURED_LOG_FILE = variables.get("STRUCTURED_LOG_FILE")

INTERVAL   = 10000
LOG_FILE = wget.download(LOG_FILE)
PATTERN_FILE = PATTERNS_FILE
STRUCTERED_LOG_FILE = STRUCTURED_LOG_FILE
#===================================== Detect the different patterns =================================
print("Reading the Pattern_file")
df_patterns = pd.read_csv(PATTERN_FILE, sep = ';')
df_columns = pd.Series([''])
table = list()
for index, row in df_patterns.iterrows():
  for e in row[2].split(','):
    if e.strip() != '*':
      table.append(e.strip())
table.append('pattern_id')
myList = list(OrderedDict.fromkeys(table))
df_columns = pd.Series(myList)
print("The different patterns included in the Pattern_file were extracted")
#===================================== Parse raw logs =================================   
df_structured_logs = pd.DataFrame(columns = df_columns)
print("Processing " + LOG_FILE)
k = 0
t = clock.time()
#variables = list()
my_dict = OrderedDict()
print("Logs patterns matching is in progress")
with open(LOG_FILE) as infile:
  for line in infile:
    k = k + 1
    if k % INTERVAL == 0:
      elapsed_time = clock.time() - t
      print(str(k) + " " + str(elapsed_time) + "sec " + line)
    for index,variable_name in df_columns.iteritems():
      vide = np.nan
      my_dict.__setitem__(variable_name.strip(),vide)
    for index, row in df_patterns.iterrows():
      p = row[1]
      pattern = re.compile(p, re.IGNORECASE)
      m = pattern.match(line)
      if m:
        #print('Match found: ', m.group())
        i = 0
        for e in row[2].split(','):
          i = i+1
          if e.strip() != '*':
            var = m.group(i)
            if e.strip() == "date":
              if len(e.strip())<8:
                if len(var)==5:
                  idx=3
                elif len(var)==6:
                  idx=4
                str1_split1 = var[:idx]
                str1_split2 = var[idx:]
                tranformed_date =  str1_split1 + '20' + str1_split2
                my_dict.__setitem__(e.strip(),datetime.strptime(tranformed_date, DATE_FORMAT))
              else:
                my_dict.__setitem__(e.strip(),datetime.strptime(var, DATE_FORMAT))
            elif e.strip() == "time":
              my_dict.__setitem__(e.strip(),datetime.strptime(var.strip(), TIME_FORMAT).time())
            else:
              my_dict.__setitem__(e.strip(),repr(var.strip()).strip("0"))
              my_dict.__setitem__('pattern_id', int(row[0]))
        break
    df_inter = pd.DataFrame([my_dict.values()], columns=df_columns)
    df_structured_logs = df_structured_logs.append(df_inter, ignore_index=True)
        
print("All logs were matched")
#===================================== Preview results =================================
STRUCTURED_LOG_FILE=STRUCTURED_LOG_FILE.lower()
if STRUCTURED_LOG_FILE.endswith('csv'):
  result = df_structured_logs.to_csv()
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", result+".csv")
  resultMetadata.put("content.type", "text/csv")
elif STRUCTURED_LOG_FILE.endswith('html'):
  result = df_structured_logs.to_html()
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", result+".html")
  resultMetadata.put("content.type", "text/html") 
else:
  print('Your data is empty')
  
#===================================== Save the linked variables =================================    
df_json_logs = df_structured_logs.to_json(orient='split')
    
print("Finshed " + LOG_FILE + "PARSING")

variables.put("DATAFRAME_JSON", df_json_logs) 

print("END LOG PARSING")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Feature_Vector_Extractor">
      <description>
        <![CDATA[ Encodes structured data into numerical feature vectors whereby machine learning models can be applied. ]]>
      </description>
      <variables>
        <variable inherited="false" name="SESSION_COLUMN" value="id_block"/>
        <variable inherited="false" name="FILE_OUT_FEATURES" value="HTML"/>
        <variable inherited="false" name="PATTERN_COLUMN" value="pattern_id"/>
        <variable inherited="false" model="PA:Boolean" name="PATTERNS_COUNT_FEATURES" value="False"/>
        <variable inherited="false" name="STATE_VARIABLES" value="status,date"/>
        <variable inherited="false" name="COUNT_VARIABLES" value="ip_from,ip_to,pid,date,time"/>
        <variable inherited="false" name="STATE_COUNT_FEATURES_VARIABLES" value="True"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/feature_extraction.png"/>
      </genericInformation>
      <depends>
        <task ref="Log_Parser"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN FEATURE VECTOR EXTRACTION")
import pandas as pd
import numpy as np

SESSION_COLUMN = variables.get("SESSION_COLUMN")
DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
FILE_OUT_FEATURES = variables.get("FILE_OUT_FEATURES")
PATTERN_COLUMN = variables.get("PATTERN_COLUMN")
PATTERNS_COUNT_FEATURES = variables.get("PATTERNS_COUNT_FEATURES")
STATE_COUNT_FEATURES_VARIABLES = variables.get("STATE_COUNT_FEATURES_VARIABLES")
STRUCTURED_LOGS = variables.get("DATAFRAME_JSON")
#===================================== Extract variables =================================
STATE_VARIABLES_INTER = variables.get("STATE_VARIABLES")
COUNT_VARIABLES_INTER = variables.get("COUNT_VARIABLES")
STATE_VARIABLES = STATE_VARIABLES_INTER.split(",")
COUNT_VARIABLES = COUNT_VARIABLES_INTER.split(",")
print("State Variables:")
print(STATE_VARIABLES)
print("Count Variables:")
print(COUNT_VARIABLES)

df_pattern_features = pd.DataFrame.empty
df_state_features = pd.DataFrame.empty
df_count_features = pd.DataFrame.empty

df_structured_logs  = pd.read_json(DATAFRAME_JSON,orient='split')
pattern_number = int(df_structured_logs[PATTERN_COLUMN].max())

#is usefull when there is multiple identifiers in a single row
def id_extraction(session_col=None):
  session_col = str(session_col)
  ids_list = session_col.split(' ')
  return ids_list

feature_vector = []
dict_block_features = {}
variables_name = list(df_structured_logs)
state_features_names = []
dict_states = {}
#dict_variables_set = {}
dict_variables_blk = {}
dict_block_features_state = {}
dict_block_features_state_1 = {}
dict_variables_set = {}

#===================================== Extract the state variables =================================
for i in range (len(STATE_VARIABLES)):
    variables_count = df_structured_logs[STATE_VARIABLES[i]].value_counts()
    for j in range(len(variables_count.keys())):
      dict_states[STATE_VARIABLES[i]]=variables_count.keys()
      state_features_names.append(variables_count.keys()[j])
#     del dict_states["''"]

for index,row in df_structured_logs.iterrows():
  if not(row[SESSION_COLUMN] == None):
    ids_list = id_extraction(row[SESSION_COLUMN])

#===================================== Features (count pattern) =================================
    if PATTERNS_COUNT_FEATURES=='True':
      j = int(row[PATTERN_COLUMN]-1)
      for i in range(len(ids_list)):
        #dict_variables_blk = {}
        # update existing entry
        if ids_list[i] in dict_block_features:
          features = dict_block_features.get(ids_list[i])
          features[j] = features[j] + 1
          dict_block_features[ids_list[i]] = features
        # add new entry
        else:
          feature_vector = [0] * pattern_number
          feature_vector[j] = feature_vector[j] + 1
          dict_block_features[ids_list[i]] = feature_vector
                
#===================================== Features (count state + variables) ======================================
    if STATE_COUNT_FEATURES_VARIABLES=='True':
      for f in range(len(ids_list)):
        # update existing entry
        if ids_list[f] in dict_block_features_state_1:
          features_count = dict_block_features_state_1.get(ids_list[f])
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]] == dict_states[STATE_VARIABLES[i]][j]:
                features_count[m] = features_count[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = features_count
                dict_block_features_state_1[ids_list[f]] = features_count
              m = m+1

          for h in range (len(COUNT_VARIABLES)):
            table_of_variable = dict_variables_blk[ids_list[f]].get(COUNT_VARIABLES[h])
            if (str(row[COUNT_VARIABLES[h]]) not in table_of_variable) and not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_blk[ids_list[f]][COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              features_count[m] = features_count[m] + 1
              dict_block_features_state_1[ids_list[f]] = features_count
            m = m+1

        # add new entry
        else:
          feature_vector_state_variables = [0]*(len(state_features_names)+len(COUNT_VARIABLES))
          m = 0
          for i in range (len(STATE_VARIABLES)):
            for j in range(len(dict_states[STATE_VARIABLES[i]])):
              if row[STATE_VARIABLES[i]]==dict_states[STATE_VARIABLES[i]][j]:
                feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
                dict_block_features_state[dict_states[STATE_VARIABLES[i]][j]] = feature_vector_state_variables
                dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
              m = m+1
          dict_variables_set_1 = {}
          for h in range (len(COUNT_VARIABLES)):
            dict_variables_set_1[COUNT_VARIABLES[h]] = []
            if not(row[COUNT_VARIABLES[h]] == None):
              dict_variables_set_1[COUNT_VARIABLES[h]].append(str(row[COUNT_VARIABLES[h]]))
              feature_vector_state_variables[m] = feature_vector_state_variables[m] + 1
            m = m+1
            dict_block_features_state_1[ids_list[f]] = feature_vector_state_variables
            dict_variables_blk[ids_list[f]] = dict_variables_set_1

#===================================== Save the different features in a dataframe ======================================
frames = []
if PATTERNS_COUNT_FEATURES=='True':
  features = dict_block_features.values()
  block_ids = dict_block_features.keys()
  df_pattern_features = pd.DataFrame(dict_block_features, index = ["pattern "+ str(i) for i in range(1,pattern_number+1)]).T
  frames.append(df_pattern_features)
if STATE_COUNT_FEATURES_VARIABLES=='True':
  df_state_features = pd.DataFrame(dict_block_features_state_1,index = state_features_names+COUNT_VARIABLES).T
  frames.append(df_state_features)
if not frames:
  df_features = pd.DataFrame.empty
  print("ERROR: No features extracted, check your input variables")
else:
  df_features = pd.concat(frames, axis=1)

#===================================== Preview results =================================
FILE_OUT_FEATURES = FILE_OUT_FEATURES.lower()
if FILE_OUT_FEATURES.endswith('csv'):
  result = df_features.to_csv()
  resultMetadata.put("file.extension", ".csv")
  resultMetadata.put("file.name", result+".csv")
  resultMetadata.put("content.type", "text/csv")
elif FILE_OUT_FEATURES.endswith('html'):
  result = df_features.to_html()
  resultMetadata.put("file.extension", ".html")
  resultMetadata.put("file.name", result+".html")
  resultMetadata.put("content.type", "text/html") 
else:
  print('Your data is empty')

#===================================== Save the linked variables =================================  
columns_name = df_features.columns
df_json_features = df_features.to_json(orient='split')
variables.put("DATA_TRAIN_DF_JSON", df_json_features)
variables.put("DATA_TEST_DF_JSON", df_json_features) 
variables.put("DATAFRAME_JSON", df_json_features)
variables.put("COLUMNS_NAME_JSON", pd.Series(columns_name).to_json()) 

print("END FEATURE VECTOR EXTRACTION")
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Train_Anomaly_Model">
      <description>
        <![CDATA[ Train an anomaly model. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/train.png"/>
      </genericInformation>
      <depends>
        <task ref="Split_Data"/>
        <task ref="One_Class_SVM"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Train_Anomaly_Detection")

import pandas as pd
import random
import pickle

  
IS_UNSUPERVIDED_ALGORITHM = variables.get("UNSUPERVISED_ALGORITHM")
DATA_TRAIN_DF_JSON  = variables.get("DATA_TRAIN_DF_JSON")

if IS_UNSUPERVIDED_ALGORITHM == 'True'and DATA_TRAIN_DF_JSON != None:
  ALGORITHM_NAME = variables.get("ALGORITHM_NAME")
  data_train_df = pd.read_json(DATA_TRAIN_DF_JSON, orient='split')
  model = None

  
if IS_UNSUPERVIDED_ALGORITHM  == 'True' and DATA_TRAIN_DF_JSON != None:
  ALGORITHM_NAME = variables.get("ALGORITHM_NAME")
  data_train_df = pd.read_json(DATA_TRAIN_DF_JSON, orient='split') 
  model = None
  
  if ALGORITHM_NAME == 'OneClassSVM':
    from sklearn import svm
    nu_para = variables.get("NU_PARA")
    kernel_para = variables.get("KERNEL_PARA")
    gamma_para = variables.get("GAMMA_PARA")
    model = svm.OneClassSVM(nu=nu_para, kernel=kernel_para, gamma = gamma_para) 
    
  if ALGORITHM_NAME == 'LocalOutlierFactor':
    from sklearn.neighbors import LocalOutlierFactor
    neighbors_para = variables.get("N_NEIGHBORS_PARA")
    n_jobs_para = variables.get("N_JOBS_PARA")
    model = LocalOutlierFactor(n_neighbors=int(neighbors_para), n_jobs=int(n_jobs_para))
  
    
  model.fit(data_train_df.values)  
  model_bin = pickle.dumps(model)
  variables.put("MODEL", model_bin)
  print("END Train_Model")  
  
  print("END Train_Anomaly_Detection")

else:
  print('Please check your ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Predict_Anomaly_Model">
      <description>
        <![CDATA[ Generate predictions using a trained model. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/predict.png"/>
      </genericInformation>
      <depends>
        <task ref="Train_Anomaly_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Predict_Anomaly_Model")

import os
import pickle
import pandas as pd

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn import metrics


MODEL_BIN = variables.get("MODEL")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
is_oneclass_algorithm = variables.get("DATA_TEST_DF_JSON")
IS_UNSUPERVIDED_ALGORITHM = variables.get("UNSUPERVISED_ALGORITHM")
ALGORITHM_NAME = variables.get("ALGORITHM_NAME")


# ONE-CLASS SVM
if ALGORITHM_NAME == 'OneClassSVM' and MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON, orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data  = loaded_model.predict(data_test_df.values)
  predict_data_df = pd.DataFrame(predict_data)
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  
  # ONE-CLASS MEASURE MEASURES 
  try: 
    is_oneclass_algorithm = variables.get("ONECLASS_MEASURE")  
    LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON")   

    if is_oneclass_algorithm == 'True' and LABEL_TEST_DF_JSON != None:  
      label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')
      print("**********************CLASSIFICATION MEASURES**********************")
      accuracy_score_result = metrics.accuracy_score(label_test_df.values.ravel(), predict_data)
      precision_score_result = metrics.precision_score(label_test_df.values.ravel(), predict_data, average='micro')
      recall_score_result = metrics.recall_score(label_test_df.values.ravel(), predict_data, average='micro')
      f1_score_result = metrics.f1_score(label_test_df.values.ravel(), predict_data, average='micro') 
      auc_score_result = metrics.roc_auc_score(label_test_df.values.ravel(), predict_data, average='micro')       
      print("ACCURACY SCORE: %.2f" % accuracy_score_result)
      print("PRECISION SCORE: %.2f" % precision_score_result)
      print("RECALL SCORE:\n%s" %  recall_score_result)
      print("F1 SCORE:\n%s" %  f1_score_result)     
      print("AREA UNDER CURVE(AUC):\n%s" %  auc_score_result)         
      print("*********************************************************************************")
   
  except NameError:
    print(predict_data_df)
  print("END Predict_Anomaly_Model")    
  
# LOCAL OUTLIER FACTOR MEASURES  
elif ALGORITHM_NAME == 'LocalOutlierFactor' and MODEL_BIN != None and DATA_TEST_DF_JSON != None:
  data_test_df = pd.read_json(DATA_TEST_DF_JSON, orient='split')
  loaded_model = pickle.loads(MODEL_BIN)
  predict_data  = loaded_model.fit_predict(data_test_df.values)
  predict_data_df = pd.DataFrame(predict_data)
  variables.put("PREDICT_DATA_JSON", predict_data_df.to_json(orient='split'))
  
  # LOCAL OUTLIER FACTOR MEASURES
  try: 
    is_oneclass_algorithm = variables.get("ONECLASS_MEASURE") 
    LABEL_TEST_DF_JSON = variables.get("LABEL_TEST_DF_JSON") 
    
    if is_oneclass_algorithm == 'True' and LABEL_TEST_DF_JSON != None:     
      label_test_df = pd.read_json(LABEL_TEST_DF_JSON, orient='split')  
      print("**********************CLASSIFICATION MEASURES**********************")
      accuracy_score_result = metrics.accuracy_score(label_test_df.values.ravel(), predict_data)
      precision_score_result = metrics.precision_score(label_test_df.values.ravel(), predict_data, average='micro')
      recall_score_result = metrics.recall_score(label_test_df.values.ravel(), predict_data, average='micro')
      f1_score_result = metrics.f1_score(label_test_df.values.ravel(), predict_data, average='micro') 
      auc_score_result = metrics.roc_auc_score(label_test_df.values.ravel(), predict_data, average='micro')       
      print("ACCURACY SCORE: %.2f" % accuracy_score_result)
      print("PRECISION SCORE: %.2f" % precision_score_result)
      print("RECALL SCORE:\n%s" %  recall_score_result)
      print("F1 SCORE:\n%s" %  f1_score_result)     
      print("AREA UNDER CURVE(AUC):\n%s" %  auc_score_result)         
      print("*********************************************************************************")
   
  except NameError:
    print(predict_data_df)
  print("END Predict_Anomaly_Model")    
  
else:
  print('Please check your ML pipeline')
]]>
          </code>
        </script>
      </scriptExecutable>
      <controlFlow block="none"/>
    </task>
    <task name="Export_Results">
      <description>
        <![CDATA[ Export the results. ]]>
      </description>
      <variables>
        <variable inherited="false" name="OUTPUT_FILE" value="HTML"/>
      </variables>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/export_data.png"/>
      </genericInformation>
      <depends>
        <task ref="Predict_Anomaly_Model"/>
      </depends>
      <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
        <envScript>
          <script>
            <code language="python">
              <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
            </code>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
print("BEGIN Export_Results")

import pandas as pd
import numpy as np

OUTPUT_FILE = variables.get("OUTPUT_FILE")
DATA_TEST_DF_JSON = variables.get("DATA_TEST_DF_JSON")
PREDICT_DATA = variables.get("PREDICT_DATA_JSON")
OUTPUT_FILE=OUTPUT_FILE.upper()


if DATA_TEST_DF_JSON != None and PREDICT_DATA != None: 
    data_test_df  = pd.read_json(DATA_TEST_DF_JSON, orient='split')   
    predict_data  = pd.read_json(PREDICT_DATA, orient='split')    
    frame_prediction = pd.DataFrame(predict_data)    
    prediction_result = data_test_df.assign(predictions=frame_prediction.values)
    prediction_result = prediction_result.sort_index(ascending=True)
    
     
    result = ''
    with pd.option_context('display.max_colwidth', -1):
      #result = df.to_html(escape=False)
      result = prediction_result.to_html(escape=False)
    
    css_style="""
    table {
      border: 1px solid #999999;
      text-align: center;
      border-collapse: collapse;
      width: 100%; 
    }
    td {
      border: 1px solid #999999;         
      padding: 3px 2px;
      font-size: 13px;
      border-bottom: 1px solid #999999;
      #border-bottom: 1px solid #FF8C00;  
      border-bottom: 1px solid #0B6FA4;   
    }
    th {
      font-size: 17px;
      font-weight: bold;
      color: #FFFFFF;
      text-align: center;
      background: #0B6FA4;
      #background: #E7702A;       
      #border-left: 2px solid #999999
      border-bottom: 1px solid #FF8C00;            
    }
    """
    result = """
           
                
            
            
            
            
            
            
            
            <!DOCTYPE html>
            <html>
              <head>
                <meta charset="UTF-8">
                  <style>{0}</style>
                </head>
                <body>{1}</body></html>
    """.format(css_style, result)
    
    if OUTPUT_FILE == 'HTML':  
        result = result.encode('utf-8')
        resultMetadata.put("file.extension", ".html")
        resultMetadata.put("file.name", "result.html")
        resultMetadata.put("content.type", "text/html")
        print("END Export_Results")
    elif OUTPUT_FILE == 'CSV':      
        result = prediction_result.to_csv()    
        resultMetadata.put("file.extension", ".csv")
        resultMetadata.put("file.name", "result.csv")
        resultMetadata.put("content.type", "text/csv") 
        print("END Export_Results")
else:
  print('It is not possible to export the data')
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"/>
        </task>
        <task name="Split_Data">
          <description>
            <![CDATA[ Divide the data into two sets. ]]>
          </description>
          <variables>
            <variable inherited="false" name="TRAIN_SIZE" value="0.7"/>
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/split_data.png"/>
          </genericInformation>
          <depends>
            <task ref="Feature_Vector_Extractor"/>
          </depends>
          <forkEnvironment javaHome="${PA_SCHEDULER_HOME}/jre">
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
#Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
# Prepare Docker parameters 
containerName = 'activeeon/dlm3' 
dockerRunCommand =  'docker run ' 
dockerParameters = '--rm ' 
# Prepare ProActive home volume 
paHomeHost = variables.get("PA_SCHEDULER_HOME") 
paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
# Prepare working directory (For Dataspaces and serialized task file) 
workspaceHost = localspace 
workspaceContainer = localspace 
workspaceVolume = '-v '+localspace +':'+localspace+' ' 
# Prepare container working directory 
containerWorkingDirectory = '-w '+workspaceContainer+' ' 
# Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
print("BEGIN Split_Data")

from sklearn import model_selection
import pandas as pd

TRAIN_SIZE = 0.7
try:
  IS_LABELED_DATA = variables.get("IS_LABELED_DATA")
  TRAIN_SIZE = float(variables.get("TRAIN_SIZE"))
except NameError:
  pass
test_size = 1 - TRAIN_SIZE

if IS_LABELED_DATA == 'True':
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
    
  data = dataframe.values[:,0:columns_number-1]
  label = dataframe.values[:,columns_number-1]
  indice = dataframe.index.values
  
  data_train, data_test, label_train, label_test, idx_train, idx_test = model_selection.train_test_split(data, label, indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name[0:columns_number-1],index=idx_train)
  label_train_df = pd.DataFrame(data=label_train,columns=[columns_name[columns_number-1]])
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name[0:columns_number-1],index=idx_test)
  label_test_df = pd.DataFrame(data=label_test,columns=[columns_name[columns_number-1]])
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  LABEL_TRAIN_DF_JSON = label_train_df.to_json(orient='split')
  LABEL_TEST_DF_JSON = label_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
    variables.put("LABEL_TRAIN_DF_JSON", LABEL_TRAIN_DF_JSON)
    variables.put("LABEL_TEST_DF_JSON", LABEL_TEST_DF_JSON)
  except NameError:
    pass
  
  print("END Split_Data")
  
elif IS_LABELED_DATA == 'False' or IS_LABELED_DATA == None:
  try:
    DATAFRAME_JSON = variables.get("DATAFRAME_JSON")
    COLUMNS_NAME_JSON = variables.get("COLUMNS_NAME_JSON")
  except NameError:
    pass
  
  dataframe = pd.read_json(DATAFRAME_JSON, orient='split')
  columns_name_df = pd.read_json(COLUMNS_NAME_JSON,typ='series')
  columns_name = columns_name_df.values
  columns_number = len(columns_name)
  indice = dataframe.index.values
  data = dataframe.values
  
  data_train, data_test, idx_train, idx_test = model_selection.train_test_split(data,indice, test_size=test_size)
  data_train_df = pd.DataFrame(data=data_train,columns=columns_name,index=idx_train)
  data_test_df = pd.DataFrame(data=data_test,columns=columns_name,index=idx_test)
  
  DATA_TRAIN_DF_JSON = data_train_df.to_json(orient='split')
  DATA_TEST_DF_JSON = data_test_df.to_json(orient='split')
  
  try:
    variables.put("DATA_TRAIN_DF_JSON", DATA_TRAIN_DF_JSON)
    variables.put("DATA_TEST_DF_JSON", DATA_TEST_DF_JSON)
  except NameError:
    pass
  
  print("END Split_Data")
else:
  print('The data could not be split, please check ypur ML pipeline')
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"/>
        </task>
        <task name="One_Class_SVM">
          <description>
            <![CDATA[ One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. ]]>
          </description>
          <variables>
            <variable inherited="false" name="NU" value="0.1"/>
            <variable inherited="false" name="KERNEL" value="rbf"/>
            <variable inherited="false" name="GAMMA" value="0.1"/>
            <variable inherited="true" model="PA:Boolean" name="DOCKER_ENABLED" value="True"/>
          </variables>
          <genericInformation>
            <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/ml_anomaly.png"/>
            <info name="task.documentation" value="https://doc.activeeon.com/latest/MLOS/MLOSUserGuide.html#_one_class_svm"/>
          </genericInformation>
          <forkEnvironment javaHome="/usr">
            <envScript>
              <script>
                <code language="python">
                  <![CDATA[
if str(variables.get("DOCKER_ENABLED")).lower() == 'true':
  #Be aware, that the prefix command is internally split by spaces. So paths with spaces won't work.
  # Prepare Docker parameters 
  containerName = 'activeeon/dlm3' 
  dockerRunCommand =  'docker run ' 
  dockerParameters = '--rm ' 
  # Prepare ProActive home volume 
  paHomeHost = variables.get("PA_SCHEDULER_HOME") 
  paHomeContainer = variables.get("PA_SCHEDULER_HOME") 
  proActiveHomeVolume = '-v '+paHomeHost +':'+paHomeContainer+' ' 
  # Prepare working directory (For Dataspaces and serialized task file) 
  workspaceHost = localspace 
  workspaceContainer = localspace 
  workspaceVolume = '-v '+localspace +':'+localspace+' ' 
  # Prepare container working directory 
  containerWorkingDirectory = '-w '+workspaceContainer+' ' 
  # Save pre execution command into magic variable 'preJavaHomeCmd', which is picked up by the node 
  preJavaHomeCmd = dockerRunCommand + dockerParameters + proActiveHomeVolume + workspaceVolume + containerWorkingDirectory + containerName
else:
  print("Fork environment disabled")
]]>
                </code>
              </script>
            </envScript>
          </forkEnvironment>
          <scriptExecutable>
            <script>
              <code language="cpython">
                <![CDATA[
KERNEL = variables.get("KERNEL")
NU = float(variables.get("NU"))
GAMMA = float(variables.get("GAMMA"))

variables.put("NU_PARA", NU)
variables.put("KERNEL_PARA", KERNEL)
variables.put("GAMMA_PARA", GAMMA)
variables.put("ALGORITHM_NAME", "OneClassSVM")
variables.put("UNSUPERVISED_ALGORITHM", "True")
variables.put("ONECLASS_MEASURE", "True")
]]>
              </code>
            </script>
          </scriptExecutable>
          <controlFlow block="none"/>
        </task>
      </taskFlow>
    </job>
