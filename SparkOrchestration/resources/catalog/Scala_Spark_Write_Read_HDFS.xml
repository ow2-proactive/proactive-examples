<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.14" xsi:schemaLocation="urn:proactive:jobdescriptor:3.14 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.14/schedulerjob.xsd"  name="Scala_Spark_Write_Read_HDFS" tags="Orchestration,Big Data,Artificial Intelligence,HDFS,Spark,Building blocks,Analytics" projectName="01. Spark" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="parquet_file_path" value="/user/hdfs/wiki/testwiki" />
    <variable name="csv_file_path" value="/user/hdfs/wiki/testwiki.csv" />
    <variable name="spark_service_instance_id" value="xx" model="PA:NOT_EMPTY_STRING"/>
  </variables>
  <description>
    <![CDATA[ A workflow to submit a Spark job from a docker container, to read/write files from/to HDFS. This workflow requires to start the PaaS Service Docker\_Swarm, HDFS and Spark before. Set the `spark_service_instance_id` parameter to the running Spark service instance id (also exposed in the results of the running Spark platform job). ]]>
  </description>
  <genericInformation>
    <info name="bucketName" value="data-big-data"/>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
    <info name="group" value="public-objects"/>
  </genericInformation>
  <taskFlow>
    <task name="retrieve_service_variables"




    fork="true">
      <scriptExecutable>
        <script>
          <file url="${PA_CATALOG_REST_URL}/buckets/service-automation/resources/Retrieve_variables_from_service_instance_id/raw" language="groovy">
            <arguments>
              <argument value="$spark_service_instance_id"/>
              <argument value="spark_network_name"/>
              <argument value="spark_network_name"/>
              <argument value="spark_master_url"/>
              <argument value="spark_master_url"/>
              <argument value="spark_token_name"/>
              <argument value="PSA_%{INSTANCE_NAME}"/>
              <argument value="hdfs_namenode_host_port"/>
              <argument value="hdfs_namenode_host_port"/>
            </arguments>
          </file>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            312.2000198364258
        </positionTop>
        <positionLeft>
            466
        </positionLeft>
      </metadata>
    </task>
    <task name="Spark_Write_Read_HDFS"




    fork="true">
      <description>
        <![CDATA[ A Scala Spark task to read/write files from/to HDFS. ]]>
      </description>
      <genericInformation>
        <info name="PRE_SCRIPT_AS_FILE" value="script.scala"/>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
        <info name="NODE_ACCESS_TOKEN" value="$spark_token_name"/>
      </genericInformation>
      <depends>
        <task ref="retrieve_service_variables"/>
      </depends>
      <pre>
        <script>
          <code language="scalaw">
            <![CDATA[
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode

// Defining an Helloworld class
case class HelloWorld(message: String)

object WriteReadHDFS {
  def main() {
    val spark = SparkSession
          .builder
          .appName("Spark Pi")
          .getOrCreate()
    val sc = spark.sparkContext

    // Get args
    val args = sc.getConf.get("spark.driver.args").split("\\s+")
    val parquet_file_path = args(0)
    val csv_file_path = args(1)
    val hdfs_url = args(2)

    // ====== Creating a dataframe with 1 partition
    val df = Seq(HelloWorld("helloworld")).toDF().coalesce(1)

    // ======= Writing files
    // Writing Dataframe as parquet file
    df.write.mode(SaveMode.Overwrite).parquet(hdfs_url + parquet_file_path)
    // Writing Dataframe as csv file
    df.write.mode(SaveMode.Overwrite).csv(hdfs_url + csv_file_path)

    // ======= Reading files
    // Reading parquet files into a Spark Dataframe
    val df_parquet = spark.read.parquet(hdfs_url + parquet_file_path)
    //  Reading csv files into a Spark Dataframe
    val df_csv = spark.read.option("inferSchema", "true").csv(hdfs_url + csv_file_path)

    // ======= Printing Dataframe
    println("df_parquet")
    df_parquet.show()
    println("df_csv")
    df_csv.show()
  }
}

WriteReadHDFS.main()
]]>
          </code>
        </script>
      </pre>
      <scriptExecutable>
        <script>
          <code language="groovy">
            <![CDATA[
// Retrieve variables
def spark_network_name = variables.get("spark_network_name")
def spark_master_url = variables.get("spark_master_url")
def hdfs_url = "hdfs://" + variables.get("hdfs_namenode_host_port")
def parquet_file_path = variables.get("parquet_file_path")
def csv_file_path = variables.get("csv_file_path")

// Submit the Spark job
def spark_shell_command = "/usr/local/spark/bin/spark-shell --driver-memory 800m --executor-memory 800m --master " + spark_master_url + " --jars /usr/local/spark/jars/* -I /localspace/script.scala --conf spark.driver.args='" + parquet_file_path + " " + csv_file_path + " " + hdfs_url + "'"
cmd = ["docker", "run", "--rm", "--net", spark_network_name, "-v", localspace + ":/localspace", "activeeon/hdfs-spark:latest", "bash", "-c", spark_shell_command]

cmd.execute().waitForProcessOutput(System.out, System.err)
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            440.19998931884766
        </positionTop>
        <positionLeft>
            466
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2254px;
            height:2725px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-307.2000198364258px;left:-461px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable" id="jsPlumb_1_106" style="top: 312.2px; left: 466px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="This task has no description"><img src="/studio/images/Groovy.png" width="20px">&nbsp;<span class="name">retrieve_service_variables</span></a></div><div class="task ui-draggable _jsPlumb_endpoint_anchor_" id="jsPlumb_1_109" style="top: 440.2px; left: 466px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="A Scala Spark task to read/write files from/to HDFS."><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">Spark_Write_Read_HDFS</span></a></div><svg style="position:absolute;left:530.5px;top:351.5px" width="22" height="89" pointer-events="none" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml" class="_jsPlumb_connector "><path d="M 1 88 C 11 38 -10 50 0 0 " transform="translate(10.5,0.5)" pointer-events="visibleStroke" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="none" stroke="#666" style=""></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.7341562499999994,66.78168750000002 L9.900828592736769,46.50923939383077 L3.155021153255475,52.793671109542124 L-4.087187797721125,47.08837449057529 L3.7341562499999994,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path><path pointer-events="all" version="1.1" xmlns="http://www.w3.org/1999/xhtml" d="M3.7341562499999994,66.78168750000002 L9.900828592736769,46.50923939383077 L3.155021153255475,52.793671109542124 L-4.087187797721125,47.08837449057529 L3.7341562499999994,66.78168750000002" class="" stroke="#666" fill="#666" transform="translate(10.5,0.5)"></path></svg><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 531px; top: 342px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 532px; top: 470px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div><div class="_jsPlumb_endpoint target-endpoint dependency-target-endpoint _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable _jsPlumb_endpoint_connected" style="position: absolute; height: 20px; width: 20px; left: 532px; top: 430px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>
